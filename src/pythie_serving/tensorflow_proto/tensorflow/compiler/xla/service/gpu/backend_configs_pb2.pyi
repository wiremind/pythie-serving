"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.message
import tensorflow.compiler.xla.xla_data_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class CudnnConvBackendConfig(google.protobuf.message.Message):
    """Backend configs for XLA:GPU.

    These are metadata that the GPU backend attaches to HloInstructions and later
    uses during e.g. codegen.

    Remember that proto3 doesn't give clients a way to tell the difference
    between a field not being present and a field having the default value.
    Choose your defaults carefully.

    No guarantee is made about the stability of these protos.

    See HloInstruction::backend_config() for more info.

    Backend config for a convolution that runs through cudnn.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    ALGORITHM_FIELD_NUMBER: builtins.int
    TENSOR_OPS_ENABLED_FIELD_NUMBER: builtins.int
    CONV_RESULT_SCALE_FIELD_NUMBER: builtins.int
    ACTIVATION_MODE_FIELD_NUMBER: builtins.int
    SIDE_INPUT_SCALE_FIELD_NUMBER: builtins.int
    algorithm: builtins.int
    """Opaque algorithm number of cudnn algorithm chosen for this conv."""

    tensor_ops_enabled: builtins.bool
    """Whether we may use tensor cores when running this conv.  Even if this is
    true, cudnn may choose not to use tensor cores, e.g. because the GPU or
    selected algorithm doesn't support it.
    """

    conv_result_scale: builtins.float
    """The scaling factor multiplied with the convolution result."""

    activation_mode: builtins.int
    """Below are the fields related to cuDNN's fused convolution. Refer to
    GpuConvParams for their meanings.

    The requested activation (e.g. relu) after the convolution. It is with type
    stream_executor::dnn::ActivationMode.
    """

    side_input_scale: builtins.float
    """The scaling factor multiplied with the side input. If no side input buffer
    is provided, this field must be 0.
    """

    def __init__(self,
        *,
        algorithm: builtins.int = ...,
        tensor_ops_enabled: builtins.bool = ...,
        conv_result_scale: builtins.float = ...,
        activation_mode: builtins.int = ...,
        side_input_scale: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["activation_mode",b"activation_mode","algorithm",b"algorithm","conv_result_scale",b"conv_result_scale","side_input_scale",b"side_input_scale","tensor_ops_enabled",b"tensor_ops_enabled"]) -> None: ...
global___CudnnConvBackendConfig = CudnnConvBackendConfig

class GemmBackendConfig(google.protobuf.message.Message):
    """Backend config for the GEMM operation running through cuBLAS."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    SELECTED_ALGORITHM_FIELD_NUMBER: builtins.int
    ALPHA_REAL_FIELD_NUMBER: builtins.int
    ALPHA_IMAG_FIELD_NUMBER: builtins.int
    BETA_FIELD_NUMBER: builtins.int
    DOT_DIMENSION_NUMBERS_FIELD_NUMBER: builtins.int
    BATCH_SIZE_FIELD_NUMBER: builtins.int
    selected_algorithm: builtins.int
    alpha_real: builtins.float
    alpha_imag: builtins.float
    beta: builtins.float
    @property
    def dot_dimension_numbers(self) -> tensorflow.compiler.xla.xla_data_pb2.DotDimensionNumbers: ...
    batch_size: builtins.int
    def __init__(self,
        *,
        selected_algorithm: builtins.int = ...,
        alpha_real: builtins.float = ...,
        alpha_imag: builtins.float = ...,
        beta: builtins.float = ...,
        dot_dimension_numbers: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.DotDimensionNumbers] = ...,
        batch_size: builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["algorithm",b"algorithm","dot_dimension_numbers",b"dot_dimension_numbers","selected_algorithm",b"selected_algorithm"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["algorithm",b"algorithm","alpha_imag",b"alpha_imag","alpha_real",b"alpha_real","batch_size",b"batch_size","beta",b"beta","dot_dimension_numbers",b"dot_dimension_numbers","selected_algorithm",b"selected_algorithm"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["algorithm",b"algorithm"]) -> typing.Optional[typing_extensions.Literal["selected_algorithm"]]: ...
global___GemmBackendConfig = GemmBackendConfig

class BitcastBackendConfig(google.protobuf.message.Message):
    """Backend config for bitcast operation generated from MLIR MHLO dialect."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    SOURCE_LAYOUT_FIELD_NUMBER: builtins.int
    RESULT_LAYOUT_FIELD_NUMBER: builtins.int
    @property
    def source_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.LayoutProto: ...
    @property
    def result_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.LayoutProto: ...
    def __init__(self,
        *,
        source_layout: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.LayoutProto] = ...,
        result_layout: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.LayoutProto] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["result_layout",b"result_layout","source_layout",b"source_layout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["result_layout",b"result_layout","source_layout",b"source_layout"]) -> None: ...
global___BitcastBackendConfig = BitcastBackendConfig
