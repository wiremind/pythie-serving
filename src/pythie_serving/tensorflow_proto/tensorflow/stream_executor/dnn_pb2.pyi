"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _DataType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DataTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    kFloat: _DataType.ValueType  # 0
    kDouble: _DataType.ValueType  # 1
    kHalf: _DataType.ValueType  # 2
    kInt8: _DataType.ValueType  # 3
    kInt32: _DataType.ValueType  # 4
    kComplexFloat: _DataType.ValueType  # 5
    kComplexDouble: _DataType.ValueType  # 6
class DataType(_DataType, metaclass=_DataTypeEnumTypeWrapper):
    """Specifies the data type used by an operation."""
    pass

kFloat: DataType.ValueType  # 0
kDouble: DataType.ValueType  # 1
kHalf: DataType.ValueType  # 2
kInt8: DataType.ValueType  # 3
kInt32: DataType.ValueType  # 4
kComplexFloat: DataType.ValueType  # 5
kComplexDouble: DataType.ValueType  # 6
global___DataType = DataType


class _DataLayout:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DataLayoutEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataLayout.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    kYXDepthBatch: _DataLayout.ValueType  # 0
    """Naming convention:
    Y <-> row or height
    X <-> column or width
    Batch <-> batch, or N
    Depth <-> feature, or channel
    TODO(timshen): turn them into cuDNN names, e.g. kNCHW.

    Note: In cudnn, kBatchDepthYX4 and kBatchDepthYX32 are the same layout
    (namely, NCHW_VECT_C).  It differentiates between these two by using a
    different data type (int8x4 vs int8x32).  In StreamExecutor we use
    different layouts for these, because we don't usually pass an explicit data
    type to StreamExecutor functions.
    """

    kYXBatchDepth: _DataLayout.ValueType  # 1
    kBatchYXDepth: _DataLayout.ValueType  # 2
    """cuDNN's NHWC layout"""

    kBatchDepthYX: _DataLayout.ValueType  # 3
    """cuDNN's NCHW layout"""

    kBatchDepthYX4: _DataLayout.ValueType  # 4
    """cuDNN's NCHW_VECT_C with 4-elem vectors (e.g. int8x4)"""

    kBatchDepthYX32: _DataLayout.ValueType  # 5
    """cuDNN's NCHW_VECT_C with 32-elem vects (e.g. int8x32)"""

class DataLayout(_DataLayout, metaclass=_DataLayoutEnumTypeWrapper):
    """Describes how a convolution input or output layer's data is formatted."""
    pass

kYXDepthBatch: DataLayout.ValueType  # 0
"""Naming convention:
Y <-> row or height
X <-> column or width
Batch <-> batch, or N
Depth <-> feature, or channel
TODO(timshen): turn them into cuDNN names, e.g. kNCHW.

Note: In cudnn, kBatchDepthYX4 and kBatchDepthYX32 are the same layout
(namely, NCHW_VECT_C).  It differentiates between these two by using a
different data type (int8x4 vs int8x32).  In StreamExecutor we use
different layouts for these, because we don't usually pass an explicit data
type to StreamExecutor functions.
"""

kYXBatchDepth: DataLayout.ValueType  # 1
kBatchYXDepth: DataLayout.ValueType  # 2
"""cuDNN's NHWC layout"""

kBatchDepthYX: DataLayout.ValueType  # 3
"""cuDNN's NCHW layout"""

kBatchDepthYX4: DataLayout.ValueType  # 4
"""cuDNN's NCHW_VECT_C with 4-elem vectors (e.g. int8x4)"""

kBatchDepthYX32: DataLayout.ValueType  # 5
"""cuDNN's NCHW_VECT_C with 32-elem vects (e.g. int8x32)"""

global___DataLayout = DataLayout


class _FilterLayout:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _FilterLayoutEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FilterLayout.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    kOutputInputYX: _FilterLayout.ValueType  # 0
    """Naming convention:
    Y <-> row or height
    X <-> column or width
    Output <-> output feature, or N
    Input <-> input feature, or N
    TODO(timshen): turn them into cuDNN names, e.g. kNCHW.
    cuDNN's NCHW layout
    """

    kOutputYXInput: _FilterLayout.ValueType  # 1
    """cuDNN's NHWC layout"""

    kOutputInputYX4: _FilterLayout.ValueType  # 2
    """cuDNN's NCHW_VECT_C layout with 4-elem vectors"""

    kOutputInputYX32: _FilterLayout.ValueType  # 5
    """cuDNN's NCHW_VECT_C layout with 32-elem vectors"""

    kInputYXOutput: _FilterLayout.ValueType  # 3
    kYXInputOutput: _FilterLayout.ValueType  # 4
class FilterLayout(_FilterLayout, metaclass=_FilterLayoutEnumTypeWrapper):
    """Describes how a convolution filter is laid out in the memory."""
    pass

kOutputInputYX: FilterLayout.ValueType  # 0
"""Naming convention:
Y <-> row or height
X <-> column or width
Output <-> output feature, or N
Input <-> input feature, or N
TODO(timshen): turn them into cuDNN names, e.g. kNCHW.
cuDNN's NCHW layout
"""

kOutputYXInput: FilterLayout.ValueType  # 1
"""cuDNN's NHWC layout"""

kOutputInputYX4: FilterLayout.ValueType  # 2
"""cuDNN's NCHW_VECT_C layout with 4-elem vectors"""

kOutputInputYX32: FilterLayout.ValueType  # 5
"""cuDNN's NCHW_VECT_C layout with 32-elem vectors"""

kInputYXOutput: FilterLayout.ValueType  # 3
kYXInputOutput: FilterLayout.ValueType  # 4
global___FilterLayout = FilterLayout


class _ActivationMode:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ActivationModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ActivationMode.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    kNone: _ActivationMode.ValueType  # 0
    kSigmoid: _ActivationMode.ValueType  # 1
    kRelu: _ActivationMode.ValueType  # 2
    """Rectified linear activation: f(x) = x < 0 ? 0 : x"""

    kRelu6: _ActivationMode.ValueType  # 3
    """Rectified linear activation; where upper maximum is 6.0."""

    kReluX: _ActivationMode.ValueType  # 4
    """Rectified linear activation; where upper maximum specified by
    BatchDescriptor::value_max().
    """

    kTanh: _ActivationMode.ValueType  # 5
    kBandPass: _ActivationMode.ValueType  # 6
    """Like ReluX; but passes all values in the range [-X,X]."""

class ActivationMode(_ActivationMode, metaclass=_ActivationModeEnumTypeWrapper):
    """Describes a kind of non-linearity (threshold-like mathematical function)."""
    pass

kNone: ActivationMode.ValueType  # 0
kSigmoid: ActivationMode.ValueType  # 1
kRelu: ActivationMode.ValueType  # 2
"""Rectified linear activation: f(x) = x < 0 ? 0 : x"""

kRelu6: ActivationMode.ValueType  # 3
"""Rectified linear activation; where upper maximum is 6.0."""

kReluX: ActivationMode.ValueType  # 4
"""Rectified linear activation; where upper maximum specified by
BatchDescriptor::value_max().
"""

kTanh: ActivationMode.ValueType  # 5
kBandPass: ActivationMode.ValueType  # 6
"""Like ReluX; but passes all values in the range [-X,X]."""

global___ActivationMode = ActivationMode


class _ConvolutionMode:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ConvolutionModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ConvolutionMode.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    CROSS_CORRELATION: _ConvolutionMode.ValueType  # 0
    CONVOLUTION: _ConvolutionMode.ValueType  # 1
class ConvolutionMode(_ConvolutionMode, metaclass=_ConvolutionModeEnumTypeWrapper):
    """Describe the math definition for the conv op. The popular behavior is
    actually called cross-correlation in math, despite the operation is often
    referred as convolution. See cuDNN cudnnConvolutionMode_t.
    """
    pass

CROSS_CORRELATION: ConvolutionMode.ValueType  # 0
CONVOLUTION: ConvolutionMode.ValueType  # 1
global___ConvolutionMode = ConvolutionMode


class _ConvolutionKind:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ConvolutionKindEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ConvolutionKind.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    INVALID: _ConvolutionKind.ValueType  # 0
    FORWARD: _ConvolutionKind.ValueType  # 1
    BACKWARD_FILTER: _ConvolutionKind.ValueType  # 2
    BACKWARD_DATA: _ConvolutionKind.ValueType  # 3
    FORWARD_BIAS_ACTIVATION: _ConvolutionKind.ValueType  # 4
class ConvolutionKind(_ConvolutionKind, metaclass=_ConvolutionKindEnumTypeWrapper):
    pass

INVALID: ConvolutionKind.ValueType  # 0
FORWARD: ConvolutionKind.ValueType  # 1
BACKWARD_FILTER: ConvolutionKind.ValueType  # 2
BACKWARD_DATA: ConvolutionKind.ValueType  # 3
FORWARD_BIAS_ACTIVATION: ConvolutionKind.ValueType  # 4
global___ConvolutionKind = ConvolutionKind


class TensorDescriptorProto(google.protobuf.message.Message):
    """Generic tensor representation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    DIMENSIONS_FIELD_NUMBER: builtins.int
    DATA_TYPE_FIELD_NUMBER: builtins.int
    DATA_LAYOUT_FIELD_NUMBER: builtins.int
    FILTER_LAYOUT_FIELD_NUMBER: builtins.int
    @property
    def dimensions(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    data_type: global___DataType.ValueType
    data_layout: global___DataLayout.ValueType
    filter_layout: global___FilterLayout.ValueType
    def __init__(self,
        *,
        dimensions: typing.Optional[typing.Iterable[builtins.int]] = ...,
        data_type: global___DataType.ValueType = ...,
        data_layout: global___DataLayout.ValueType = ...,
        filter_layout: global___FilterLayout.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data_layout",b"data_layout","filter_layout",b"filter_layout","layout_oneof",b"layout_oneof"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_layout",b"data_layout","data_type",b"data_type","dimensions",b"dimensions","filter_layout",b"filter_layout","layout_oneof",b"layout_oneof"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["layout_oneof",b"layout_oneof"]) -> typing.Optional[typing_extensions.Literal["data_layout","filter_layout"]]: ...
global___TensorDescriptorProto = TensorDescriptorProto

class AlgorithmProto(google.protobuf.message.Message):
    """Generic algorithm representation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _MathType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _MathTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[AlgorithmProto._MathType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DEFAULT_MATH: AlgorithmProto._MathType.ValueType  # 0
        TENSOR_OP_MATH: AlgorithmProto._MathType.ValueType  # 1
        """The GPU may operate 4x4 matrix FMA.
        See cuDNN's documentation for CUDNN_TENSOR_OP_MATH.
        """

    class MathType(_MathType, metaclass=_MathTypeEnumTypeWrapper):
        pass

    DEFAULT_MATH: AlgorithmProto.MathType.ValueType  # 0
    TENSOR_OP_MATH: AlgorithmProto.MathType.ValueType  # 1
    """The GPU may operate 4x4 matrix FMA.
    See cuDNN's documentation for CUDNN_TENSOR_OP_MATH.
    """


    ALGO_ID_FIELD_NUMBER: builtins.int
    MATH_TYPE_FIELD_NUMBER: builtins.int
    EXEC_PLAN_ID_FIELD_NUMBER: builtins.int
    algo_id: builtins.int
    math_type: global___AlgorithmProto.MathType.ValueType
    exec_plan_id: typing.Text
    """cuDNN v8 uses a string to uniquely represent the backend plan."""

    def __init__(self,
        *,
        algo_id: builtins.int = ...,
        math_type: global___AlgorithmProto.MathType.ValueType = ...,
        exec_plan_id: typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["algo_id",b"algo_id","exec_plan_id",b"exec_plan_id","math_type",b"math_type"]) -> None: ...
global___AlgorithmProto = AlgorithmProto

class ConvolutionDescriptorProto(google.protobuf.message.Message):
    """Convolution-specific parameters."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PADDINGS_FIELD_NUMBER: builtins.int
    STRIDES_FIELD_NUMBER: builtins.int
    DILATIONS_FIELD_NUMBER: builtins.int
    COMPUTE_MODE_FIELD_NUMBER: builtins.int
    GROUP_COUNT_FIELD_NUMBER: builtins.int
    CONVOLUTION_MODE_FIELD_NUMBER: builtins.int
    NAME_FIELD_NUMBER: builtins.int
    @property
    def paddings(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def strides(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def dilations(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    compute_mode: global___DataType.ValueType
    """The "accumulator" type. For example, use F32 as an accumulator for F16
    convolutions.
    See cuDNN's cudnnConvolutionMode_t.
    """

    group_count: builtins.int
    """See cuDNN's group count."""

    convolution_mode: global___ConvolutionMode.ValueType
    name: typing.Text
    """Tensorflow node name, same as in NodeDef, for debugging purposes."""

    def __init__(self,
        *,
        paddings: typing.Optional[typing.Iterable[builtins.int]] = ...,
        strides: typing.Optional[typing.Iterable[builtins.int]] = ...,
        dilations: typing.Optional[typing.Iterable[builtins.int]] = ...,
        compute_mode: global___DataType.ValueType = ...,
        group_count: builtins.int = ...,
        convolution_mode: global___ConvolutionMode.ValueType = ...,
        name: typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["compute_mode",b"compute_mode","convolution_mode",b"convolution_mode","dilations",b"dilations","group_count",b"group_count","name",b"name","paddings",b"paddings","strides",b"strides"]) -> None: ...
global___ConvolutionDescriptorProto = ConvolutionDescriptorProto
