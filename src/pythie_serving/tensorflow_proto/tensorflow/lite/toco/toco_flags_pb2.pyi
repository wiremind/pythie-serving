"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
Copyright 2022 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import tensorflow.lite.toco.types_pb2
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _FileFormat:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _FileFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FileFormat.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    FILE_FORMAT_UNKNOWN: _FileFormat.ValueType  # 0
    TENSORFLOW_GRAPHDEF: _FileFormat.ValueType  # 1
    """GraphDef, third_party/tensorflow/core/framework/graph.proto"""
    TFLITE: _FileFormat.ValueType  # 2
    """Tensorflow's mobile inference model.
    third_party/tensorflow/lite/schema/schema.fbs
    """
    GRAPHVIZ_DOT: _FileFormat.ValueType  # 3
    """GraphViz
    Export-only.
    """

class FileFormat(_FileFormat, metaclass=_FileFormatEnumTypeWrapper):
    """Supported I/O file formats. Some formats may be input-only or output-only."""

FILE_FORMAT_UNKNOWN: FileFormat.ValueType  # 0
TENSORFLOW_GRAPHDEF: FileFormat.ValueType  # 1
"""GraphDef, third_party/tensorflow/core/framework/graph.proto"""
TFLITE: FileFormat.ValueType  # 2
"""Tensorflow's mobile inference model.
third_party/tensorflow/lite/schema/schema.fbs
"""
GRAPHVIZ_DOT: FileFormat.ValueType  # 3
"""GraphViz
Export-only.
"""
global___FileFormat = FileFormat

@typing_extensions.final
class TocoFlags(google.protobuf.message.Message):
    """TocoFlags encodes extra parameters that drive tooling operations, that
    are not normally encoded in model files and in general may not be thought
    of as properties of models, instead describing how models are to be
    processed in the context of the present tooling job.

    Next ID to use: 51.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INPUT_FORMAT_FIELD_NUMBER: builtins.int
    OUTPUT_FORMAT_FIELD_NUMBER: builtins.int
    INFERENCE_INPUT_TYPE_FIELD_NUMBER: builtins.int
    INFERENCE_TYPE_FIELD_NUMBER: builtins.int
    DEFAULT_RANGES_MIN_FIELD_NUMBER: builtins.int
    DEFAULT_RANGES_MAX_FIELD_NUMBER: builtins.int
    DEFAULT_INT16_RANGES_MIN_FIELD_NUMBER: builtins.int
    DEFAULT_INT16_RANGES_MAX_FIELD_NUMBER: builtins.int
    DROP_FAKE_QUANT_FIELD_NUMBER: builtins.int
    REORDER_ACROSS_FAKE_QUANT_FIELD_NUMBER: builtins.int
    ALLOW_CUSTOM_OPS_FIELD_NUMBER: builtins.int
    DROP_CONTROL_DEPENDENCY_FIELD_NUMBER: builtins.int
    DEBUG_DISABLE_RECURRENT_CELL_FUSION_FIELD_NUMBER: builtins.int
    PROPAGATE_FAKE_QUANT_NUM_BITS_FIELD_NUMBER: builtins.int
    ALLOW_NUDGING_WEIGHTS_TO_USE_FAST_GEMM_KERNEL_FIELD_NUMBER: builtins.int
    DEDUPE_ARRAY_MIN_SIZE_BYTES_FIELD_NUMBER: builtins.int
    SPLIT_TFLITE_LSTM_INPUTS_FIELD_NUMBER: builtins.int
    QUANTIZE_WEIGHTS_FIELD_NUMBER: builtins.int
    DUMP_GRAPHVIZ_DIR_FIELD_NUMBER: builtins.int
    DUMP_GRAPHVIZ_INCLUDE_VIDEO_FIELD_NUMBER: builtins.int
    POST_TRAINING_QUANTIZE_FIELD_NUMBER: builtins.int
    ENABLE_SELECT_TF_OPS_FIELD_NUMBER: builtins.int
    FORCE_SELECT_TF_OPS_FIELD_NUMBER: builtins.int
    QUANTIZE_TO_FLOAT16_FIELD_NUMBER: builtins.int
    ALLOW_DYNAMIC_TENSORS_FIELD_NUMBER: builtins.int
    CONVERSION_SUMMARY_DIR_FIELD_NUMBER: builtins.int
    CUSTOM_OPDEFS_FIELD_NUMBER: builtins.int
    SELECT_USER_TF_OPS_FIELD_NUMBER: builtins.int
    ENABLE_TFLITE_RESOURCE_VARIABLES_FIELD_NUMBER: builtins.int
    UNFOLD_BATCHMATMUL_FIELD_NUMBER: builtins.int
    LOWER_TENSOR_LIST_OPS_FIELD_NUMBER: builtins.int
    ACCUMULATION_TYPE_FIELD_NUMBER: builtins.int
    ALLOW_BFLOAT16_FIELD_NUMBER: builtins.int
    ALLOW_ALL_SELECT_TF_OPS_FIELD_NUMBER: builtins.int
    UNFOLD_LARGE_SPLAT_CONSTANT_FIELD_NUMBER: builtins.int
    SUPPORTED_BACKENDS_FIELD_NUMBER: builtins.int
    DEFAULT_TO_SINGLE_BATCH_IN_TENSOR_LIST_OPS_FIELD_NUMBER: builtins.int
    DISABLE_PER_CHANNEL_QUANTIZATION_FIELD_NUMBER: builtins.int
    ENABLE_MLIR_DYNAMIC_RANGE_QUANTIZER_FIELD_NUMBER: builtins.int
    TF_QUANTIZATION_MODE_FIELD_NUMBER: builtins.int
    DISABLE_INFER_TENSOR_RANGE_FIELD_NUMBER: builtins.int
    USE_FAKE_QUANT_NUM_BITS_FIELD_NUMBER: builtins.int
    ENABLE_DYNAMIC_UPDATE_SLICE_FIELD_NUMBER: builtins.int
    PRESERVE_ASSERT_OP_FIELD_NUMBER: builtins.int
    GUARANTEE_ALL_FUNCS_ONE_USE_FIELD_NUMBER: builtins.int
    input_format: global___FileFormat.ValueType
    """Input file format"""
    output_format: global___FileFormat.ValueType
    """Output file format"""
    inference_input_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType
    """Similar to inference_type, but allows to control specifically the
    quantization of input arrays, separately from other arrays.

    If not set, then the value of inference_type is implicitly used, i.e.
    by default input arrays are quantized like other arrays.

    Like inference_type, this only affects real-number arrays. By "real-number"
    we mean float arrays, and quantized arrays. This excludes plain
    integer arrays, strings arrays, and every other data type.

    The typical use for this flag is for vision models taking a bitmap
    as input, typically with uint8 channels, yet still requiring floating-point
    inference. For such image models, the uint8 input is quantized, i.e.
    the uint8 values are interpreted as real numbers, and the quantization
    parameters used for such input arrays are their mean_value, std_value
    parameters.
    """
    inference_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType
    """Sets the type of real-number arrays in the output file, that is, controls
    the representation (quantization) of real numbers in the output file,
    except for input arrays, which are controlled by inference_input_type.

    NOTE: this flag only impacts real-number arrays. By "real-number"
    we mean float arrays, and quantized arrays. This excludes plain
    integer arrays, strings arrays, and every other data type.

    For real-number arrays, the impact of this flag is to allow the output
    file to choose a different real-numbers representation (quantization)
    from what the input file used. For any other types of arrays, changing
    the data type would not make sense.

    Specifically:
       - If FLOAT, then real-numbers arrays will be of type float in
         the output file. If they were quantized in the input file, then
         they get dequantized.
       - If QUANTIZED_UINT8, then real-numbers arrays will be quantized
         as uint8 in the output file. If they were float in the input file,
         then they get quantized.
       - If not set, then all real-numbers arrays retain the same type in the
         output file as they have in the input file.
    """
    default_ranges_min: builtins.float
    """default_ranges_min and default_ranges_max are helpers to experiment
    with quantization of models. Normally, quantization requires the input
    model to have (min, max) range information for every activations array.
    This is needed in order to know how to quantize arrays and still achieve
    satisfactory accuracy. However, in some circumstances one would just like
    to estimate the performance of quantized inference, without caring about
    accuracy. That is what default_ranges_min and default_ranges_max are for:
    when specified, they will be used as default (min, max) range boundaries
    for all activation arrays that lack (min, max) range information, thus
    allowing for quantization to proceed.

    It should be clear from the above explanation that these parameters are
    for experimentation purposes only and should not be used in production:
    they make it easy to quantize models, but the resulting quantized model
    will be inaccurate.

    These values only apply to arrays quantized with the kUint8 data type.
    """
    default_ranges_max: builtins.float
    default_int16_ranges_min: builtins.float
    """Equivalent versions of default_ranges_min/_max for arrays quantized with
    the kInt16 data type.
    """
    default_int16_ranges_max: builtins.float
    drop_fake_quant: builtins.bool
    """Ignore and discard FakeQuant nodes. For instance, that can be used to
    generate plain float code without fake-quantization from a quantized
    graph.
    """
    reorder_across_fake_quant: builtins.bool
    """Normally, FakeQuant nodes must be strict boundaries for graph
    transformations, in order to ensure that quantized inference has the
    exact same arithmetic behavior as quantized training --- which is the
    whole point of quantized training and of FakeQuant nodes in the first
    place. However, that entails subtle requirements on where exactly
    FakeQuant nodes must be placed in the graph. Some quantized graphs
    have FakeQuant nodes at unexpected locations, that prevent graph
    transformations that are necessary in order to generate inference
    code for these graphs. Such graphs should be fixed, but as a
    temporary work-around, setting this reorder_across_fake_quant flag
    allows toco to perform necessary graph transformations on them,
    at the cost of no longer faithfully matching inference and training
    arithmetic.
    """
    allow_custom_ops: builtins.bool
    """If true, allow TOCO to create TF Lite Custom operators for all the
    unsupported Tensorflow ops.
    """
    drop_control_dependency: builtins.bool
    """Applies only to the case when the input format is TENSORFLOW_GRAPHDEF.
    If true, then control dependencies will be immediately dropped during
    import.
    If not set, the default behavior is as follows:
       - Default to false if the output format is TENSORFLOW_GRAPHDEF.
       - Default to true in all other cases.
    """
    debug_disable_recurrent_cell_fusion: builtins.bool
    """Disables transformations that fuse subgraphs such as known LSTMs (not all
    LSTMs are identified).
    """
    propagate_fake_quant_num_bits: builtins.bool
    """Uses the FakeQuantWithMinMaxArgs.num_bits attribute to adjust quantized
    array data types throughout the graph. The graph must be properly annotated
    with FakeQuant* ops on at least the edges and may contain additional ops on
    the interior of the graph to widen/narrow as desired.

    Input and output array data types may change because of this propagation
    and users must be sure to query the final data_type values.
    """
    allow_nudging_weights_to_use_fast_gemm_kernel: builtins.bool
    """Some fast uint8 GEMM kernels require uint8 weights to avoid the value 0.
    This flag allows nudging them to 1 to allow proceeding, with moderate
    inaccuracy.
    """
    dedupe_array_min_size_bytes: builtins.int
    """Minimum size of constant arrays to deduplicate; arrays smaller will not be
    deduplicated.
    """
    split_tflite_lstm_inputs: builtins.bool
    """Split the LSTM inputs from 5 tensors to 18 tensors for TFLite.
    Ignored if the output format is not TFLite.
    """
    quantize_weights: builtins.bool
    """Store weights as quantized weights followed by dequantize operations.
    Computation is still done in float, but reduces model size (at the cost of
    accuracy and latency).
    DEPRECATED: Please use post_training_quantize instead.
    """
    dump_graphviz_dir: builtins.str
    """Full filepath of folder to dump the graphs at various stages of processing
    GraphViz .dot files. Preferred over --output_format=GRAPHVIZ_DOT in order
    to keep the requirements of the output file.
    """
    dump_graphviz_include_video: builtins.bool
    """Boolean indicating whether to dump the graph after every graph
    transformation.
    """
    post_training_quantize: builtins.bool
    """Boolean indicating whether to quantize the weights of the converted float
    model. Model size will be reduced and there will be latency improvements
    (at the cost of accuracy).
    """
    enable_select_tf_ops: builtins.bool
    """This flag only works when converting to TensorFlow Lite format.
    When enabled, unsupported ops will be converted to select TensorFlow ops.
    TODO(ycling): Consider to rename the following 2 flags and don't call it
    "Flex".
    `enable_select_tf_ops` should always be used with `allow_custom_ops`.
    WARNING: Experimental interface, subject to change
    """
    force_select_tf_ops: builtins.bool
    """This flag only works when converting to TensorFlow Lite format.
    When enabled, all TensorFlow ops will be converted to select TensorFlow
    ops.
    This will force `enable_select_tf_ops` to true.
    `force_select_tf_ops` should always be used with `enable_select_tf_ops`.
    WARNING: Experimental interface, subject to change
    """
    quantize_to_float16: builtins.bool
    """Boolean indicating whether to convert float32 constant buffers to
    float16. This is typically done to reduce model size. Delegates may also
    wish to implement kernels on reduced precision floats for performance
    gains.
    """
    allow_dynamic_tensors: builtins.bool
    """Boolean flag indicating whether the converter should allow models with
    dynamic Tensor shape. When set to False, the converter will generate
    runtime memory offsets for activation Tensors (with 128 bits alignment)
    and error out on models with undetermined Tensor shape. (Default: True)
    """
    conversion_summary_dir: builtins.str
    """Full filepath of the folder to dump conversion logs. This includes a global
    view of the conversion process, and user can choose to submit those logs.
    """
    @property
    def custom_opdefs(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """String representing the custom ops OpDefs that are included in the
        GraphDef.
        Deprecated do not use.
        """
    @property
    def select_user_tf_ops(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """Name of user's defined Tensorflow ops required in the TensorFlow Lite
        runtime. These ops will be supported as select TensorFlow ops.
        """
    enable_tflite_resource_variables: builtins.bool
    """Whether to enable tflite resource variables during conversion or not.
    Note: This is an experimental feature.
    """
    unfold_batchmatmul: builtins.bool
    """Whether to unfold tf.BatchMatMul to a set of tfl.fully_connected ops. If
    not, translate to tfl.batch_matmul.
    WARNING: Experimental interface, subject to change.
    """
    lower_tensor_list_ops: builtins.bool
    """Whether to lower static Tensor List ops to builtin ops. If not, use Flex
    tensor list ops.
    WARNING: Experimental interface, subject to change.
    """
    accumulation_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType
    """The accumulation type to use when quantize_to_float16 is true. Typical
    choices would be either float16 or float32.
    """
    allow_bfloat16: builtins.bool
    """Whether this model supports inference in bfloat16.
    Note: This is an experimental feature.
    """
    allow_all_select_tf_ops: builtins.bool
    """If true, automatically adds all tf ops into the model as select Tensorflow
    ops.
    """
    unfold_large_splat_constant: builtins.bool
    """Whether to unfold large splat constant tensors in the flatbuffer to reduce
    model size.
    """
    @property
    def supported_backends(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """Name of TFLite backends which are needed to check compatibility.
        WARNING: Experimental interface, subject to change.
        """
    default_to_single_batch_in_tensor_list_ops: builtins.bool
    """Whether to force to use batch size one when the batch size is None during
    lowering tensor list ops.
    """
    disable_per_channel_quantization: builtins.bool
    """Disable per_channel quantization for dynamic range quantization.
    Note: This is an experimental feature
    """
    enable_mlir_dynamic_range_quantizer: builtins.bool
    """If false, the old TOCO dynamic range quantization is used.
    Note: This is an experimental feature
    """
    tf_quantization_mode: builtins.str
    """When the output model is used for TF Quantization, this flag indicates the
    mode of TF Quantization. Ex: DEFAULT, LEGACY_INTEGER,...
    """
    disable_infer_tensor_range: builtins.bool
    """Disable inferring tensor range for quantization.
    Note: This is an experimental feature
    """
    use_fake_quant_num_bits: builtins.bool
    """Enable using num bits set in fake quant attributes for quantization.
    Note: This is an experimental feature
    """
    enable_dynamic_update_slice: builtins.bool
    """Enable converting to DynamicUpdateSlice op (for ops like TensorListSetItem)
    Note: This is an experimental feature
    """
    preserve_assert_op: builtins.bool
    """Whether to preserve `TF::AssertOp`."""
    guarantee_all_funcs_one_use: builtins.bool
    """Whether to ensure each function has a single use."""
    def __init__(
        self,
        *,
        input_format: global___FileFormat.ValueType | None = ...,
        output_format: global___FileFormat.ValueType | None = ...,
        inference_input_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType | None = ...,
        inference_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType | None = ...,
        default_ranges_min: builtins.float | None = ...,
        default_ranges_max: builtins.float | None = ...,
        default_int16_ranges_min: builtins.float | None = ...,
        default_int16_ranges_max: builtins.float | None = ...,
        drop_fake_quant: builtins.bool | None = ...,
        reorder_across_fake_quant: builtins.bool | None = ...,
        allow_custom_ops: builtins.bool | None = ...,
        drop_control_dependency: builtins.bool | None = ...,
        debug_disable_recurrent_cell_fusion: builtins.bool | None = ...,
        propagate_fake_quant_num_bits: builtins.bool | None = ...,
        allow_nudging_weights_to_use_fast_gemm_kernel: builtins.bool | None = ...,
        dedupe_array_min_size_bytes: builtins.int | None = ...,
        split_tflite_lstm_inputs: builtins.bool | None = ...,
        quantize_weights: builtins.bool | None = ...,
        dump_graphviz_dir: builtins.str | None = ...,
        dump_graphviz_include_video: builtins.bool | None = ...,
        post_training_quantize: builtins.bool | None = ...,
        enable_select_tf_ops: builtins.bool | None = ...,
        force_select_tf_ops: builtins.bool | None = ...,
        quantize_to_float16: builtins.bool | None = ...,
        allow_dynamic_tensors: builtins.bool | None = ...,
        conversion_summary_dir: builtins.str | None = ...,
        custom_opdefs: collections.abc.Iterable[builtins.str] | None = ...,
        select_user_tf_ops: collections.abc.Iterable[builtins.str] | None = ...,
        enable_tflite_resource_variables: builtins.bool | None = ...,
        unfold_batchmatmul: builtins.bool | None = ...,
        lower_tensor_list_ops: builtins.bool | None = ...,
        accumulation_type: tensorflow.lite.toco.types_pb2.IODataType.ValueType | None = ...,
        allow_bfloat16: builtins.bool | None = ...,
        allow_all_select_tf_ops: builtins.bool | None = ...,
        unfold_large_splat_constant: builtins.bool | None = ...,
        supported_backends: collections.abc.Iterable[builtins.str] | None = ...,
        default_to_single_batch_in_tensor_list_ops: builtins.bool | None = ...,
        disable_per_channel_quantization: builtins.bool | None = ...,
        enable_mlir_dynamic_range_quantizer: builtins.bool | None = ...,
        tf_quantization_mode: builtins.str | None = ...,
        disable_infer_tensor_range: builtins.bool | None = ...,
        use_fake_quant_num_bits: builtins.bool | None = ...,
        enable_dynamic_update_slice: builtins.bool | None = ...,
        preserve_assert_op: builtins.bool | None = ...,
        guarantee_all_funcs_one_use: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["accumulation_type", b"accumulation_type", "allow_all_select_tf_ops", b"allow_all_select_tf_ops", "allow_bfloat16", b"allow_bfloat16", "allow_custom_ops", b"allow_custom_ops", "allow_dynamic_tensors", b"allow_dynamic_tensors", "allow_nudging_weights_to_use_fast_gemm_kernel", b"allow_nudging_weights_to_use_fast_gemm_kernel", "conversion_summary_dir", b"conversion_summary_dir", "debug_disable_recurrent_cell_fusion", b"debug_disable_recurrent_cell_fusion", "dedupe_array_min_size_bytes", b"dedupe_array_min_size_bytes", "default_int16_ranges_max", b"default_int16_ranges_max", "default_int16_ranges_min", b"default_int16_ranges_min", "default_ranges_max", b"default_ranges_max", "default_ranges_min", b"default_ranges_min", "default_to_single_batch_in_tensor_list_ops", b"default_to_single_batch_in_tensor_list_ops", "disable_infer_tensor_range", b"disable_infer_tensor_range", "disable_per_channel_quantization", b"disable_per_channel_quantization", "drop_control_dependency", b"drop_control_dependency", "drop_fake_quant", b"drop_fake_quant", "dump_graphviz_dir", b"dump_graphviz_dir", "dump_graphviz_include_video", b"dump_graphviz_include_video", "enable_dynamic_update_slice", b"enable_dynamic_update_slice", "enable_mlir_dynamic_range_quantizer", b"enable_mlir_dynamic_range_quantizer", "enable_select_tf_ops", b"enable_select_tf_ops", "enable_tflite_resource_variables", b"enable_tflite_resource_variables", "force_select_tf_ops", b"force_select_tf_ops", "guarantee_all_funcs_one_use", b"guarantee_all_funcs_one_use", "inference_input_type", b"inference_input_type", "inference_type", b"inference_type", "input_format", b"input_format", "lower_tensor_list_ops", b"lower_tensor_list_ops", "output_format", b"output_format", "post_training_quantize", b"post_training_quantize", "preserve_assert_op", b"preserve_assert_op", "propagate_fake_quant_num_bits", b"propagate_fake_quant_num_bits", "quantize_to_float16", b"quantize_to_float16", "quantize_weights", b"quantize_weights", "reorder_across_fake_quant", b"reorder_across_fake_quant", "split_tflite_lstm_inputs", b"split_tflite_lstm_inputs", "tf_quantization_mode", b"tf_quantization_mode", "unfold_batchmatmul", b"unfold_batchmatmul", "unfold_large_splat_constant", b"unfold_large_splat_constant", "use_fake_quant_num_bits", b"use_fake_quant_num_bits"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["accumulation_type", b"accumulation_type", "allow_all_select_tf_ops", b"allow_all_select_tf_ops", "allow_bfloat16", b"allow_bfloat16", "allow_custom_ops", b"allow_custom_ops", "allow_dynamic_tensors", b"allow_dynamic_tensors", "allow_nudging_weights_to_use_fast_gemm_kernel", b"allow_nudging_weights_to_use_fast_gemm_kernel", "conversion_summary_dir", b"conversion_summary_dir", "custom_opdefs", b"custom_opdefs", "debug_disable_recurrent_cell_fusion", b"debug_disable_recurrent_cell_fusion", "dedupe_array_min_size_bytes", b"dedupe_array_min_size_bytes", "default_int16_ranges_max", b"default_int16_ranges_max", "default_int16_ranges_min", b"default_int16_ranges_min", "default_ranges_max", b"default_ranges_max", "default_ranges_min", b"default_ranges_min", "default_to_single_batch_in_tensor_list_ops", b"default_to_single_batch_in_tensor_list_ops", "disable_infer_tensor_range", b"disable_infer_tensor_range", "disable_per_channel_quantization", b"disable_per_channel_quantization", "drop_control_dependency", b"drop_control_dependency", "drop_fake_quant", b"drop_fake_quant", "dump_graphviz_dir", b"dump_graphviz_dir", "dump_graphviz_include_video", b"dump_graphviz_include_video", "enable_dynamic_update_slice", b"enable_dynamic_update_slice", "enable_mlir_dynamic_range_quantizer", b"enable_mlir_dynamic_range_quantizer", "enable_select_tf_ops", b"enable_select_tf_ops", "enable_tflite_resource_variables", b"enable_tflite_resource_variables", "force_select_tf_ops", b"force_select_tf_ops", "guarantee_all_funcs_one_use", b"guarantee_all_funcs_one_use", "inference_input_type", b"inference_input_type", "inference_type", b"inference_type", "input_format", b"input_format", "lower_tensor_list_ops", b"lower_tensor_list_ops", "output_format", b"output_format", "post_training_quantize", b"post_training_quantize", "preserve_assert_op", b"preserve_assert_op", "propagate_fake_quant_num_bits", b"propagate_fake_quant_num_bits", "quantize_to_float16", b"quantize_to_float16", "quantize_weights", b"quantize_weights", "reorder_across_fake_quant", b"reorder_across_fake_quant", "select_user_tf_ops", b"select_user_tf_ops", "split_tflite_lstm_inputs", b"split_tflite_lstm_inputs", "supported_backends", b"supported_backends", "tf_quantization_mode", b"tf_quantization_mode", "unfold_batchmatmul", b"unfold_batchmatmul", "unfold_large_splat_constant", b"unfold_large_splat_constant", "use_fake_quant_num_bits", b"use_fake_quant_num_bits"]) -> None: ...

global___TocoFlags = TocoFlags
