"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
This schema defines how to configure TFLite for delegation. These
definitions can be used in multiple ways: as output of a compatibility list,
in benchmarking tools and to decouple delegate instantiation from code.

The schema is work-in-progress, covering the most broadly used delegates and
options.
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _ExecutionPreference:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _ExecutionPreferenceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ExecutionPreference.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    ANY: _ExecutionPreference.ValueType  # 0
    """Match any selected preference. Allowlist (semantically - value is same as
    on input).
    """
    LOW_LATENCY: _ExecutionPreference.ValueType  # 1
    """Match low latency preference. Both compatibility list and input."""
    LOW_POWER: _ExecutionPreference.ValueType  # 2
    """Math low power preference. Both compatibility list and input."""
    FORCE_CPU: _ExecutionPreference.ValueType  # 3
    """Never accelerate. Can be used for input to compatibility list or for
    standalone Acceleration configuration.
    """

class ExecutionPreference(_ExecutionPreference, metaclass=_ExecutionPreferenceEnumTypeWrapper):
    """ExecutionPreference is used to match accelerators against the preferences of
    the current application or usecase. Some of the values here can appear both
    in the compatibility list and as input, some only as input.

    These are separate from NNAPIExecutionPreference - the compatibility list
    design doesn't assume a one-to-one mapping between which usecases
    compatibility list entries have been developed for and what settings are used
    for NNAPI.
    """

ANY: ExecutionPreference.ValueType  # 0
"""Match any selected preference. Allowlist (semantically - value is same as
on input).
"""
LOW_LATENCY: ExecutionPreference.ValueType  # 1
"""Match low latency preference. Both compatibility list and input."""
LOW_POWER: ExecutionPreference.ValueType  # 2
"""Math low power preference. Both compatibility list and input."""
FORCE_CPU: ExecutionPreference.ValueType  # 3
"""Never accelerate. Can be used for input to compatibility list or for
standalone Acceleration configuration.
"""
global___ExecutionPreference = ExecutionPreference

class _Delegate:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _DelegateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Delegate.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    NONE: _Delegate.ValueType  # 0
    NNAPI: _Delegate.ValueType  # 1
    GPU: _Delegate.ValueType  # 2
    HEXAGON: _Delegate.ValueType  # 3
    XNNPACK: _Delegate.ValueType  # 4
    EDGETPU: _Delegate.ValueType  # 5
    """The EdgeTpu in Pixel devices."""
    EDGETPU_CORAL: _Delegate.ValueType  # 6
    """The Coral EdgeTpu Dev Board / USB accelerator."""
    CORE_ML: _Delegate.ValueType  # 7
    """Apple CoreML."""

class Delegate(_Delegate, metaclass=_DelegateEnumTypeWrapper):
    """TFLite accelerator to use."""

NONE: Delegate.ValueType  # 0
NNAPI: Delegate.ValueType  # 1
GPU: Delegate.ValueType  # 2
HEXAGON: Delegate.ValueType  # 3
XNNPACK: Delegate.ValueType  # 4
EDGETPU: Delegate.ValueType  # 5
"""The EdgeTpu in Pixel devices."""
EDGETPU_CORAL: Delegate.ValueType  # 6
"""The Coral EdgeTpu Dev Board / USB accelerator."""
CORE_ML: Delegate.ValueType  # 7
"""Apple CoreML."""
global___Delegate = Delegate

class _NNAPIExecutionPreference:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _NNAPIExecutionPreferenceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_NNAPIExecutionPreference.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED: _NNAPIExecutionPreference.ValueType  # 0
    """Undefined."""
    NNAPI_LOW_POWER: _NNAPIExecutionPreference.ValueType  # 1
    """Prefer executing in a way that minimizes battery drain."""
    NNAPI_FAST_SINGLE_ANSWER: _NNAPIExecutionPreference.ValueType  # 2
    """Prefer returning a single answer as fast as possible, even if this causes
    more power consumption.
    """
    NNAPI_SUSTAINED_SPEED: _NNAPIExecutionPreference.ValueType  # 3
    """Prefer maximizing the throughput of successive frames, for example when
    processing successive frames coming from the camera.
    """

class NNAPIExecutionPreference(_NNAPIExecutionPreference, metaclass=_NNAPIExecutionPreferenceEnumTypeWrapper): ...

UNDEFINED: NNAPIExecutionPreference.ValueType  # 0
"""Undefined."""
NNAPI_LOW_POWER: NNAPIExecutionPreference.ValueType  # 1
"""Prefer executing in a way that minimizes battery drain."""
NNAPI_FAST_SINGLE_ANSWER: NNAPIExecutionPreference.ValueType  # 2
"""Prefer returning a single answer as fast as possible, even if this causes
more power consumption.
"""
NNAPI_SUSTAINED_SPEED: NNAPIExecutionPreference.ValueType  # 3
"""Prefer maximizing the throughput of successive frames, for example when
processing successive frames coming from the camera.
"""
global___NNAPIExecutionPreference = NNAPIExecutionPreference

class _NNAPIExecutionPriority:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _NNAPIExecutionPriorityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_NNAPIExecutionPriority.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    NNAPI_PRIORITY_UNDEFINED: _NNAPIExecutionPriority.ValueType  # 0
    NNAPI_PRIORITY_LOW: _NNAPIExecutionPriority.ValueType  # 1
    NNAPI_PRIORITY_MEDIUM: _NNAPIExecutionPriority.ValueType  # 2
    NNAPI_PRIORITY_HIGH: _NNAPIExecutionPriority.ValueType  # 3

class NNAPIExecutionPriority(_NNAPIExecutionPriority, metaclass=_NNAPIExecutionPriorityEnumTypeWrapper): ...

NNAPI_PRIORITY_UNDEFINED: NNAPIExecutionPriority.ValueType  # 0
NNAPI_PRIORITY_LOW: NNAPIExecutionPriority.ValueType  # 1
NNAPI_PRIORITY_MEDIUM: NNAPIExecutionPriority.ValueType  # 2
NNAPI_PRIORITY_HIGH: NNAPIExecutionPriority.ValueType  # 3
global___NNAPIExecutionPriority = NNAPIExecutionPriority

class _GPUBackend:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _GPUBackendEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_GPUBackend.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNSET: _GPUBackend.ValueType  # 0
    OPENCL: _GPUBackend.ValueType  # 1
    OPENGL: _GPUBackend.ValueType  # 2
    """Not yet supported.
    VULKAN = 3;
    METAL = 4;
    """

class GPUBackend(_GPUBackend, metaclass=_GPUBackendEnumTypeWrapper):
    """Which GPU backend to select. Default behaviour on Android is to try OpenCL
    and if it's not available fall back to OpenGL.
    """

UNSET: GPUBackend.ValueType  # 0
OPENCL: GPUBackend.ValueType  # 1
OPENGL: GPUBackend.ValueType  # 2
"""Not yet supported.
VULKAN = 3;
METAL = 4;
"""
global___GPUBackend = GPUBackend

class _GPUInferencePriority:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _GPUInferencePriorityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_GPUInferencePriority.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    GPU_PRIORITY_AUTO: _GPUInferencePriority.ValueType  # 0
    GPU_PRIORITY_MAX_PRECISION: _GPUInferencePriority.ValueType  # 1
    GPU_PRIORITY_MIN_LATENCY: _GPUInferencePriority.ValueType  # 2
    GPU_PRIORITY_MIN_MEMORY_USAGE: _GPUInferencePriority.ValueType  # 3

class GPUInferencePriority(_GPUInferencePriority, metaclass=_GPUInferencePriorityEnumTypeWrapper):
    """GPU inference priorities define relative priorities given by the GPU delegate
    to different client needs.
    Corresponds to TfLiteGpuInferencePriority.
    """

GPU_PRIORITY_AUTO: GPUInferencePriority.ValueType  # 0
GPU_PRIORITY_MAX_PRECISION: GPUInferencePriority.ValueType  # 1
GPU_PRIORITY_MIN_LATENCY: GPUInferencePriority.ValueType  # 2
GPU_PRIORITY_MIN_MEMORY_USAGE: GPUInferencePriority.ValueType  # 3
global___GPUInferencePriority = GPUInferencePriority

class _GPUInferenceUsage:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _GPUInferenceUsageEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_GPUInferenceUsage.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER: _GPUInferenceUsage.ValueType  # 0
    """Delegate will be used only once, therefore, bootstrap/init time should
    be taken into account.
    """
    GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED: _GPUInferenceUsage.ValueType  # 1
    """Prefer maximizing the throughput. Same delegate will be used repeatedly on
    multiple inputs.
    """

class GPUInferenceUsage(_GPUInferenceUsage, metaclass=_GPUInferenceUsageEnumTypeWrapper):
    """GPU inference preference for initialization time vs. inference time.
    Corresponds to TfLiteGpuInferenceUsage.
    """

GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER: GPUInferenceUsage.ValueType  # 0
"""Delegate will be used only once, therefore, bootstrap/init time should
be taken into account.
"""
GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED: GPUInferenceUsage.ValueType  # 1
"""Prefer maximizing the throughput. Same delegate will be used repeatedly on
multiple inputs.
"""
global___GPUInferenceUsage = GPUInferenceUsage

class _XNNPackFlags:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _XNNPackFlagsEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_XNNPackFlags.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    TFLITE_XNNPACK_DELEGATE_NO_FLAGS: _XNNPackFlags.ValueType  # 0
    """These flags match the flags in xnnpack_delegate.h."""
    TFLITE_XNNPACK_DELEGATE_FLAG_QS8: _XNNPackFlags.ValueType  # 1
    """Enable fast signed integer XNNpack kernels."""
    TFLITE_XNNPACK_DELEGATE_FLAG_QU8: _XNNPackFlags.ValueType  # 2
    """Enable fast unsigned integer XNNpack kernels."""
    TFLITE_XNNPACK_DELEGATE_FLAG_QS8_QU8: _XNNPackFlags.ValueType  # 3
    """Enable both, signed and unsigned integer XNNpack kernels."""
    TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16: _XNNPackFlags.ValueType  # 4
    """Force 16-bit floating point inference."""

class XNNPackFlags(_XNNPackFlags, metaclass=_XNNPackFlagsEnumTypeWrapper):
    """XNNPack Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h
    """

TFLITE_XNNPACK_DELEGATE_NO_FLAGS: XNNPackFlags.ValueType  # 0
"""These flags match the flags in xnnpack_delegate.h."""
TFLITE_XNNPACK_DELEGATE_FLAG_QS8: XNNPackFlags.ValueType  # 1
"""Enable fast signed integer XNNpack kernels."""
TFLITE_XNNPACK_DELEGATE_FLAG_QU8: XNNPackFlags.ValueType  # 2
"""Enable fast unsigned integer XNNpack kernels."""
TFLITE_XNNPACK_DELEGATE_FLAG_QS8_QU8: XNNPackFlags.ValueType  # 3
"""Enable both, signed and unsigned integer XNNpack kernels."""
TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16: XNNPackFlags.ValueType  # 4
"""Force 16-bit floating point inference."""
global___XNNPackFlags = XNNPackFlags

class _EdgeTpuPowerState:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _EdgeTpuPowerStateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_EdgeTpuPowerState.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED_POWERSTATE: _EdgeTpuPowerState.ValueType  # 0
    """Undefined power state."""
    TPU_CORE_OFF: _EdgeTpuPowerState.ValueType  # 1
    """TPU core is off but control cluster is on."""
    READY: _EdgeTpuPowerState.ValueType  # 2
    """A non-active low-power state that has much smaller transition time to
    active compared to off.
    """
    ACTIVE_MIN_POWER: _EdgeTpuPowerState.ValueType  # 3
    """Minimum power active state."""
    ACTIVE_VERY_LOW_POWER: _EdgeTpuPowerState.ValueType  # 4
    """Very low performance, very low power."""
    ACTIVE_LOW_POWER: _EdgeTpuPowerState.ValueType  # 5
    """Low performance, low power."""
    ACTIVE: _EdgeTpuPowerState.ValueType  # 6
    """The normal performance and power. This setting usually provides the
    optimal perf/power trade-off for the average use-case.
    """
    OVER_DRIVE: _EdgeTpuPowerState.ValueType  # 7
    """Maximum performance level. Potentially higher power and thermal. This
    setting may not be allowed in production depending on the system.
    """

class EdgeTpuPowerState(_EdgeTpuPowerState, metaclass=_EdgeTpuPowerStateEnumTypeWrapper):
    """Generic definitions of EdgeTPU power states."""

UNDEFINED_POWERSTATE: EdgeTpuPowerState.ValueType  # 0
"""Undefined power state."""
TPU_CORE_OFF: EdgeTpuPowerState.ValueType  # 1
"""TPU core is off but control cluster is on."""
READY: EdgeTpuPowerState.ValueType  # 2
"""A non-active low-power state that has much smaller transition time to
active compared to off.
"""
ACTIVE_MIN_POWER: EdgeTpuPowerState.ValueType  # 3
"""Minimum power active state."""
ACTIVE_VERY_LOW_POWER: EdgeTpuPowerState.ValueType  # 4
"""Very low performance, very low power."""
ACTIVE_LOW_POWER: EdgeTpuPowerState.ValueType  # 5
"""Low performance, low power."""
ACTIVE: EdgeTpuPowerState.ValueType  # 6
"""The normal performance and power. This setting usually provides the
optimal perf/power trade-off for the average use-case.
"""
OVER_DRIVE: EdgeTpuPowerState.ValueType  # 7
"""Maximum performance level. Potentially higher power and thermal. This
setting may not be allowed in production depending on the system.
"""
global___EdgeTpuPowerState = EdgeTpuPowerState

class _BenchmarkEventType:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _BenchmarkEventTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_BenchmarkEventType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED_BENCHMARK_EVENT_TYPE: _BenchmarkEventType.ValueType  # 0
    START: _BenchmarkEventType.ValueType  # 1
    """Benchmark start. A start without an end can be interpreted as a test that
    has crashed or hung.
    """
    END: _BenchmarkEventType.ValueType  # 2
    """Benchmarking completion. A model was successfully loaded, acceleration
    configured and inference run without errors. There may still be an issue
    with correctness of results, or with performance.
    """
    ERROR: _BenchmarkEventType.ValueType  # 3
    """Benchmark was not completed due to an error. The error may be a handled
    error (e.g., failure in a delegate), or a crash.
    """
    LOGGED: _BenchmarkEventType.ValueType  # 4
    """Benchmark data has been sent for logging."""
    RECOVERED_ERROR: _BenchmarkEventType.ValueType  # 5
    """Benchmark encountered an error but was able to continue. The error is not
    related to the model execution but to the mini-benchmark logic. An example
    of error is a failure when trying to set the CPU affinity of the benchmark
    runner process.
    """

class BenchmarkEventType(_BenchmarkEventType, metaclass=_BenchmarkEventTypeEnumTypeWrapper):
    """On-device mini-benchmark result storage. The following definitions are used
    to keep an append-only log of benchmark results on-device. (Hence there is
    single top-level event that is used for all data).

    These definitions don't need a proto-to-flatbuffer conversion, since they are
    not used for specifying configuration in the Tasks library.

    Which stage of benchmarking the event is for.
    There might be multiple events with the same type, if a benchmark is run
    multiple times.
    """

UNDEFINED_BENCHMARK_EVENT_TYPE: BenchmarkEventType.ValueType  # 0
START: BenchmarkEventType.ValueType  # 1
"""Benchmark start. A start without an end can be interpreted as a test that
has crashed or hung.
"""
END: BenchmarkEventType.ValueType  # 2
"""Benchmarking completion. A model was successfully loaded, acceleration
configured and inference run without errors. There may still be an issue
with correctness of results, or with performance.
"""
ERROR: BenchmarkEventType.ValueType  # 3
"""Benchmark was not completed due to an error. The error may be a handled
error (e.g., failure in a delegate), or a crash.
"""
LOGGED: BenchmarkEventType.ValueType  # 4
"""Benchmark data has been sent for logging."""
RECOVERED_ERROR: BenchmarkEventType.ValueType  # 5
"""Benchmark encountered an error but was able to continue. The error is not
related to the model execution but to the mini-benchmark logic. An example
of error is a failure when trying to set the CPU affinity of the benchmark
runner process.
"""
global___BenchmarkEventType = BenchmarkEventType

class _BenchmarkStage:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _BenchmarkStageEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_BenchmarkStage.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNKNOWN: _BenchmarkStage.ValueType  # 0
    INITIALIZATION: _BenchmarkStage.ValueType  # 1
    """During model loading or delegation."""
    INFERENCE: _BenchmarkStage.ValueType  # 2
    """During inference."""

class BenchmarkStage(_BenchmarkStage, metaclass=_BenchmarkStageEnumTypeWrapper):
    """When during benchmark execution an error occurred."""

UNKNOWN: BenchmarkStage.ValueType  # 0
INITIALIZATION: BenchmarkStage.ValueType  # 1
"""During model loading or delegation."""
INFERENCE: BenchmarkStage.ValueType  # 2
"""During inference."""
global___BenchmarkStage = BenchmarkStage

@typing_extensions.final
class ComputeSettings(google.protobuf.message.Message):
    """One possible acceleration configuration."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    PREFERENCE_FIELD_NUMBER: builtins.int
    TFLITE_SETTINGS_FIELD_NUMBER: builtins.int
    MODEL_NAMESPACE_FOR_STATISTICS_FIELD_NUMBER: builtins.int
    MODEL_IDENTIFIER_FOR_STATISTICS_FIELD_NUMBER: builtins.int
    SETTINGS_TO_TEST_LOCALLY_FIELD_NUMBER: builtins.int
    preference: global___ExecutionPreference.ValueType
    """Which preference to use this accelerator for."""
    @property
    def tflite_settings(self) -> global___TFLiteSettings:
        """How to configure TFLite"""
    model_namespace_for_statistics: builtins.str
    """Identifiers to use for instrumentation and telemetry."""
    model_identifier_for_statistics: builtins.str
    @property
    def settings_to_test_locally(self) -> global___MinibenchmarkSettings:
        """'Maybe' acceleration: use mini-benchmark to select settings."""
    def __init__(
        self,
        *,
        preference: global___ExecutionPreference.ValueType | None = ...,
        tflite_settings: global___TFLiteSettings | None = ...,
        model_namespace_for_statistics: builtins.str | None = ...,
        model_identifier_for_statistics: builtins.str | None = ...,
        settings_to_test_locally: global___MinibenchmarkSettings | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["model_identifier_for_statistics", b"model_identifier_for_statistics", "model_namespace_for_statistics", b"model_namespace_for_statistics", "preference", b"preference", "settings_to_test_locally", b"settings_to_test_locally", "tflite_settings", b"tflite_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_identifier_for_statistics", b"model_identifier_for_statistics", "model_namespace_for_statistics", b"model_namespace_for_statistics", "preference", b"preference", "settings_to_test_locally", b"settings_to_test_locally", "tflite_settings", b"tflite_settings"]) -> None: ...

global___ComputeSettings = ComputeSettings

@typing_extensions.final
class NNAPISettings(google.protobuf.message.Message):
    """NNAPI delegate settings."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ACCELERATOR_NAME_FIELD_NUMBER: builtins.int
    CACHE_DIRECTORY_FIELD_NUMBER: builtins.int
    MODEL_TOKEN_FIELD_NUMBER: builtins.int
    EXECUTION_PREFERENCE_FIELD_NUMBER: builtins.int
    NO_OF_NNAPI_INSTANCES_TO_CACHE_FIELD_NUMBER: builtins.int
    FALLBACK_SETTINGS_FIELD_NUMBER: builtins.int
    ALLOW_NNAPI_CPU_ON_ANDROID_10_PLUS_FIELD_NUMBER: builtins.int
    EXECUTION_PRIORITY_FIELD_NUMBER: builtins.int
    ALLOW_DYNAMIC_DIMENSIONS_FIELD_NUMBER: builtins.int
    ALLOW_FP16_PRECISION_FOR_FP32_FIELD_NUMBER: builtins.int
    USE_BURST_COMPUTATION_FIELD_NUMBER: builtins.int
    SUPPORT_LIBRARY_HANDLE_FIELD_NUMBER: builtins.int
    accelerator_name: builtins.str
    """Which instance (NNAPI accelerator) to use. One driver may provide several
    accelerators (though a driver may also hide several back-ends behind one
    name, at the choice of the driver vendor).
    Note that driver introspection is only available in Android Q and later.
    """
    cache_directory: builtins.str
    """NNAPI model compilation caching settings to be passed to
    tflite::StatefulNnApiDelegate
    """
    model_token: builtins.str
    execution_preference: global___NNAPIExecutionPreference.ValueType
    """NNAPI execution preference to pass. See
    https://developer.android.com/ndk/reference/group/neural-networks.html
    """
    no_of_nnapi_instances_to_cache: builtins.int
    """Number of instances to cache for the same model (for input size
    changes). This is mandatory for getting reasonable performance in that
    case.
    """
    @property
    def fallback_settings(self) -> global___FallbackSettings:
        """Deprecated; use the fallback_settings in TFLiteSettings.

        Whether to automatically fall back to TFLite CPU path.
        """
    allow_nnapi_cpu_on_android_10_plus: builtins.bool
    """Whether to allow use of NNAPI CPU (nnapi-reference accelerator) on Android
    10+ when an accelerator name is not specified. The NNAPI CPU typically
    performs less well than the TfLite built-in kernels; but allowing allows a
    model to be partially accelerated which may be a win.
    """
    execution_priority: global___NNAPIExecutionPriority.ValueType
    allow_dynamic_dimensions: builtins.bool
    """Whether to allow dynamic dimension sizes without re-compilation.
    A tensor of with dynamic dimension must have a valid dims_signature
    defined.
    Only supported in NNAPI 1.1 and newer versions.
    WARNING: Setting this flag to true may result in model being rejected by
    accelerator. This should only be enabled if the target device supports
    dynamic dimensions of the model.
    By default this is set to false.
    """
    allow_fp16_precision_for_fp32: builtins.bool
    """Whether to allow the NNAPI accelerator to optionally use lower-precision
    float16 (16-bit floating point) arithmetic when doing calculations on
    float32 (32-bit floating point).
    """
    use_burst_computation: builtins.bool
    """Whether to use NNAPI Burst mode.
    Burst mode allows accelerators to efficiently manage resources, which
    would significantly reduce overhead especially if the same delegate
    instance is to be used for multiple inferences.
    """
    support_library_handle: builtins.int
    """Optional pointer to NNAPI Support Library provided pointer to
    NnApiSLDriverImplFL5 which can be used to construct the
    NNAPI delegate.
    """
    def __init__(
        self,
        *,
        accelerator_name: builtins.str | None = ...,
        cache_directory: builtins.str | None = ...,
        model_token: builtins.str | None = ...,
        execution_preference: global___NNAPIExecutionPreference.ValueType | None = ...,
        no_of_nnapi_instances_to_cache: builtins.int | None = ...,
        fallback_settings: global___FallbackSettings | None = ...,
        allow_nnapi_cpu_on_android_10_plus: builtins.bool | None = ...,
        execution_priority: global___NNAPIExecutionPriority.ValueType | None = ...,
        allow_dynamic_dimensions: builtins.bool | None = ...,
        allow_fp16_precision_for_fp32: builtins.bool | None = ...,
        use_burst_computation: builtins.bool | None = ...,
        support_library_handle: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["accelerator_name", b"accelerator_name", "allow_dynamic_dimensions", b"allow_dynamic_dimensions", "allow_fp16_precision_for_fp32", b"allow_fp16_precision_for_fp32", "allow_nnapi_cpu_on_android_10_plus", b"allow_nnapi_cpu_on_android_10_plus", "cache_directory", b"cache_directory", "execution_preference", b"execution_preference", "execution_priority", b"execution_priority", "fallback_settings", b"fallback_settings", "model_token", b"model_token", "no_of_nnapi_instances_to_cache", b"no_of_nnapi_instances_to_cache", "support_library_handle", b"support_library_handle", "use_burst_computation", b"use_burst_computation"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["accelerator_name", b"accelerator_name", "allow_dynamic_dimensions", b"allow_dynamic_dimensions", "allow_fp16_precision_for_fp32", b"allow_fp16_precision_for_fp32", "allow_nnapi_cpu_on_android_10_plus", b"allow_nnapi_cpu_on_android_10_plus", "cache_directory", b"cache_directory", "execution_preference", b"execution_preference", "execution_priority", b"execution_priority", "fallback_settings", b"fallback_settings", "model_token", b"model_token", "no_of_nnapi_instances_to_cache", b"no_of_nnapi_instances_to_cache", "support_library_handle", b"support_library_handle", "use_burst_computation", b"use_burst_computation"]) -> None: ...

global___NNAPISettings = NNAPISettings

@typing_extensions.final
class GPUSettings(google.protobuf.message.Message):
    """GPU Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    IS_PRECISION_LOSS_ALLOWED_FIELD_NUMBER: builtins.int
    ENABLE_QUANTIZED_INFERENCE_FIELD_NUMBER: builtins.int
    FORCE_BACKEND_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY1_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY2_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY3_FIELD_NUMBER: builtins.int
    INFERENCE_PREFERENCE_FIELD_NUMBER: builtins.int
    CACHE_DIRECTORY_FIELD_NUMBER: builtins.int
    MODEL_TOKEN_FIELD_NUMBER: builtins.int
    is_precision_loss_allowed: builtins.bool
    """Ignored if inference_priority1/2/3 are set."""
    enable_quantized_inference: builtins.bool
    force_backend: global___GPUBackend.ValueType
    inference_priority1: global___GPUInferencePriority.ValueType
    """Ordered priorities provide better control over desired semantics,
    where priority(n) is more important than priority(n+1). Therefore,
    each time inference engine needs to make a decision, it uses
    ordered priorities to do so.

    Default values correspond to GPU_PRIORITY_AUTO.
    AUTO priority can only be used when higher priorities are fully specified.
    For example:
      VALID:   priority1 = MIN_LATENCY, priority2 = AUTO, priority3 = AUTO
      VALID:   priority1 = MIN_LATENCY, priority2 = MAX_PRECISION,
               priority3 = AUTO
      INVALID: priority1 = AUTO, priority2 = MIN_LATENCY, priority3 = AUTO
      INVALID: priority1 = MIN_LATENCY, priority2 = AUTO,
               priority3 = MAX_PRECISION
    Invalid priorities will result in error.

    For more information, see TfLiteGpuDelegateOptionsV2.
    """
    inference_priority2: global___GPUInferencePriority.ValueType
    inference_priority3: global___GPUInferencePriority.ValueType
    inference_preference: global___GPUInferenceUsage.ValueType
    """Whether to optimize for compilation+execution time or execution time only."""
    cache_directory: builtins.str
    """Model serialization. Setting both of these fields will also set the
    TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_SERIALIZATION flag on the delegate.

    GPU model serialization directory passed in TfLiteGpuDelegateOptionsV2.
    This should be set to the application's code cache directory so that it can
    not be accessed by other apps and is correctly deleted on app updates.
    tflite::StatefulNnApiDelegate
    """
    model_token: builtins.str
    """Normally, the model name with version number should be provided here, since
    each model needs an unique ID to avoid cache collision.
    """
    def __init__(
        self,
        *,
        is_precision_loss_allowed: builtins.bool | None = ...,
        enable_quantized_inference: builtins.bool | None = ...,
        force_backend: global___GPUBackend.ValueType | None = ...,
        inference_priority1: global___GPUInferencePriority.ValueType | None = ...,
        inference_priority2: global___GPUInferencePriority.ValueType | None = ...,
        inference_priority3: global___GPUInferencePriority.ValueType | None = ...,
        inference_preference: global___GPUInferenceUsage.ValueType | None = ...,
        cache_directory: builtins.str | None = ...,
        model_token: builtins.str | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["cache_directory", b"cache_directory", "enable_quantized_inference", b"enable_quantized_inference", "force_backend", b"force_backend", "inference_preference", b"inference_preference", "inference_priority1", b"inference_priority1", "inference_priority2", b"inference_priority2", "inference_priority3", b"inference_priority3", "is_precision_loss_allowed", b"is_precision_loss_allowed", "model_token", b"model_token"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cache_directory", b"cache_directory", "enable_quantized_inference", b"enable_quantized_inference", "force_backend", b"force_backend", "inference_preference", b"inference_preference", "inference_priority1", b"inference_priority1", "inference_priority2", b"inference_priority2", "inference_priority3", b"inference_priority3", "is_precision_loss_allowed", b"is_precision_loss_allowed", "model_token", b"model_token"]) -> None: ...

global___GPUSettings = GPUSettings

@typing_extensions.final
class HexagonSettings(google.protobuf.message.Message):
    """Hexagon Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.h
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DEBUG_LEVEL_FIELD_NUMBER: builtins.int
    POWERSAVE_LEVEL_FIELD_NUMBER: builtins.int
    PRINT_GRAPH_PROFILE_FIELD_NUMBER: builtins.int
    PRINT_GRAPH_DEBUG_FIELD_NUMBER: builtins.int
    debug_level: builtins.int
    powersave_level: builtins.int
    print_graph_profile: builtins.bool
    print_graph_debug: builtins.bool
    def __init__(
        self,
        *,
        debug_level: builtins.int | None = ...,
        powersave_level: builtins.int | None = ...,
        print_graph_profile: builtins.bool | None = ...,
        print_graph_debug: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["debug_level", b"debug_level", "powersave_level", b"powersave_level", "print_graph_debug", b"print_graph_debug", "print_graph_profile", b"print_graph_profile"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["debug_level", b"debug_level", "powersave_level", b"powersave_level", "print_graph_debug", b"print_graph_debug", "print_graph_profile", b"print_graph_profile"]) -> None: ...

global___HexagonSettings = HexagonSettings

@typing_extensions.final
class XNNPackSettings(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NUM_THREADS_FIELD_NUMBER: builtins.int
    FLAGS_FIELD_NUMBER: builtins.int
    num_threads: builtins.int
    flags: global___XNNPackFlags.ValueType
    def __init__(
        self,
        *,
        num_threads: builtins.int | None = ...,
        flags: global___XNNPackFlags.ValueType | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["flags", b"flags", "num_threads", b"num_threads"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["flags", b"flags", "num_threads", b"num_threads"]) -> None: ...

global___XNNPackSettings = XNNPackSettings

@typing_extensions.final
class CoreMLSettings(google.protobuf.message.Message):
    """CoreML Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate.h
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _EnabledDevices:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _EnabledDevicesEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[CoreMLSettings._EnabledDevices.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DEVICES_ALL: CoreMLSettings._EnabledDevices.ValueType  # 0
        """Always create Core ML delegate."""
        DEVICES_WITH_NEURAL_ENGINE: CoreMLSettings._EnabledDevices.ValueType  # 1
        """Create Core ML delegate only on devices with Apple Neural Engine."""

    class EnabledDevices(_EnabledDevices, metaclass=_EnabledDevicesEnumTypeWrapper):
        """Note the enum order change from the above header for better proto practice."""

    DEVICES_ALL: CoreMLSettings.EnabledDevices.ValueType  # 0
    """Always create Core ML delegate."""
    DEVICES_WITH_NEURAL_ENGINE: CoreMLSettings.EnabledDevices.ValueType  # 1
    """Create Core ML delegate only on devices with Apple Neural Engine."""

    ENABLED_DEVICES_FIELD_NUMBER: builtins.int
    COREML_VERSION_FIELD_NUMBER: builtins.int
    MAX_DELEGATED_PARTITIONS_FIELD_NUMBER: builtins.int
    MIN_NODES_PER_PARTITION_FIELD_NUMBER: builtins.int
    enabled_devices: global___CoreMLSettings.EnabledDevices.ValueType
    """Only create delegate when Neural Engine is available on the device."""
    coreml_version: builtins.int
    """Specifies target Core ML version for model conversion.
    Core ML 3 come with a lot more ops, but some ops (e.g. reshape) is not
    delegated due to input rank constraint.
    if not set to one of the valid versions, the delegate will use highest
    version possible in the platform.
    Valid versions: (2, 3)
    """
    max_delegated_partitions: builtins.int
    """This sets the maximum number of Core ML delegates created.
    Each graph corresponds to one delegated node subset in the
    TFLite model. Set this to 0 to delegate all possible partitions.
    """
    min_nodes_per_partition: builtins.int
    """This sets the minimum number of nodes per partition delegated with
    Core ML delegate. Defaults to 2.
    """
    def __init__(
        self,
        *,
        enabled_devices: global___CoreMLSettings.EnabledDevices.ValueType | None = ...,
        coreml_version: builtins.int | None = ...,
        max_delegated_partitions: builtins.int | None = ...,
        min_nodes_per_partition: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["coreml_version", b"coreml_version", "enabled_devices", b"enabled_devices", "max_delegated_partitions", b"max_delegated_partitions", "min_nodes_per_partition", b"min_nodes_per_partition"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["coreml_version", b"coreml_version", "enabled_devices", b"enabled_devices", "max_delegated_partitions", b"max_delegated_partitions", "min_nodes_per_partition", b"min_nodes_per_partition"]) -> None: ...

global___CoreMLSettings = CoreMLSettings

@typing_extensions.final
class EdgeTpuDeviceSpec(google.protobuf.message.Message):
    """EdgeTPU device spec."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _PlatformType:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _PlatformTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[EdgeTpuDeviceSpec._PlatformType.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        MMIO: EdgeTpuDeviceSpec._PlatformType.ValueType  # 0
        REFERENCE: EdgeTpuDeviceSpec._PlatformType.ValueType  # 1
        SIMULATOR: EdgeTpuDeviceSpec._PlatformType.ValueType  # 2
        REMOTE_SIMULATOR: EdgeTpuDeviceSpec._PlatformType.ValueType  # 3

    class PlatformType(_PlatformType, metaclass=_PlatformTypeEnumTypeWrapper):
        """EdgeTPU platform types."""

    MMIO: EdgeTpuDeviceSpec.PlatformType.ValueType  # 0
    REFERENCE: EdgeTpuDeviceSpec.PlatformType.ValueType  # 1
    SIMULATOR: EdgeTpuDeviceSpec.PlatformType.ValueType  # 2
    REMOTE_SIMULATOR: EdgeTpuDeviceSpec.PlatformType.ValueType  # 3

    PLATFORM_TYPE_FIELD_NUMBER: builtins.int
    NUM_CHIPS_FIELD_NUMBER: builtins.int
    DEVICE_PATHS_FIELD_NUMBER: builtins.int
    CHIP_FAMILY_FIELD_NUMBER: builtins.int
    platform_type: global___EdgeTpuDeviceSpec.PlatformType.ValueType
    """Execution platform for the EdgeTPU device."""
    num_chips: builtins.int
    """Number of chips to use for the EdgeTPU device."""
    @property
    def device_paths(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """Paths to the EdgeTPU devices;"""
    chip_family: builtins.int
    """Chip family used by the EdgeTpu device."""
    def __init__(
        self,
        *,
        platform_type: global___EdgeTpuDeviceSpec.PlatformType.ValueType | None = ...,
        num_chips: builtins.int | None = ...,
        device_paths: collections.abc.Iterable[builtins.str] | None = ...,
        chip_family: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["chip_family", b"chip_family", "num_chips", b"num_chips", "platform_type", b"platform_type"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["chip_family", b"chip_family", "device_paths", b"device_paths", "num_chips", b"num_chips", "platform_type", b"platform_type"]) -> None: ...

global___EdgeTpuDeviceSpec = EdgeTpuDeviceSpec

@typing_extensions.final
class EdgeTpuInactivePowerConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INACTIVE_POWER_STATE_FIELD_NUMBER: builtins.int
    INACTIVE_TIMEOUT_US_FIELD_NUMBER: builtins.int
    inactive_power_state: global___EdgeTpuPowerState.ValueType
    """Inactive power states between inferences."""
    inactive_timeout_us: builtins.int
    """Inactive timeout in microseconds between inferences."""
    def __init__(
        self,
        *,
        inactive_power_state: global___EdgeTpuPowerState.ValueType | None = ...,
        inactive_timeout_us: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["inactive_power_state", b"inactive_power_state", "inactive_timeout_us", b"inactive_timeout_us"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["inactive_power_state", b"inactive_power_state", "inactive_timeout_us", b"inactive_timeout_us"]) -> None: ...

global___EdgeTpuInactivePowerConfig = EdgeTpuInactivePowerConfig

@typing_extensions.final
class EdgeTpuSettings(google.protobuf.message.Message):
    """EdgeTPU Delegate settings."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _FloatTruncationType:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _FloatTruncationTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[EdgeTpuSettings._FloatTruncationType.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNSPECIFIED: EdgeTpuSettings._FloatTruncationType.ValueType  # 0
        NO_TRUNCATION: EdgeTpuSettings._FloatTruncationType.ValueType  # 1
        BFLOAT16: EdgeTpuSettings._FloatTruncationType.ValueType  # 2
        HALF: EdgeTpuSettings._FloatTruncationType.ValueType  # 3

    class FloatTruncationType(_FloatTruncationType, metaclass=_FloatTruncationTypeEnumTypeWrapper):
        """Float truncation types for EdgeTPU."""

    UNSPECIFIED: EdgeTpuSettings.FloatTruncationType.ValueType  # 0
    NO_TRUNCATION: EdgeTpuSettings.FloatTruncationType.ValueType  # 1
    BFLOAT16: EdgeTpuSettings.FloatTruncationType.ValueType  # 2
    HALF: EdgeTpuSettings.FloatTruncationType.ValueType  # 3

    class _QosClass:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _QosClassEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[EdgeTpuSettings._QosClass.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        QOS_UNDEFINED: EdgeTpuSettings._QosClass.ValueType  # 0
        BEST_EFFORT: EdgeTpuSettings._QosClass.ValueType  # 1
        REALTIME: EdgeTpuSettings._QosClass.ValueType  # 2

    class QosClass(_QosClass, metaclass=_QosClassEnumTypeWrapper): ...
    QOS_UNDEFINED: EdgeTpuSettings.QosClass.ValueType  # 0
    BEST_EFFORT: EdgeTpuSettings.QosClass.ValueType  # 1
    REALTIME: EdgeTpuSettings.QosClass.ValueType  # 2

    INFERENCE_POWER_STATE_FIELD_NUMBER: builtins.int
    INACTIVE_POWER_CONFIGS_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY_FIELD_NUMBER: builtins.int
    EDGETPU_DEVICE_SPEC_FIELD_NUMBER: builtins.int
    MODEL_TOKEN_FIELD_NUMBER: builtins.int
    FLOAT_TRUNCATION_TYPE_FIELD_NUMBER: builtins.int
    QOS_CLASS_FIELD_NUMBER: builtins.int
    inference_power_state: global___EdgeTpuPowerState.ValueType
    """Target inference power state for running the model."""
    @property
    def inactive_power_configs(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EdgeTpuInactivePowerConfig]:
        """Inactive power states between inferences."""
    inference_priority: builtins.int
    """Priority for the inference request."""
    @property
    def edgetpu_device_spec(self) -> global___EdgeTpuDeviceSpec:
        """Device spec for creating the EdgeTpu device."""
    model_token: builtins.str
    """A unique identifier of the input TfLite model."""
    float_truncation_type: global___EdgeTpuSettings.FloatTruncationType.ValueType
    """Float truncation type for EdgeTPU."""
    qos_class: global___EdgeTpuSettings.QosClass.ValueType
    """QoS class to determine chunking size for PRO onward."""
    def __init__(
        self,
        *,
        inference_power_state: global___EdgeTpuPowerState.ValueType | None = ...,
        inactive_power_configs: collections.abc.Iterable[global___EdgeTpuInactivePowerConfig] | None = ...,
        inference_priority: builtins.int | None = ...,
        edgetpu_device_spec: global___EdgeTpuDeviceSpec | None = ...,
        model_token: builtins.str | None = ...,
        float_truncation_type: global___EdgeTpuSettings.FloatTruncationType.ValueType | None = ...,
        qos_class: global___EdgeTpuSettings.QosClass.ValueType | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["edgetpu_device_spec", b"edgetpu_device_spec", "float_truncation_type", b"float_truncation_type", "inference_power_state", b"inference_power_state", "inference_priority", b"inference_priority", "model_token", b"model_token", "qos_class", b"qos_class"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["edgetpu_device_spec", b"edgetpu_device_spec", "float_truncation_type", b"float_truncation_type", "inactive_power_configs", b"inactive_power_configs", "inference_power_state", b"inference_power_state", "inference_priority", b"inference_priority", "model_token", b"model_token", "qos_class", b"qos_class"]) -> None: ...

global___EdgeTpuSettings = EdgeTpuSettings

@typing_extensions.final
class CoralSettings(google.protobuf.message.Message):
    """Coral Dev Board / USB accelerator delegate settings.

    See
    https://github.com/google-coral/edgetpu/blob/master/libedgetpu/edgetpu_c.h
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _Performance:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _PerformanceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[CoralSettings._Performance.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNDEFINED: CoralSettings._Performance.ValueType  # 0
        MAXIMUM: CoralSettings._Performance.ValueType  # 1
        HIGH: CoralSettings._Performance.ValueType  # 2
        MEDIUM: CoralSettings._Performance.ValueType  # 3
        LOW: CoralSettings._Performance.ValueType  # 4

    class Performance(_Performance, metaclass=_PerformanceEnumTypeWrapper): ...
    UNDEFINED: CoralSettings.Performance.ValueType  # 0
    MAXIMUM: CoralSettings.Performance.ValueType  # 1
    HIGH: CoralSettings.Performance.ValueType  # 2
    MEDIUM: CoralSettings.Performance.ValueType  # 3
    LOW: CoralSettings.Performance.ValueType  # 4

    DEVICE_FIELD_NUMBER: builtins.int
    PERFORMANCE_FIELD_NUMBER: builtins.int
    USB_ALWAYS_DFU_FIELD_NUMBER: builtins.int
    USB_MAX_BULK_IN_QUEUE_LENGTH_FIELD_NUMBER: builtins.int
    device: builtins.str
    """The Edge Tpu device to be used. See
    https://github.com/google-coral/libcoral/blob/982426546dfa10128376d0c24fd8a8b161daac97/coral/tflite_utils.h#L131-L137
    """
    performance: global___CoralSettings.Performance.ValueType
    """The desired performance level. This setting adjusts the internal clock
    rate to achieve different performance / power balance. Higher performance
    values improve speed, but increase power usage.
    """
    usb_always_dfu: builtins.bool
    """If true, always perform device firmware update (DFU) after reset. DFU is
    usually only necessary after power cycle.
    """
    usb_max_bulk_in_queue_length: builtins.int
    """The maximum bulk in queue length. Larger queue length may improve USB
    performance on the direction from device to host. When not specified (or
    zero), `usb_max_bulk_in_queue_length` will default to 32 according to the
    current EdgeTpu Coral implementation.
    """
    def __init__(
        self,
        *,
        device: builtins.str | None = ...,
        performance: global___CoralSettings.Performance.ValueType | None = ...,
        usb_always_dfu: builtins.bool | None = ...,
        usb_max_bulk_in_queue_length: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device", b"device", "performance", b"performance", "usb_always_dfu", b"usb_always_dfu", "usb_max_bulk_in_queue_length", b"usb_max_bulk_in_queue_length"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device", b"device", "performance", b"performance", "usb_always_dfu", b"usb_always_dfu", "usb_max_bulk_in_queue_length", b"usb_max_bulk_in_queue_length"]) -> None: ...

global___CoralSettings = CoralSettings

@typing_extensions.final
class CPUSettings(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NUM_THREADS_FIELD_NUMBER: builtins.int
    num_threads: builtins.int
    """Set to -1 to let the interpreter choose. Otherwise, must be > 0."""
    def __init__(
        self,
        *,
        num_threads: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["num_threads", b"num_threads"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["num_threads", b"num_threads"]) -> None: ...

global___CPUSettings = CPUSettings

@typing_extensions.final
class TFLiteSettings(google.protobuf.message.Message):
    """How to configure TFLite."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DELEGATE_FIELD_NUMBER: builtins.int
    NNAPI_SETTINGS_FIELD_NUMBER: builtins.int
    GPU_SETTINGS_FIELD_NUMBER: builtins.int
    HEXAGON_SETTINGS_FIELD_NUMBER: builtins.int
    XNNPACK_SETTINGS_FIELD_NUMBER: builtins.int
    COREML_SETTINGS_FIELD_NUMBER: builtins.int
    CPU_SETTINGS_FIELD_NUMBER: builtins.int
    MAX_DELEGATED_PARTITIONS_FIELD_NUMBER: builtins.int
    EDGETPU_SETTINGS_FIELD_NUMBER: builtins.int
    CORAL_SETTINGS_FIELD_NUMBER: builtins.int
    FALLBACK_SETTINGS_FIELD_NUMBER: builtins.int
    DISABLE_DEFAULT_DELEGATES_FIELD_NUMBER: builtins.int
    delegate: global___Delegate.ValueType
    """Which delegate to use."""
    @property
    def nnapi_settings(self) -> global___NNAPISettings:
        """How to configure the chosen delegate.
        (In principle we would like to use 'oneof', but flatc turns that into an
        nested anonymous table rather than a union. See
        https://github.com/google/flatbuffers/issues/4628).
        """
    @property
    def gpu_settings(self) -> global___GPUSettings: ...
    @property
    def hexagon_settings(self) -> global___HexagonSettings: ...
    @property
    def xnnpack_settings(self) -> global___XNNPackSettings: ...
    @property
    def coreml_settings(self) -> global___CoreMLSettings: ...
    @property
    def cpu_settings(self) -> global___CPUSettings:
        """How to configure CPU execution."""
    max_delegated_partitions: builtins.int
    """Shared delegation settings."""
    @property
    def edgetpu_settings(self) -> global___EdgeTpuSettings:
        """For configuring the EdgeTpuDelegate."""
    @property
    def coral_settings(self) -> global___CoralSettings:
        """For configuring the Coral EdgeTpu Delegate."""
    @property
    def fallback_settings(self) -> global___FallbackSettings:
        """Whether to automatically fall back to TFLite CPU path."""
    disable_default_delegates: builtins.bool
    """Whether to disable default delegates (XNNPack)."""
    def __init__(
        self,
        *,
        delegate: global___Delegate.ValueType | None = ...,
        nnapi_settings: global___NNAPISettings | None = ...,
        gpu_settings: global___GPUSettings | None = ...,
        hexagon_settings: global___HexagonSettings | None = ...,
        xnnpack_settings: global___XNNPackSettings | None = ...,
        coreml_settings: global___CoreMLSettings | None = ...,
        cpu_settings: global___CPUSettings | None = ...,
        max_delegated_partitions: builtins.int | None = ...,
        edgetpu_settings: global___EdgeTpuSettings | None = ...,
        coral_settings: global___CoralSettings | None = ...,
        fallback_settings: global___FallbackSettings | None = ...,
        disable_default_delegates: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["coral_settings", b"coral_settings", "coreml_settings", b"coreml_settings", "cpu_settings", b"cpu_settings", "delegate", b"delegate", "disable_default_delegates", b"disable_default_delegates", "edgetpu_settings", b"edgetpu_settings", "fallback_settings", b"fallback_settings", "gpu_settings", b"gpu_settings", "hexagon_settings", b"hexagon_settings", "max_delegated_partitions", b"max_delegated_partitions", "nnapi_settings", b"nnapi_settings", "xnnpack_settings", b"xnnpack_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["coral_settings", b"coral_settings", "coreml_settings", b"coreml_settings", "cpu_settings", b"cpu_settings", "delegate", b"delegate", "disable_default_delegates", b"disable_default_delegates", "edgetpu_settings", b"edgetpu_settings", "fallback_settings", b"fallback_settings", "gpu_settings", b"gpu_settings", "hexagon_settings", b"hexagon_settings", "max_delegated_partitions", b"max_delegated_partitions", "nnapi_settings", b"nnapi_settings", "xnnpack_settings", b"xnnpack_settings"]) -> None: ...

global___TFLiteSettings = TFLiteSettings

@typing_extensions.final
class FallbackSettings(google.protobuf.message.Message):
    """Whether to automatically fallback to TFLite CPU path on delegation errors.

    Typically fallback is enabled in production use but disabled in tests and
    benchmarks to ensure they test the intended path.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ALLOW_AUTOMATIC_FALLBACK_ON_COMPILATION_ERROR_FIELD_NUMBER: builtins.int
    ALLOW_AUTOMATIC_FALLBACK_ON_EXECUTION_ERROR_FIELD_NUMBER: builtins.int
    allow_automatic_fallback_on_compilation_error: builtins.bool
    """Whether to allow automatically falling back to TfLite CPU path on
    compilation failure. Default is not allowing automatic fallback.

    This is useful in naive production usecases where the caller would prefer
    for the model to run even if it's not accelerated. More advanced users will
    implement fallback themselves; e.g., by using a different model on CPU.

    Note that compilation errors may occur either at initial
    ModifyGraphWithDelegate() time, or when calling AllocateTensors() after
    resizing.
    """
    allow_automatic_fallback_on_execution_error: builtins.bool
    """Whether to allow automatically falling back to TfLite CPU path on
    execution error. Default is not allowing automatic fallback.

    Experimental, use with care (only when you have complete control over the
    client code).

    The caveat above for compilation error holds.  Additionally, execution-time
    errors are harder to handle automatically as they require invalidating the
    TfLite interpreter which most client code has not been designed to deal
    with.
    """
    def __init__(
        self,
        *,
        allow_automatic_fallback_on_compilation_error: builtins.bool | None = ...,
        allow_automatic_fallback_on_execution_error: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["allow_automatic_fallback_on_compilation_error", b"allow_automatic_fallback_on_compilation_error", "allow_automatic_fallback_on_execution_error", b"allow_automatic_fallback_on_execution_error"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["allow_automatic_fallback_on_compilation_error", b"allow_automatic_fallback_on_compilation_error", "allow_automatic_fallback_on_execution_error", b"allow_automatic_fallback_on_execution_error"]) -> None: ...

global___FallbackSettings = FallbackSettings

@typing_extensions.final
class BenchmarkMetric(google.protobuf.message.Message):
    """A correctness metric from a benchmark, for example KL-divergence between
    known-good CPU output and on-device output. These are primarily used for
    telemetry and monitored server-side.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NAME_FIELD_NUMBER: builtins.int
    VALUES_FIELD_NUMBER: builtins.int
    name: builtins.str
    @property
    def values(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
    def __init__(
        self,
        *,
        name: builtins.str | None = ...,
        values: collections.abc.Iterable[builtins.float] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["name", b"name"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["name", b"name", "values", b"values"]) -> None: ...

global___BenchmarkMetric = BenchmarkMetric

@typing_extensions.final
class BenchmarkResult(google.protobuf.message.Message):
    """Outcome of a successfully complete benchmark run. This information is
    intended to both be used on-device to select best compute configuration as
    well as sent to server for monitoring.

    Used with event type END.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INITIALIZATION_TIME_US_FIELD_NUMBER: builtins.int
    INFERENCE_TIME_US_FIELD_NUMBER: builtins.int
    MAX_MEMORY_KB_FIELD_NUMBER: builtins.int
    OK_FIELD_NUMBER: builtins.int
    METRICS_FIELD_NUMBER: builtins.int
    @property
    def initialization_time_us(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Time to load model and apply acceleration. Initialization may get run
        multiple times to get information on variance.
        """
    @property
    def inference_time_us(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Time to run inference (call Invoke()). Inference may get run multiple times
        to get information on variance.
        """
    max_memory_kb: builtins.int
    """Maximum memory used. Measures size of application heap (does not
    necessarily take into account driver-side allocation.
    """
    ok: builtins.bool
    """Whether the inference produced correct results (validation graph output
    'ok' for all test inputs). Used on-device to disallow configurations that
    produce incorrect results (e.g., due to OpenCL driver bugs).
    """
    @property
    def metrics(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___BenchmarkMetric]:
        """Metrics that were used to determine the 'ok' status."""
    def __init__(
        self,
        *,
        initialization_time_us: collections.abc.Iterable[builtins.int] | None = ...,
        inference_time_us: collections.abc.Iterable[builtins.int] | None = ...,
        max_memory_kb: builtins.int | None = ...,
        ok: builtins.bool | None = ...,
        metrics: collections.abc.Iterable[global___BenchmarkMetric] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["max_memory_kb", b"max_memory_kb", "ok", b"ok"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["inference_time_us", b"inference_time_us", "initialization_time_us", b"initialization_time_us", "max_memory_kb", b"max_memory_kb", "metrics", b"metrics", "ok", b"ok"]) -> None: ...

global___BenchmarkResult = BenchmarkResult

@typing_extensions.final
class ErrorCode(google.protobuf.message.Message):
    """A handled error."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SOURCE_FIELD_NUMBER: builtins.int
    TFLITE_ERROR_FIELD_NUMBER: builtins.int
    UNDERLYING_API_ERROR_FIELD_NUMBER: builtins.int
    source: global___Delegate.ValueType
    """Which delegate the error comes from (or NONE, if it comes from the tflite
    framework).
    """
    tflite_error: builtins.int
    """What the tflite level error is."""
    underlying_api_error: builtins.int
    """What the underlying error is (e.g., NNAPI or OpenGL error)."""
    def __init__(
        self,
        *,
        source: global___Delegate.ValueType | None = ...,
        tflite_error: builtins.int | None = ...,
        underlying_api_error: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["source", b"source", "tflite_error", b"tflite_error", "underlying_api_error", b"underlying_api_error"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["source", b"source", "tflite_error", b"tflite_error", "underlying_api_error", b"underlying_api_error"]) -> None: ...

global___ErrorCode = ErrorCode

@typing_extensions.final
class BenchmarkError(google.protobuf.message.Message):
    """An error that occurred during benchmarking.

    Used with event type ERROR.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    STAGE_FIELD_NUMBER: builtins.int
    EXIT_CODE_FIELD_NUMBER: builtins.int
    SIGNAL_FIELD_NUMBER: builtins.int
    ERROR_CODE_FIELD_NUMBER: builtins.int
    MINI_BENCHMARK_ERROR_CODE_FIELD_NUMBER: builtins.int
    stage: global___BenchmarkStage.ValueType
    """How far benchmarking got."""
    exit_code: builtins.int
    """Process exit code."""
    signal: builtins.int
    """Signal the process received."""
    @property
    def error_code(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ErrorCode]:
        """Handled tflite error."""
    mini_benchmark_error_code: builtins.int
    """Mini-benchmark error code."""
    def __init__(
        self,
        *,
        stage: global___BenchmarkStage.ValueType | None = ...,
        exit_code: builtins.int | None = ...,
        signal: builtins.int | None = ...,
        error_code: collections.abc.Iterable[global___ErrorCode] | None = ...,
        mini_benchmark_error_code: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["exit_code", b"exit_code", "mini_benchmark_error_code", b"mini_benchmark_error_code", "signal", b"signal", "stage", b"stage"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_code", b"error_code", "exit_code", b"exit_code", "mini_benchmark_error_code", b"mini_benchmark_error_code", "signal", b"signal", "stage", b"stage"]) -> None: ...

global___BenchmarkError = BenchmarkError

@typing_extensions.final
class BenchmarkEvent(google.protobuf.message.Message):
    """Top-level benchmarking event stored on-device. All events for a model are
    parsed to detect the status.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TFLITE_SETTINGS_FIELD_NUMBER: builtins.int
    EVENT_TYPE_FIELD_NUMBER: builtins.int
    RESULT_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    BOOTTIME_US_FIELD_NUMBER: builtins.int
    WALLCLOCK_US_FIELD_NUMBER: builtins.int
    @property
    def tflite_settings(self) -> global___TFLiteSettings:
        """Which settings were used for benchmarking."""
    event_type: global___BenchmarkEventType.ValueType
    """Type of the event."""
    @property
    def result(self) -> global___BenchmarkResult:
        """Result of benchmark, used when type is END."""
    @property
    def error(self) -> global___BenchmarkError:
        """Error during benchmark, used when type is ERROR."""
    boottime_us: builtins.int
    """Start timestamps. These are used for
    1. Checking whether a test was started but not completed within a given
    deadline.
    2. Optionally, telemetry timestamps.
    """
    wallclock_us: builtins.int
    def __init__(
        self,
        *,
        tflite_settings: global___TFLiteSettings | None = ...,
        event_type: global___BenchmarkEventType.ValueType | None = ...,
        result: global___BenchmarkResult | None = ...,
        error: global___BenchmarkError | None = ...,
        boottime_us: builtins.int | None = ...,
        wallclock_us: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["boottime_us", b"boottime_us", "error", b"error", "event_type", b"event_type", "result", b"result", "tflite_settings", b"tflite_settings", "wallclock_us", b"wallclock_us"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["boottime_us", b"boottime_us", "error", b"error", "event_type", b"event_type", "result", b"result", "tflite_settings", b"tflite_settings", "wallclock_us", b"wallclock_us"]) -> None: ...

global___BenchmarkEvent = BenchmarkEvent

@typing_extensions.final
class BestAccelerationDecision(google.protobuf.message.Message):
    """Represent the decision on the best acceleration from the mini-benchmark."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NUMBER_OF_SOURCE_EVENTS_FIELD_NUMBER: builtins.int
    MIN_LATENCY_EVENT_FIELD_NUMBER: builtins.int
    MIN_INFERENCE_TIME_US_FIELD_NUMBER: builtins.int
    number_of_source_events: builtins.int
    """Number of events used to take the decision.
    Using just the size instaed of the full list of events to save space.
    """
    @property
    def min_latency_event(self) -> global___BenchmarkEvent:
        """Event with min latency in the source ones."""
    min_inference_time_us: builtins.int
    """Min latency as read from min_latency_event."""
    def __init__(
        self,
        *,
        number_of_source_events: builtins.int | None = ...,
        min_latency_event: global___BenchmarkEvent | None = ...,
        min_inference_time_us: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["min_inference_time_us", b"min_inference_time_us", "min_latency_event", b"min_latency_event", "number_of_source_events", b"number_of_source_events"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["min_inference_time_us", b"min_inference_time_us", "min_latency_event", b"min_latency_event", "number_of_source_events", b"number_of_source_events"]) -> None: ...

global___BestAccelerationDecision = BestAccelerationDecision

@typing_extensions.final
class BenchmarkInitializationFailure(google.protobuf.message.Message):
    """Represent a failure during the initialization of the mini-benchmark."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INITIALIZATION_STATUS_FIELD_NUMBER: builtins.int
    initialization_status: builtins.int
    """Status code returned by the mini-benchmark initialization function."""
    def __init__(
        self,
        *,
        initialization_status: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["initialization_status", b"initialization_status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["initialization_status", b"initialization_status"]) -> None: ...

global___BenchmarkInitializationFailure = BenchmarkInitializationFailure

@typing_extensions.final
class MiniBenchmarkEvent(google.protobuf.message.Message):
    """Events generated by the mini-benchmark before and after triggering
    the different configuration-specific benchmarks
    Not using oneof because of the way the generated cpp code.
    See comment above on TfLite settings for details.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    IS_LOG_FLUSHING_EVENT_FIELD_NUMBER: builtins.int
    BEST_ACCELERATION_DECISION_FIELD_NUMBER: builtins.int
    INITIALIZATION_FAILURE_FIELD_NUMBER: builtins.int
    BENCHMARK_EVENT_FIELD_NUMBER: builtins.int
    is_log_flushing_event: builtins.bool
    """If set to true, this event is used to mark all previous events in the
    mini-benchmark internal storage as read and one of the other fields
    in this message will have a value.
    """
    @property
    def best_acceleration_decision(self) -> global___BestAccelerationDecision:
        """Event generated when a best acceleration decision is taken."""
    @property
    def initialization_failure(self) -> global___BenchmarkInitializationFailure:
        """Reports a failure during mini-benchmark initialization."""
    @property
    def benchmark_event(self) -> global___BenchmarkEvent:
        """Event generated while benchmarking the different settings to test locally."""
    def __init__(
        self,
        *,
        is_log_flushing_event: builtins.bool | None = ...,
        best_acceleration_decision: global___BestAccelerationDecision | None = ...,
        initialization_failure: global___BenchmarkInitializationFailure | None = ...,
        benchmark_event: global___BenchmarkEvent | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["benchmark_event", b"benchmark_event", "best_acceleration_decision", b"best_acceleration_decision", "initialization_failure", b"initialization_failure", "is_log_flushing_event", b"is_log_flushing_event"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["benchmark_event", b"benchmark_event", "best_acceleration_decision", b"best_acceleration_decision", "initialization_failure", b"initialization_failure", "is_log_flushing_event", b"is_log_flushing_event"]) -> None: ...

global___MiniBenchmarkEvent = MiniBenchmarkEvent

@typing_extensions.final
class ModelFile(google.protobuf.message.Message):
    """How to access the model for mini-benchmark.
    Since mini-benchmark runs in a separate process, it can not access an
    in-memory model. It can read the model either from a file or from a file
    descriptor. The file descriptor typically comes from the Android asset
    manager.

    Users should set either filename, or all of fd, offset and length.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    FILENAME_FIELD_NUMBER: builtins.int
    FD_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    LENGTH_FIELD_NUMBER: builtins.int
    filename: builtins.str
    """Filename for reading model from."""
    fd: builtins.int
    """File descriptor to read model from."""
    offset: builtins.int
    """Offset for model in file descriptor."""
    length: builtins.int
    """Length of model in file descriptor."""
    def __init__(
        self,
        *,
        filename: builtins.str | None = ...,
        fd: builtins.int | None = ...,
        offset: builtins.int | None = ...,
        length: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["fd", b"fd", "filename", b"filename", "length", b"length", "offset", b"offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["fd", b"fd", "filename", b"filename", "length", b"length", "offset", b"offset"]) -> None: ...

global___ModelFile = ModelFile

@typing_extensions.final
class BenchmarkStoragePaths(google.protobuf.message.Message):
    """Where to store mini-benchmark state."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    STORAGE_FILE_PATH_FIELD_NUMBER: builtins.int
    DATA_DIRECTORY_PATH_FIELD_NUMBER: builtins.int
    storage_file_path: builtins.str
    """Base path to the files used to store benchmark results in. Two files
    will be generated: one with the given path and an extra file to store
    events related to best acceleration results at path storage_file_path +
    ".extra.fb". Must be specific to the model.
    Note on Android, this should be the code cache directory.
    """
    data_directory_path: builtins.str
    """Path to a directory for intermediate files (lock files, extracted
    binaries).
    Note on Android, this typically is the data cache directory (i.e. the one
    returned by `getCacheDir()`).
    """
    def __init__(
        self,
        *,
        storage_file_path: builtins.str | None = ...,
        data_directory_path: builtins.str | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data_directory_path", b"data_directory_path", "storage_file_path", b"storage_file_path"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_directory_path", b"data_directory_path", "storage_file_path", b"storage_file_path"]) -> None: ...

global___BenchmarkStoragePaths = BenchmarkStoragePaths

@typing_extensions.final
class ValidationSettings(google.protobuf.message.Message):
    """Validation related settings.
    Next ID: 2
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    PER_TEST_TIMEOUT_MS_FIELD_NUMBER: builtins.int
    per_test_timeout_ms: builtins.int
    """Timeout for one settings under test. If test didn't finish within this
    timeout, this setting is considered hanging.
    """
    def __init__(
        self,
        *,
        per_test_timeout_ms: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["per_test_timeout_ms", b"per_test_timeout_ms"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["per_test_timeout_ms", b"per_test_timeout_ms"]) -> None: ...

global___ValidationSettings = ValidationSettings

@typing_extensions.final
class MinibenchmarkSettings(google.protobuf.message.Message):
    """How to run a minibenchmark.
    Next ID: 5
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SETTINGS_TO_TEST_FIELD_NUMBER: builtins.int
    MODEL_FILE_FIELD_NUMBER: builtins.int
    STORAGE_PATHS_FIELD_NUMBER: builtins.int
    VALIDATION_SETTINGS_FIELD_NUMBER: builtins.int
    @property
    def settings_to_test(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TFLiteSettings]:
        """Which settings to test. This would typically be filled in from an
        allowlist.
        """
    @property
    def model_file(self) -> global___ModelFile:
        """How to access the model. This would typically be set dynamically, as it
        depends on the application folder and/or runtime state.
        """
    @property
    def storage_paths(self) -> global___BenchmarkStoragePaths:
        """Where to store state. This would typically be set dynamically, as it
        depends on the application folder.
        """
    @property
    def validation_settings(self) -> global___ValidationSettings:
        """Validation test related settings."""
    def __init__(
        self,
        *,
        settings_to_test: collections.abc.Iterable[global___TFLiteSettings] | None = ...,
        model_file: global___ModelFile | None = ...,
        storage_paths: global___BenchmarkStoragePaths | None = ...,
        validation_settings: global___ValidationSettings | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["model_file", b"model_file", "storage_paths", b"storage_paths", "validation_settings", b"validation_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_file", b"model_file", "settings_to_test", b"settings_to_test", "storage_paths", b"storage_paths", "validation_settings", b"validation_settings"]) -> None: ...

global___MinibenchmarkSettings = MinibenchmarkSettings
