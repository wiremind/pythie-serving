"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _ExecutionPreference:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ExecutionPreferenceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ExecutionPreference.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    ANY: _ExecutionPreference.ValueType  # 0
    """Match any selected preference. Allowlist (semantically - value is same as
    on input).
    """

    LOW_LATENCY: _ExecutionPreference.ValueType  # 1
    """Match low latency preference. Both compatibility list and input."""

    LOW_POWER: _ExecutionPreference.ValueType  # 2
    """Math low power preference. Both compatibility list and input."""

    FORCE_CPU: _ExecutionPreference.ValueType  # 3
    """Never accelerate. Can be used for input to compatibility list or for
    standalone Acceleration configuration.
    """

class ExecutionPreference(_ExecutionPreference, metaclass=_ExecutionPreferenceEnumTypeWrapper):
    """ExecutionPreference is used to match accelerators against the preferences of
    the current application or usecase. Some of the values here can appear both
    in the compatibility list and as input, some only as input.

    These are separate from NNAPIExecutionPreference - the compatibility list
    design doesn't assume a one-to-one mapping between which usecases
    compatibility list entries have been developed for and what settings are used
    for NNAPI.
    """
    pass

ANY: ExecutionPreference.ValueType  # 0
"""Match any selected preference. Allowlist (semantically - value is same as
on input).
"""

LOW_LATENCY: ExecutionPreference.ValueType  # 1
"""Match low latency preference. Both compatibility list and input."""

LOW_POWER: ExecutionPreference.ValueType  # 2
"""Math low power preference. Both compatibility list and input."""

FORCE_CPU: ExecutionPreference.ValueType  # 3
"""Never accelerate. Can be used for input to compatibility list or for
standalone Acceleration configuration.
"""

global___ExecutionPreference = ExecutionPreference


class _Delegate:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DelegateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Delegate.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    NONE: _Delegate.ValueType  # 0
    NNAPI: _Delegate.ValueType  # 1
    GPU: _Delegate.ValueType  # 2
    HEXAGON: _Delegate.ValueType  # 3
    XNNPACK: _Delegate.ValueType  # 4
    EDGETPU: _Delegate.ValueType  # 5
    """The EdgeTpu in Pixel devices."""

    EDGETPU_CORAL: _Delegate.ValueType  # 6
    """The Coral EdgeTpu Dev Board / USB accelerator."""

class Delegate(_Delegate, metaclass=_DelegateEnumTypeWrapper):
    """TFLite accelerator to use."""
    pass

NONE: Delegate.ValueType  # 0
NNAPI: Delegate.ValueType  # 1
GPU: Delegate.ValueType  # 2
HEXAGON: Delegate.ValueType  # 3
XNNPACK: Delegate.ValueType  # 4
EDGETPU: Delegate.ValueType  # 5
"""The EdgeTpu in Pixel devices."""

EDGETPU_CORAL: Delegate.ValueType  # 6
"""The Coral EdgeTpu Dev Board / USB accelerator."""

global___Delegate = Delegate


class _NNAPIExecutionPreference:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _NNAPIExecutionPreferenceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_NNAPIExecutionPreference.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED: _NNAPIExecutionPreference.ValueType  # 0
    """Undefined."""

    NNAPI_LOW_POWER: _NNAPIExecutionPreference.ValueType  # 1
    """Prefer executing in a way that minimizes battery drain."""

    NNAPI_FAST_SINGLE_ANSWER: _NNAPIExecutionPreference.ValueType  # 2
    """Prefer returning a single answer as fast as possible, even if this causes
    more power consumption.
    """

    NNAPI_SUSTAINED_SPEED: _NNAPIExecutionPreference.ValueType  # 3
    """Prefer maximizing the throughput of successive frames, for example when
    processing successive frames coming from the camera.
    """

class NNAPIExecutionPreference(_NNAPIExecutionPreference, metaclass=_NNAPIExecutionPreferenceEnumTypeWrapper):
    pass

UNDEFINED: NNAPIExecutionPreference.ValueType  # 0
"""Undefined."""

NNAPI_LOW_POWER: NNAPIExecutionPreference.ValueType  # 1
"""Prefer executing in a way that minimizes battery drain."""

NNAPI_FAST_SINGLE_ANSWER: NNAPIExecutionPreference.ValueType  # 2
"""Prefer returning a single answer as fast as possible, even if this causes
more power consumption.
"""

NNAPI_SUSTAINED_SPEED: NNAPIExecutionPreference.ValueType  # 3
"""Prefer maximizing the throughput of successive frames, for example when
processing successive frames coming from the camera.
"""

global___NNAPIExecutionPreference = NNAPIExecutionPreference


class _NNAPIExecutionPriority:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _NNAPIExecutionPriorityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_NNAPIExecutionPriority.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    NNAPI_PRIORITY_UNDEFINED: _NNAPIExecutionPriority.ValueType  # 0
    NNAPI_PRIORITY_LOW: _NNAPIExecutionPriority.ValueType  # 1
    NNAPI_PRIORITY_MEDIUM: _NNAPIExecutionPriority.ValueType  # 2
    NNAPI_PRIORITY_HIGH: _NNAPIExecutionPriority.ValueType  # 3
class NNAPIExecutionPriority(_NNAPIExecutionPriority, metaclass=_NNAPIExecutionPriorityEnumTypeWrapper):
    pass

NNAPI_PRIORITY_UNDEFINED: NNAPIExecutionPriority.ValueType  # 0
NNAPI_PRIORITY_LOW: NNAPIExecutionPriority.ValueType  # 1
NNAPI_PRIORITY_MEDIUM: NNAPIExecutionPriority.ValueType  # 2
NNAPI_PRIORITY_HIGH: NNAPIExecutionPriority.ValueType  # 3
global___NNAPIExecutionPriority = NNAPIExecutionPriority


class _GPUBackend:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _GPUBackendEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_GPUBackend.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNSET: _GPUBackend.ValueType  # 0
    OPENCL: _GPUBackend.ValueType  # 1
    OPENGL: _GPUBackend.ValueType  # 2
    """Not yet supported.
    VULKAN = 3;
    METAL = 4;
    """

class GPUBackend(_GPUBackend, metaclass=_GPUBackendEnumTypeWrapper):
    """Which GPU backend to select. Default behaviour on Android is to try OpenCL
    and if it's not available fall back to OpenGL.
    """
    pass

UNSET: GPUBackend.ValueType  # 0
OPENCL: GPUBackend.ValueType  # 1
OPENGL: GPUBackend.ValueType  # 2
"""Not yet supported.
VULKAN = 3;
METAL = 4;
"""

global___GPUBackend = GPUBackend


class _GPUInferencePriority:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _GPUInferencePriorityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_GPUInferencePriority.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    GPU_PRIORITY_AUTO: _GPUInferencePriority.ValueType  # 0
    GPU_PRIORITY_MAX_PRECISION: _GPUInferencePriority.ValueType  # 1
    GPU_PRIORITY_MIN_LATENCY: _GPUInferencePriority.ValueType  # 2
    GPU_PRIORITY_MIN_MEMORY_USAGE: _GPUInferencePriority.ValueType  # 3
class GPUInferencePriority(_GPUInferencePriority, metaclass=_GPUInferencePriorityEnumTypeWrapper):
    """GPU inference priorities define relative priorities given by the GPU delegate
    to different client needs.
    Corresponds to TfLiteGpuInferencePriority.
    """
    pass

GPU_PRIORITY_AUTO: GPUInferencePriority.ValueType  # 0
GPU_PRIORITY_MAX_PRECISION: GPUInferencePriority.ValueType  # 1
GPU_PRIORITY_MIN_LATENCY: GPUInferencePriority.ValueType  # 2
GPU_PRIORITY_MIN_MEMORY_USAGE: GPUInferencePriority.ValueType  # 3
global___GPUInferencePriority = GPUInferencePriority


class _EdgeTpuPowerState:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _EdgeTpuPowerStateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_EdgeTpuPowerState.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED_POWERSTATE: _EdgeTpuPowerState.ValueType  # 0
    """Undefined power state."""

    TPU_CORE_OFF: _EdgeTpuPowerState.ValueType  # 1
    """TPU core is off but control cluster is on."""

    READY: _EdgeTpuPowerState.ValueType  # 2
    """A non-active low-power state that has much smaller transition time to
    active compared to off.
    """

    ACTIVE_MIN_POWER: _EdgeTpuPowerState.ValueType  # 3
    """Minimum power active state."""

    ACTIVE_VERY_LOW_POWER: _EdgeTpuPowerState.ValueType  # 4
    """Very low performance, very low power."""

    ACTIVE_LOW_POWER: _EdgeTpuPowerState.ValueType  # 5
    """Low performance, low power."""

    ACTIVE: _EdgeTpuPowerState.ValueType  # 6
    """The normal performance and power. This setting usually provides the
    optimal perf/power trade-off for the average use-case.
    """

    OVER_DRIVE: _EdgeTpuPowerState.ValueType  # 7
    """Maximum performance level. Potentially higher power and thermal. This
    setting may not be allowed in production depending on the system.
    """

class EdgeTpuPowerState(_EdgeTpuPowerState, metaclass=_EdgeTpuPowerStateEnumTypeWrapper):
    """Generic definitions of EdgeTPU power states."""
    pass

UNDEFINED_POWERSTATE: EdgeTpuPowerState.ValueType  # 0
"""Undefined power state."""

TPU_CORE_OFF: EdgeTpuPowerState.ValueType  # 1
"""TPU core is off but control cluster is on."""

READY: EdgeTpuPowerState.ValueType  # 2
"""A non-active low-power state that has much smaller transition time to
active compared to off.
"""

ACTIVE_MIN_POWER: EdgeTpuPowerState.ValueType  # 3
"""Minimum power active state."""

ACTIVE_VERY_LOW_POWER: EdgeTpuPowerState.ValueType  # 4
"""Very low performance, very low power."""

ACTIVE_LOW_POWER: EdgeTpuPowerState.ValueType  # 5
"""Low performance, low power."""

ACTIVE: EdgeTpuPowerState.ValueType  # 6
"""The normal performance and power. This setting usually provides the
optimal perf/power trade-off for the average use-case.
"""

OVER_DRIVE: EdgeTpuPowerState.ValueType  # 7
"""Maximum performance level. Potentially higher power and thermal. This
setting may not be allowed in production depending on the system.
"""

global___EdgeTpuPowerState = EdgeTpuPowerState


class _BenchmarkEventType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _BenchmarkEventTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_BenchmarkEventType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNDEFINED_BENCHMARK_EVENT_TYPE: _BenchmarkEventType.ValueType  # 0
    START: _BenchmarkEventType.ValueType  # 1
    """Benchmark start. A start without an end can be interpreted as a test that
    has crashed or hung.
    """

    END: _BenchmarkEventType.ValueType  # 2
    """Benchmarking completion. A model was successfully loaded, acceleration
    configured and inference run without errors. There may still be an issue
    with correctness of results, or with performance.
    """

    ERROR: _BenchmarkEventType.ValueType  # 3
    """Benchmark was not completed due to an error. The error may be a handled
    error (e.g., failure in a delegate), or a crash.
    """

    LOGGED: _BenchmarkEventType.ValueType  # 4
    """Benchmark data has been sent for logging."""

class BenchmarkEventType(_BenchmarkEventType, metaclass=_BenchmarkEventTypeEnumTypeWrapper):
    """On-device mini-benchmark result storage. The following definitions are used
    to keep an append-only log of benchmark results on-device. (Hence there is
    single top-level event that is used for all data).

    These definitions don't need a proto-to-flatbuffer conversion, since they are
    not used for specifying configuration in the Tasks library.

    Which stage of benchmarking the event is for.
    There might be multiple events with the same type, if a benchmark is run
    multiple times.
    """
    pass

UNDEFINED_BENCHMARK_EVENT_TYPE: BenchmarkEventType.ValueType  # 0
START: BenchmarkEventType.ValueType  # 1
"""Benchmark start. A start without an end can be interpreted as a test that
has crashed or hung.
"""

END: BenchmarkEventType.ValueType  # 2
"""Benchmarking completion. A model was successfully loaded, acceleration
configured and inference run without errors. There may still be an issue
with correctness of results, or with performance.
"""

ERROR: BenchmarkEventType.ValueType  # 3
"""Benchmark was not completed due to an error. The error may be a handled
error (e.g., failure in a delegate), or a crash.
"""

LOGGED: BenchmarkEventType.ValueType  # 4
"""Benchmark data has been sent for logging."""

global___BenchmarkEventType = BenchmarkEventType


class _BenchmarkStage:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _BenchmarkStageEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_BenchmarkStage.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNKNOWN: _BenchmarkStage.ValueType  # 0
    INITIALIZATION: _BenchmarkStage.ValueType  # 1
    """During model loading or delegation."""

    INFERENCE: _BenchmarkStage.ValueType  # 2
    """During inference."""

class BenchmarkStage(_BenchmarkStage, metaclass=_BenchmarkStageEnumTypeWrapper):
    """When during benchmark execution an error occurred."""
    pass

UNKNOWN: BenchmarkStage.ValueType  # 0
INITIALIZATION: BenchmarkStage.ValueType  # 1
"""During model loading or delegation."""

INFERENCE: BenchmarkStage.ValueType  # 2
"""During inference."""

global___BenchmarkStage = BenchmarkStage


class ComputeSettings(google.protobuf.message.Message):
    """One possible acceleration configuration."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PREFERENCE_FIELD_NUMBER: builtins.int
    TFLITE_SETTINGS_FIELD_NUMBER: builtins.int
    MODEL_NAMESPACE_FOR_STATISTICS_FIELD_NUMBER: builtins.int
    MODEL_IDENTIFIER_FOR_STATISTICS_FIELD_NUMBER: builtins.int
    SETTINGS_TO_TEST_LOCALLY_FIELD_NUMBER: builtins.int
    preference: global___ExecutionPreference.ValueType
    """Which preference to use this accelerator for."""

    @property
    def tflite_settings(self) -> global___TFLiteSettings:
        """How to configure TFLite"""
        pass
    model_namespace_for_statistics: typing.Text
    """Identifiers to use for instrumentation and telemetry."""

    model_identifier_for_statistics: typing.Text
    @property
    def settings_to_test_locally(self) -> global___MinibenchmarkSettings:
        """'Maybe' acceleration: use mini-benchmark to select settings."""
        pass
    def __init__(self,
        *,
        preference: typing.Optional[global___ExecutionPreference.ValueType] = ...,
        tflite_settings: typing.Optional[global___TFLiteSettings] = ...,
        model_namespace_for_statistics: typing.Optional[typing.Text] = ...,
        model_identifier_for_statistics: typing.Optional[typing.Text] = ...,
        settings_to_test_locally: typing.Optional[global___MinibenchmarkSettings] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["model_identifier_for_statistics",b"model_identifier_for_statistics","model_namespace_for_statistics",b"model_namespace_for_statistics","preference",b"preference","settings_to_test_locally",b"settings_to_test_locally","tflite_settings",b"tflite_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_identifier_for_statistics",b"model_identifier_for_statistics","model_namespace_for_statistics",b"model_namespace_for_statistics","preference",b"preference","settings_to_test_locally",b"settings_to_test_locally","tflite_settings",b"tflite_settings"]) -> None: ...
global___ComputeSettings = ComputeSettings

class NNAPISettings(google.protobuf.message.Message):
    """NNAPI delegate settings."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    ACCELERATOR_NAME_FIELD_NUMBER: builtins.int
    CACHE_DIRECTORY_FIELD_NUMBER: builtins.int
    MODEL_TOKEN_FIELD_NUMBER: builtins.int
    EXECUTION_PREFERENCE_FIELD_NUMBER: builtins.int
    NO_OF_NNAPI_INSTANCES_TO_CACHE_FIELD_NUMBER: builtins.int
    FALLBACK_SETTINGS_FIELD_NUMBER: builtins.int
    ALLOW_NNAPI_CPU_ON_ANDROID_10_PLUS_FIELD_NUMBER: builtins.int
    EXECUTION_PRIORITY_FIELD_NUMBER: builtins.int
    ALLOW_DYNAMIC_DIMENSIONS_FIELD_NUMBER: builtins.int
    ALLOW_FP16_PRECISION_FOR_FP32_FIELD_NUMBER: builtins.int
    USE_BURST_COMPUTATION_FIELD_NUMBER: builtins.int
    accelerator_name: typing.Text
    """Which instance (NNAPI accelerator) to use. One driver may provide several
    accelerators (though a driver may also hide several back-ends behind one
    name, at the choice of the driver vendor).
    Note that driver introspection is only available in Android Q and later.
    """

    cache_directory: typing.Text
    """NNAPI model compilation caching settings to be passed to
    tflite::StatefulNnApiDelegate
    """

    model_token: typing.Text
    execution_preference: global___NNAPIExecutionPreference.ValueType
    """NNAPI execution preference to pass. See
    https://developer.android.com/ndk/reference/group/neural-networks.html
    """

    no_of_nnapi_instances_to_cache: builtins.int
    """Number of instances to cache for the same model (for input size
    changes). This is mandatory for getting reasonable performance in that
    case.
    """

    @property
    def fallback_settings(self) -> global___FallbackSettings:
        """Deprecated; use the fallback_settings in TFLiteSettings.

        Whether to automatically fall back to TFLite CPU path.
        """
        pass
    allow_nnapi_cpu_on_android_10_plus: builtins.bool
    """Whether to allow use of NNAPI CPU (nnapi-reference accelerator) on Android
    10+ when an accelerator name is not specified. The NNAPI CPU typically
    performs less well than the TfLite built-in kernels; but allowing allows a
    model to be partially accelerated which may be a win.
    """

    execution_priority: global___NNAPIExecutionPriority.ValueType
    allow_dynamic_dimensions: builtins.bool
    """Whether to allow dynamic dimension sizes without re-compilation.
    A tensor of with dynamic dimension must have a valid dims_signature
    defined.
    Only supported in NNAPI 1.1 and newer versions.
    WARNING: Setting this flag to true may result in model being rejected by
    accelerator. This should only be enabled if the target device supports
    dynamic dimensions of the model.
    By default this is set to false.
    """

    allow_fp16_precision_for_fp32: builtins.bool
    """Whether to allow the NNAPI accelerator to optionally use lower-precision
    float16 (16-bit floating point) arithmetic when doing calculations on
    float32 (32-bit floating point).
    """

    use_burst_computation: builtins.bool
    """Whether to use NNAPI Burst mode.
    Burst mode allows accelerators to efficiently manage resources, which
    would significantly reduce overhead especially if the same delegate
    instance is to be used for multiple inferences.
    """

    def __init__(self,
        *,
        accelerator_name: typing.Optional[typing.Text] = ...,
        cache_directory: typing.Optional[typing.Text] = ...,
        model_token: typing.Optional[typing.Text] = ...,
        execution_preference: typing.Optional[global___NNAPIExecutionPreference.ValueType] = ...,
        no_of_nnapi_instances_to_cache: typing.Optional[builtins.int] = ...,
        fallback_settings: typing.Optional[global___FallbackSettings] = ...,
        allow_nnapi_cpu_on_android_10_plus: typing.Optional[builtins.bool] = ...,
        execution_priority: typing.Optional[global___NNAPIExecutionPriority.ValueType] = ...,
        allow_dynamic_dimensions: typing.Optional[builtins.bool] = ...,
        allow_fp16_precision_for_fp32: typing.Optional[builtins.bool] = ...,
        use_burst_computation: typing.Optional[builtins.bool] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["accelerator_name",b"accelerator_name","allow_dynamic_dimensions",b"allow_dynamic_dimensions","allow_fp16_precision_for_fp32",b"allow_fp16_precision_for_fp32","allow_nnapi_cpu_on_android_10_plus",b"allow_nnapi_cpu_on_android_10_plus","cache_directory",b"cache_directory","execution_preference",b"execution_preference","execution_priority",b"execution_priority","fallback_settings",b"fallback_settings","model_token",b"model_token","no_of_nnapi_instances_to_cache",b"no_of_nnapi_instances_to_cache","use_burst_computation",b"use_burst_computation"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["accelerator_name",b"accelerator_name","allow_dynamic_dimensions",b"allow_dynamic_dimensions","allow_fp16_precision_for_fp32",b"allow_fp16_precision_for_fp32","allow_nnapi_cpu_on_android_10_plus",b"allow_nnapi_cpu_on_android_10_plus","cache_directory",b"cache_directory","execution_preference",b"execution_preference","execution_priority",b"execution_priority","fallback_settings",b"fallback_settings","model_token",b"model_token","no_of_nnapi_instances_to_cache",b"no_of_nnapi_instances_to_cache","use_burst_computation",b"use_burst_computation"]) -> None: ...
global___NNAPISettings = NNAPISettings

class GPUSettings(google.protobuf.message.Message):
    """GPU Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    IS_PRECISION_LOSS_ALLOWED_FIELD_NUMBER: builtins.int
    ENABLE_QUANTIZED_INFERENCE_FIELD_NUMBER: builtins.int
    FORCE_BACKEND_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY1_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY2_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY3_FIELD_NUMBER: builtins.int
    is_precision_loss_allowed: builtins.bool
    """Ignored if inference_priority1/2/3 are set."""

    enable_quantized_inference: builtins.bool
    force_backend: global___GPUBackend.ValueType
    inference_priority1: global___GPUInferencePriority.ValueType
    """Ordered priorities provide better control over desired semantics,
    where priority(n) is more important than priority(n+1). Therefore,
    each time inference engine needs to make a decision, it uses
    ordered priorities to do so.

    Default values correspond to GPU_PRIORITY_AUTO.
    AUTO priority can only be used when higher priorities are fully specified.
    For example:
      VALID:   priority1 = MIN_LATENCY, priority2 = AUTO, priority3 = AUTO
      VALID:   priority1 = MIN_LATENCY, priority2 = MAX_PRECISION,
               priority3 = AUTO
      INVALID: priority1 = AUTO, priority2 = MIN_LATENCY, priority3 = AUTO
      INVALID: priority1 = MIN_LATENCY, priority2 = AUTO,
               priority3 = MAX_PRECISION
    Invalid priorities will result in error.

    For more information, see TfLiteGpuDelegateOptionsV2.
    """

    inference_priority2: global___GPUInferencePriority.ValueType
    inference_priority3: global___GPUInferencePriority.ValueType
    def __init__(self,
        *,
        is_precision_loss_allowed: typing.Optional[builtins.bool] = ...,
        enable_quantized_inference: typing.Optional[builtins.bool] = ...,
        force_backend: typing.Optional[global___GPUBackend.ValueType] = ...,
        inference_priority1: typing.Optional[global___GPUInferencePriority.ValueType] = ...,
        inference_priority2: typing.Optional[global___GPUInferencePriority.ValueType] = ...,
        inference_priority3: typing.Optional[global___GPUInferencePriority.ValueType] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["enable_quantized_inference",b"enable_quantized_inference","force_backend",b"force_backend","inference_priority1",b"inference_priority1","inference_priority2",b"inference_priority2","inference_priority3",b"inference_priority3","is_precision_loss_allowed",b"is_precision_loss_allowed"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_quantized_inference",b"enable_quantized_inference","force_backend",b"force_backend","inference_priority1",b"inference_priority1","inference_priority2",b"inference_priority2","inference_priority3",b"inference_priority3","is_precision_loss_allowed",b"is_precision_loss_allowed"]) -> None: ...
global___GPUSettings = GPUSettings

class HexagonSettings(google.protobuf.message.Message):
    """Hexagon Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.h
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    DEBUG_LEVEL_FIELD_NUMBER: builtins.int
    POWERSAVE_LEVEL_FIELD_NUMBER: builtins.int
    PRINT_GRAPH_PROFILE_FIELD_NUMBER: builtins.int
    PRINT_GRAPH_DEBUG_FIELD_NUMBER: builtins.int
    debug_level: builtins.int
    powersave_level: builtins.int
    print_graph_profile: builtins.bool
    print_graph_debug: builtins.bool
    def __init__(self,
        *,
        debug_level: typing.Optional[builtins.int] = ...,
        powersave_level: typing.Optional[builtins.int] = ...,
        print_graph_profile: typing.Optional[builtins.bool] = ...,
        print_graph_debug: typing.Optional[builtins.bool] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["debug_level",b"debug_level","powersave_level",b"powersave_level","print_graph_debug",b"print_graph_debug","print_graph_profile",b"print_graph_profile"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["debug_level",b"debug_level","powersave_level",b"powersave_level","print_graph_debug",b"print_graph_debug","print_graph_profile",b"print_graph_profile"]) -> None: ...
global___HexagonSettings = HexagonSettings

class XNNPackSettings(google.protobuf.message.Message):
    """XNNPack Delegate settings.

    See
    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    NUM_THREADS_FIELD_NUMBER: builtins.int
    num_threads: builtins.int
    def __init__(self,
        *,
        num_threads: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["num_threads",b"num_threads"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["num_threads",b"num_threads"]) -> None: ...
global___XNNPackSettings = XNNPackSettings

class EdgeTpuDeviceSpec(google.protobuf.message.Message):
    """EdgeTPU device spec."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _PlatformType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PlatformTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[EdgeTpuDeviceSpec._PlatformType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        MMIO: EdgeTpuDeviceSpec._PlatformType.ValueType  # 0
        REFERENCE: EdgeTpuDeviceSpec._PlatformType.ValueType  # 1
        SIMULATOR: EdgeTpuDeviceSpec._PlatformType.ValueType  # 2
        REMOTE_SIMULATOR: EdgeTpuDeviceSpec._PlatformType.ValueType  # 3
    class PlatformType(_PlatformType, metaclass=_PlatformTypeEnumTypeWrapper):
        """EdgeTPU platform types."""
        pass

    MMIO: EdgeTpuDeviceSpec.PlatformType.ValueType  # 0
    REFERENCE: EdgeTpuDeviceSpec.PlatformType.ValueType  # 1
    SIMULATOR: EdgeTpuDeviceSpec.PlatformType.ValueType  # 2
    REMOTE_SIMULATOR: EdgeTpuDeviceSpec.PlatformType.ValueType  # 3

    PLATFORM_TYPE_FIELD_NUMBER: builtins.int
    NUM_CHIPS_FIELD_NUMBER: builtins.int
    DEVICE_PATHS_FIELD_NUMBER: builtins.int
    CHIP_FAMILY_FIELD_NUMBER: builtins.int
    platform_type: global___EdgeTpuDeviceSpec.PlatformType.ValueType
    """Execution platform for the EdgeTPU device."""

    num_chips: builtins.int
    """Number of chips to use for the EdgeTPU device."""

    @property
    def device_paths(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Paths to the EdgeTPU devices;"""
        pass
    chip_family: builtins.int
    """Chip family used by the EdgeTpu device."""

    def __init__(self,
        *,
        platform_type: typing.Optional[global___EdgeTpuDeviceSpec.PlatformType.ValueType] = ...,
        num_chips: typing.Optional[builtins.int] = ...,
        device_paths: typing.Optional[typing.Iterable[typing.Text]] = ...,
        chip_family: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["chip_family",b"chip_family","num_chips",b"num_chips","platform_type",b"platform_type"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["chip_family",b"chip_family","device_paths",b"device_paths","num_chips",b"num_chips","platform_type",b"platform_type"]) -> None: ...
global___EdgeTpuDeviceSpec = EdgeTpuDeviceSpec

class EdgeTpuInactivePowerConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    INACTIVE_POWER_STATE_FIELD_NUMBER: builtins.int
    INACTIVE_TIMEOUT_US_FIELD_NUMBER: builtins.int
    inactive_power_state: global___EdgeTpuPowerState.ValueType
    """Inactive power states between inferences."""

    inactive_timeout_us: builtins.int
    """Inactive timeout in microseconds between inferences."""

    def __init__(self,
        *,
        inactive_power_state: typing.Optional[global___EdgeTpuPowerState.ValueType] = ...,
        inactive_timeout_us: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["inactive_power_state",b"inactive_power_state","inactive_timeout_us",b"inactive_timeout_us"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["inactive_power_state",b"inactive_power_state","inactive_timeout_us",b"inactive_timeout_us"]) -> None: ...
global___EdgeTpuInactivePowerConfig = EdgeTpuInactivePowerConfig

class EdgeTpuSettings(google.protobuf.message.Message):
    """EdgeTPU Delegate settings."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _FloatTruncationType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _FloatTruncationTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[EdgeTpuSettings._FloatTruncationType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNSPECIFIED: EdgeTpuSettings._FloatTruncationType.ValueType  # 0
        NO_TRUNCATION: EdgeTpuSettings._FloatTruncationType.ValueType  # 1
        BFLOAT16: EdgeTpuSettings._FloatTruncationType.ValueType  # 2
        HALF: EdgeTpuSettings._FloatTruncationType.ValueType  # 3
    class FloatTruncationType(_FloatTruncationType, metaclass=_FloatTruncationTypeEnumTypeWrapper):
        """Float truncation types for EdgeTPU."""
        pass

    UNSPECIFIED: EdgeTpuSettings.FloatTruncationType.ValueType  # 0
    NO_TRUNCATION: EdgeTpuSettings.FloatTruncationType.ValueType  # 1
    BFLOAT16: EdgeTpuSettings.FloatTruncationType.ValueType  # 2
    HALF: EdgeTpuSettings.FloatTruncationType.ValueType  # 3

    INFERENCE_POWER_STATE_FIELD_NUMBER: builtins.int
    INACTIVE_POWER_CONFIGS_FIELD_NUMBER: builtins.int
    INFERENCE_PRIORITY_FIELD_NUMBER: builtins.int
    EDGETPU_DEVICE_SPEC_FIELD_NUMBER: builtins.int
    MODEL_TOKEN_FIELD_NUMBER: builtins.int
    FLOAT_TRUNCATION_TYPE_FIELD_NUMBER: builtins.int
    inference_power_state: global___EdgeTpuPowerState.ValueType
    """Target inference power state for running the model."""

    @property
    def inactive_power_configs(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EdgeTpuInactivePowerConfig]:
        """Inactive power states between inferences."""
        pass
    inference_priority: builtins.int
    """Priority for the inference request."""

    @property
    def edgetpu_device_spec(self) -> global___EdgeTpuDeviceSpec:
        """Device spec for creating the EdgeTpu device."""
        pass
    model_token: typing.Text
    """A unique identifier of the input TfLite model."""

    float_truncation_type: global___EdgeTpuSettings.FloatTruncationType.ValueType
    """Float truncation type for EdgeTPU."""

    def __init__(self,
        *,
        inference_power_state: typing.Optional[global___EdgeTpuPowerState.ValueType] = ...,
        inactive_power_configs: typing.Optional[typing.Iterable[global___EdgeTpuInactivePowerConfig]] = ...,
        inference_priority: typing.Optional[builtins.int] = ...,
        edgetpu_device_spec: typing.Optional[global___EdgeTpuDeviceSpec] = ...,
        model_token: typing.Optional[typing.Text] = ...,
        float_truncation_type: typing.Optional[global___EdgeTpuSettings.FloatTruncationType.ValueType] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["edgetpu_device_spec",b"edgetpu_device_spec","float_truncation_type",b"float_truncation_type","inference_power_state",b"inference_power_state","inference_priority",b"inference_priority","model_token",b"model_token"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["edgetpu_device_spec",b"edgetpu_device_spec","float_truncation_type",b"float_truncation_type","inactive_power_configs",b"inactive_power_configs","inference_power_state",b"inference_power_state","inference_priority",b"inference_priority","model_token",b"model_token"]) -> None: ...
global___EdgeTpuSettings = EdgeTpuSettings

class CoralSettings(google.protobuf.message.Message):
    """Coral Dev Board / USB accelerator delegate settings.

    See
    https://github.com/google-coral/edgetpu/blob/master/libedgetpu/edgetpu_c.h
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _Performance:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PerformanceEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[CoralSettings._Performance.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNDEFINED: CoralSettings._Performance.ValueType  # 0
        MAXIMUM: CoralSettings._Performance.ValueType  # 1
        HIGH: CoralSettings._Performance.ValueType  # 2
        MEDIUM: CoralSettings._Performance.ValueType  # 3
        LOW: CoralSettings._Performance.ValueType  # 4
    class Performance(_Performance, metaclass=_PerformanceEnumTypeWrapper):
        pass

    UNDEFINED: CoralSettings.Performance.ValueType  # 0
    MAXIMUM: CoralSettings.Performance.ValueType  # 1
    HIGH: CoralSettings.Performance.ValueType  # 2
    MEDIUM: CoralSettings.Performance.ValueType  # 3
    LOW: CoralSettings.Performance.ValueType  # 4

    DEVICE_FIELD_NUMBER: builtins.int
    PERFORMANCE_FIELD_NUMBER: builtins.int
    USB_ALWAYS_DFU_FIELD_NUMBER: builtins.int
    USB_MAX_BULK_IN_QUEUE_LENGTH_FIELD_NUMBER: builtins.int
    device: typing.Text
    """The Edge Tpu device to be used. See
    https://github.com/google-coral/libcoral/blob/982426546dfa10128376d0c24fd8a8b161daac97/coral/tflite_utils.h#L131-L137
    """

    performance: global___CoralSettings.Performance.ValueType
    """The desired performance level. This setting adjusts the internal clock
    rate to achieve different performance / power balance. Higher performance
    values improve speed, but increase power usage.
    """

    usb_always_dfu: builtins.bool
    """If true, always perform device firmware update (DFU) after reset. DFU is
    usually only necessary after power cycle.
    """

    usb_max_bulk_in_queue_length: builtins.int
    """The maximum bulk in queue length. Larger queue length may improve USB
    performance on the direction from device to host. When not specified (or
    zero), `usb_max_bulk_in_queue_length` will default to 32 according to the
    current EdgeTpu Coral implementation.
    """

    def __init__(self,
        *,
        device: typing.Optional[typing.Text] = ...,
        performance: typing.Optional[global___CoralSettings.Performance.ValueType] = ...,
        usb_always_dfu: typing.Optional[builtins.bool] = ...,
        usb_max_bulk_in_queue_length: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device",b"device","performance",b"performance","usb_always_dfu",b"usb_always_dfu","usb_max_bulk_in_queue_length",b"usb_max_bulk_in_queue_length"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device",b"device","performance",b"performance","usb_always_dfu",b"usb_always_dfu","usb_max_bulk_in_queue_length",b"usb_max_bulk_in_queue_length"]) -> None: ...
global___CoralSettings = CoralSettings

class CPUSettings(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    NUM_THREADS_FIELD_NUMBER: builtins.int
    num_threads: builtins.int
    """Set to -1 to let the interpreter choose. Otherwise, must be > 0."""

    def __init__(self,
        *,
        num_threads: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["num_threads",b"num_threads"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["num_threads",b"num_threads"]) -> None: ...
global___CPUSettings = CPUSettings

class TFLiteSettings(google.protobuf.message.Message):
    """How to configure TFLite."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    DELEGATE_FIELD_NUMBER: builtins.int
    NNAPI_SETTINGS_FIELD_NUMBER: builtins.int
    GPU_SETTINGS_FIELD_NUMBER: builtins.int
    HEXAGON_SETTINGS_FIELD_NUMBER: builtins.int
    XNNPACK_SETTINGS_FIELD_NUMBER: builtins.int
    CPU_SETTINGS_FIELD_NUMBER: builtins.int
    MAX_DELEGATED_PARTITIONS_FIELD_NUMBER: builtins.int
    EDGETPU_SETTINGS_FIELD_NUMBER: builtins.int
    CORAL_SETTINGS_FIELD_NUMBER: builtins.int
    FALLBACK_SETTINGS_FIELD_NUMBER: builtins.int
    delegate: global___Delegate.ValueType
    """Which delegate to use."""

    @property
    def nnapi_settings(self) -> global___NNAPISettings:
        """How to configure the chosen delegate.
        (In principle we would like to use 'oneof', but flatc turns that into an
        nested anonymous table rather than a union. See
        https://github.com/google/flatbuffers/issues/4628).
        """
        pass
    @property
    def gpu_settings(self) -> global___GPUSettings: ...
    @property
    def hexagon_settings(self) -> global___HexagonSettings: ...
    @property
    def xnnpack_settings(self) -> global___XNNPackSettings: ...
    @property
    def cpu_settings(self) -> global___CPUSettings:
        """How to configure CPU execution."""
        pass
    max_delegated_partitions: builtins.int
    """Shared delegation settings."""

    @property
    def edgetpu_settings(self) -> global___EdgeTpuSettings:
        """For configuring the EdgeTpuDelegate."""
        pass
    @property
    def coral_settings(self) -> global___CoralSettings:
        """For configuring the Coral EdgeTpu Delegate."""
        pass
    @property
    def fallback_settings(self) -> global___FallbackSettings:
        """Whether to automatically fall back to TFLite CPU path."""
        pass
    def __init__(self,
        *,
        delegate: typing.Optional[global___Delegate.ValueType] = ...,
        nnapi_settings: typing.Optional[global___NNAPISettings] = ...,
        gpu_settings: typing.Optional[global___GPUSettings] = ...,
        hexagon_settings: typing.Optional[global___HexagonSettings] = ...,
        xnnpack_settings: typing.Optional[global___XNNPackSettings] = ...,
        cpu_settings: typing.Optional[global___CPUSettings] = ...,
        max_delegated_partitions: typing.Optional[builtins.int] = ...,
        edgetpu_settings: typing.Optional[global___EdgeTpuSettings] = ...,
        coral_settings: typing.Optional[global___CoralSettings] = ...,
        fallback_settings: typing.Optional[global___FallbackSettings] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["coral_settings",b"coral_settings","cpu_settings",b"cpu_settings","delegate",b"delegate","edgetpu_settings",b"edgetpu_settings","fallback_settings",b"fallback_settings","gpu_settings",b"gpu_settings","hexagon_settings",b"hexagon_settings","max_delegated_partitions",b"max_delegated_partitions","nnapi_settings",b"nnapi_settings","xnnpack_settings",b"xnnpack_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["coral_settings",b"coral_settings","cpu_settings",b"cpu_settings","delegate",b"delegate","edgetpu_settings",b"edgetpu_settings","fallback_settings",b"fallback_settings","gpu_settings",b"gpu_settings","hexagon_settings",b"hexagon_settings","max_delegated_partitions",b"max_delegated_partitions","nnapi_settings",b"nnapi_settings","xnnpack_settings",b"xnnpack_settings"]) -> None: ...
global___TFLiteSettings = TFLiteSettings

class FallbackSettings(google.protobuf.message.Message):
    """Whether to automatically fallback to TFLite CPU path on delegation errors.

    Typically fallback is enabled in production use but disabled in tests and
    benchmarks to ensure they test the intended path.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    ALLOW_AUTOMATIC_FALLBACK_ON_COMPILATION_ERROR_FIELD_NUMBER: builtins.int
    ALLOW_AUTOMATIC_FALLBACK_ON_EXECUTION_ERROR_FIELD_NUMBER: builtins.int
    allow_automatic_fallback_on_compilation_error: builtins.bool
    """Whether to allow automatically falling back to TfLite CPU path on
    compilation failure. Default is not allowing automatic fallback.

    This is useful in naive production usecases where the caller would prefer
    for the model to run even if it's not accelerated. More advanced users will
    implement fallback themselves; e.g., by using a different model on CPU.

    Note that compilation errors may occur either at initial
    ModifyGraphWithDelegate() time, or when calling AllocateTensors() after
    resizing.
    """

    allow_automatic_fallback_on_execution_error: builtins.bool
    """Whether to allow automatically falling back to TfLite CPU path on
    execution error. Default is not allowing automatic fallback.

    Experimental, use with care (only when you have complete control over the
    client code).

    The caveat above for compilation error holds.  Additionally, execution-time
    errors are harder to handle automatically as they require invalidating the
    TfLite interpreter which most client code has not been designed to deal
    with.
    """

    def __init__(self,
        *,
        allow_automatic_fallback_on_compilation_error: typing.Optional[builtins.bool] = ...,
        allow_automatic_fallback_on_execution_error: typing.Optional[builtins.bool] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["allow_automatic_fallback_on_compilation_error",b"allow_automatic_fallback_on_compilation_error","allow_automatic_fallback_on_execution_error",b"allow_automatic_fallback_on_execution_error"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["allow_automatic_fallback_on_compilation_error",b"allow_automatic_fallback_on_compilation_error","allow_automatic_fallback_on_execution_error",b"allow_automatic_fallback_on_execution_error"]) -> None: ...
global___FallbackSettings = FallbackSettings

class BenchmarkMetric(google.protobuf.message.Message):
    """A correctness metric from a benchmark, for example KL-divergence between
    known-good CPU output and on-device output. These are primarily used for
    telemetry and monitored server-side.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    NAME_FIELD_NUMBER: builtins.int
    VALUES_FIELD_NUMBER: builtins.int
    name: typing.Text
    @property
    def values(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
    def __init__(self,
        *,
        name: typing.Optional[typing.Text] = ...,
        values: typing.Optional[typing.Iterable[builtins.float]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["name",b"name"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","values",b"values"]) -> None: ...
global___BenchmarkMetric = BenchmarkMetric

class BenchmarkResult(google.protobuf.message.Message):
    """Outcome of a successfully complete benchmark run. This information is
    intended to both be used on-device to select best compute configuration as
    well as sent to server for monitoring.

    Used with event type END.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    INITIALIZATION_TIME_US_FIELD_NUMBER: builtins.int
    INFERENCE_TIME_US_FIELD_NUMBER: builtins.int
    MAX_MEMORY_KB_FIELD_NUMBER: builtins.int
    OK_FIELD_NUMBER: builtins.int
    METRICS_FIELD_NUMBER: builtins.int
    @property
    def initialization_time_us(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Time to load model and apply acceleration. Initialization may get run
        multiple times to get information on variance.
        """
        pass
    @property
    def inference_time_us(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Time to run inference (call Invoke()). Inference may get run multiple times
        to get information on variance.
        """
        pass
    max_memory_kb: builtins.int
    """Maximum memory used. Measures size of application heap (does not
    necessarily take into account driver-side allocation.
    """

    ok: builtins.bool
    """Whether the inference produced correct results (validation graph output
    'ok' for all test inputs). Used on-device to disallow configurations that
    produce incorrect results (e.g., due to OpenCL driver bugs).
    """

    @property
    def metrics(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___BenchmarkMetric]:
        """Metrics that were used to determine the 'ok' status."""
        pass
    def __init__(self,
        *,
        initialization_time_us: typing.Optional[typing.Iterable[builtins.int]] = ...,
        inference_time_us: typing.Optional[typing.Iterable[builtins.int]] = ...,
        max_memory_kb: typing.Optional[builtins.int] = ...,
        ok: typing.Optional[builtins.bool] = ...,
        metrics: typing.Optional[typing.Iterable[global___BenchmarkMetric]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["max_memory_kb",b"max_memory_kb","ok",b"ok"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["inference_time_us",b"inference_time_us","initialization_time_us",b"initialization_time_us","max_memory_kb",b"max_memory_kb","metrics",b"metrics","ok",b"ok"]) -> None: ...
global___BenchmarkResult = BenchmarkResult

class ErrorCode(google.protobuf.message.Message):
    """A handled error."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    SOURCE_FIELD_NUMBER: builtins.int
    TFLITE_ERROR_FIELD_NUMBER: builtins.int
    UNDERLYING_API_ERROR_FIELD_NUMBER: builtins.int
    source: global___Delegate.ValueType
    """Which delegate the error comes from (or NONE, if it comes from the tflite
    framework).
    """

    tflite_error: builtins.int
    """What the tflite level error is."""

    underlying_api_error: builtins.int
    """What the underlying error is (e.g., NNAPI or OpenGL error)."""

    def __init__(self,
        *,
        source: typing.Optional[global___Delegate.ValueType] = ...,
        tflite_error: typing.Optional[builtins.int] = ...,
        underlying_api_error: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["source",b"source","tflite_error",b"tflite_error","underlying_api_error",b"underlying_api_error"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["source",b"source","tflite_error",b"tflite_error","underlying_api_error",b"underlying_api_error"]) -> None: ...
global___ErrorCode = ErrorCode

class BenchmarkError(google.protobuf.message.Message):
    """An error that occurred during benchmarking.

    Used with event type ERROR.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    STAGE_FIELD_NUMBER: builtins.int
    EXIT_CODE_FIELD_NUMBER: builtins.int
    SIGNAL_FIELD_NUMBER: builtins.int
    ERROR_CODE_FIELD_NUMBER: builtins.int
    MINI_BENCHMARK_ERROR_CODE_FIELD_NUMBER: builtins.int
    stage: global___BenchmarkStage.ValueType
    """How far benchmarking got."""

    exit_code: builtins.int
    """Process exit code."""

    signal: builtins.int
    """Signal the process received."""

    @property
    def error_code(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ErrorCode]:
        """Handled tflite error."""
        pass
    mini_benchmark_error_code: builtins.int
    """Mini-benchmark error code."""

    def __init__(self,
        *,
        stage: typing.Optional[global___BenchmarkStage.ValueType] = ...,
        exit_code: typing.Optional[builtins.int] = ...,
        signal: typing.Optional[builtins.int] = ...,
        error_code: typing.Optional[typing.Iterable[global___ErrorCode]] = ...,
        mini_benchmark_error_code: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["exit_code",b"exit_code","mini_benchmark_error_code",b"mini_benchmark_error_code","signal",b"signal","stage",b"stage"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_code",b"error_code","exit_code",b"exit_code","mini_benchmark_error_code",b"mini_benchmark_error_code","signal",b"signal","stage",b"stage"]) -> None: ...
global___BenchmarkError = BenchmarkError

class BenchmarkEvent(google.protobuf.message.Message):
    """Top-level benchmarking event stored on-device. All events for a model are
    parsed to detect the status.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    TFLITE_SETTINGS_FIELD_NUMBER: builtins.int
    EVENT_TYPE_FIELD_NUMBER: builtins.int
    RESULT_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    BOOTTIME_US_FIELD_NUMBER: builtins.int
    WALLCLOCK_US_FIELD_NUMBER: builtins.int
    @property
    def tflite_settings(self) -> global___TFLiteSettings:
        """Which settings were used for benchmarking."""
        pass
    event_type: global___BenchmarkEventType.ValueType
    """Type of the event."""

    @property
    def result(self) -> global___BenchmarkResult:
        """Result of benchmark, used when type is END."""
        pass
    @property
    def error(self) -> global___BenchmarkError:
        """Error during benchmark, used when type is ERROR."""
        pass
    boottime_us: builtins.int
    """Start timestamps. These are used for
    1. Checking whether a test was started but not completed within a given
    deadline.
    2. Optionally, telemetry timestamps.
    """

    wallclock_us: builtins.int
    def __init__(self,
        *,
        tflite_settings: typing.Optional[global___TFLiteSettings] = ...,
        event_type: typing.Optional[global___BenchmarkEventType.ValueType] = ...,
        result: typing.Optional[global___BenchmarkResult] = ...,
        error: typing.Optional[global___BenchmarkError] = ...,
        boottime_us: typing.Optional[builtins.int] = ...,
        wallclock_us: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["boottime_us",b"boottime_us","error",b"error","event_type",b"event_type","result",b"result","tflite_settings",b"tflite_settings","wallclock_us",b"wallclock_us"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["boottime_us",b"boottime_us","error",b"error","event_type",b"event_type","result",b"result","tflite_settings",b"tflite_settings","wallclock_us",b"wallclock_us"]) -> None: ...
global___BenchmarkEvent = BenchmarkEvent

class BestAccelerationDecision(google.protobuf.message.Message):
    """Represent the decision on the best acceleration from the mini-benchmark."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    NUMBER_OF_SOURCE_EVENTS_FIELD_NUMBER: builtins.int
    MIN_LATENCY_EVENT_FIELD_NUMBER: builtins.int
    MIN_INFERENCE_TIME_US_FIELD_NUMBER: builtins.int
    number_of_source_events: builtins.int
    """Number of events used to take the decision.
    Using just the size instaed of the full list of events to save space.
    """

    @property
    def min_latency_event(self) -> global___BenchmarkEvent:
        """Event with min latency in the source ones."""
        pass
    min_inference_time_us: builtins.int
    """Min latency as read from min_latency_event."""

    def __init__(self,
        *,
        number_of_source_events: typing.Optional[builtins.int] = ...,
        min_latency_event: typing.Optional[global___BenchmarkEvent] = ...,
        min_inference_time_us: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["min_inference_time_us",b"min_inference_time_us","min_latency_event",b"min_latency_event","number_of_source_events",b"number_of_source_events"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["min_inference_time_us",b"min_inference_time_us","min_latency_event",b"min_latency_event","number_of_source_events",b"number_of_source_events"]) -> None: ...
global___BestAccelerationDecision = BestAccelerationDecision

class BenchmarkInitializationFailure(google.protobuf.message.Message):
    """Represent a failure during the initialization of the mini-benchmark."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    INITIALIZATION_STATUS_FIELD_NUMBER: builtins.int
    initialization_status: builtins.int
    """Status code returned by the mini-benchmark initialization function."""

    def __init__(self,
        *,
        initialization_status: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["initialization_status",b"initialization_status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["initialization_status",b"initialization_status"]) -> None: ...
global___BenchmarkInitializationFailure = BenchmarkInitializationFailure

class MiniBenchmarkEvent(google.protobuf.message.Message):
    """Events generated by the mini-benchmark before and after triggering
    the different configuration-specific benchmarks
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    IS_LOG_FLUSHING_EVENT_FIELD_NUMBER: builtins.int
    BEST_ACCELERATION_DECISION_FIELD_NUMBER: builtins.int
    INITIALIZATION_FAILURE_FIELD_NUMBER: builtins.int
    is_log_flushing_event: builtins.bool
    """Not using oneof because of the way the generated cpp code.
    See comment above on TfLite settings for details.
    """

    @property
    def best_acceleration_decision(self) -> global___BestAccelerationDecision: ...
    @property
    def initialization_failure(self) -> global___BenchmarkInitializationFailure: ...
    def __init__(self,
        *,
        is_log_flushing_event: typing.Optional[builtins.bool] = ...,
        best_acceleration_decision: typing.Optional[global___BestAccelerationDecision] = ...,
        initialization_failure: typing.Optional[global___BenchmarkInitializationFailure] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["best_acceleration_decision",b"best_acceleration_decision","initialization_failure",b"initialization_failure","is_log_flushing_event",b"is_log_flushing_event"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["best_acceleration_decision",b"best_acceleration_decision","initialization_failure",b"initialization_failure","is_log_flushing_event",b"is_log_flushing_event"]) -> None: ...
global___MiniBenchmarkEvent = MiniBenchmarkEvent

class ModelFile(google.protobuf.message.Message):
    """How to access the model for mini-benchmark.
    Since mini-benchmark runs in a separate process, it can not access an
    in-memory model. It can read the model either from a file or from a file
    descriptor. The file descriptor typically comes from the Android asset
    manager.

    Users should set either filename, or all of fd, offset and length.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    FILENAME_FIELD_NUMBER: builtins.int
    FD_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    LENGTH_FIELD_NUMBER: builtins.int
    filename: typing.Text
    """Filename for reading model from."""

    fd: builtins.int
    """File descriptor to read model from."""

    offset: builtins.int
    """Offset for model in file descriptor."""

    length: builtins.int
    """Length of model in file descriptor."""

    def __init__(self,
        *,
        filename: typing.Optional[typing.Text] = ...,
        fd: typing.Optional[builtins.int] = ...,
        offset: typing.Optional[builtins.int] = ...,
        length: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["fd",b"fd","filename",b"filename","length",b"length","offset",b"offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["fd",b"fd","filename",b"filename","length",b"length","offset",b"offset"]) -> None: ...
global___ModelFile = ModelFile

class BenchmarkStoragePaths(google.protobuf.message.Message):
    """Where to store mini-benchmark state."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    STORAGE_FILE_PATH_FIELD_NUMBER: builtins.int
    DATA_DIRECTORY_PATH_FIELD_NUMBER: builtins.int
    storage_file_path: typing.Text
    """Base path to the files used to to store benchmark results in. Two files
    will be generated: one with the given path and an extra file to store
    the best acceleration results events at path storage_file_path +
    ".best.fb". Must be specific to the model.
    Note on Android, this should be the code cache directory.
    """

    data_directory_path: typing.Text
    """Path to a directory for intermediate files (lock files, extracted
    binaries).
    Note on Android, this typically is the data cache directory (i.e. the one
    returned by `getCacheDir()`).
    """

    def __init__(self,
        *,
        storage_file_path: typing.Optional[typing.Text] = ...,
        data_directory_path: typing.Optional[typing.Text] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data_directory_path",b"data_directory_path","storage_file_path",b"storage_file_path"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_directory_path",b"data_directory_path","storage_file_path",b"storage_file_path"]) -> None: ...
global___BenchmarkStoragePaths = BenchmarkStoragePaths

class MinibenchmarkSettings(google.protobuf.message.Message):
    """How to run a minibenchmark."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    SETTINGS_TO_TEST_FIELD_NUMBER: builtins.int
    MODEL_FILE_FIELD_NUMBER: builtins.int
    STORAGE_PATHS_FIELD_NUMBER: builtins.int
    @property
    def settings_to_test(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TFLiteSettings]:
        """Which settings to test. This would typically be filled in from an
        allowlist.
        """
        pass
    @property
    def model_file(self) -> global___ModelFile:
        """How to access the model. This would typically be set dynamically, as it
        depends on the application folder and/or runtime state.
        """
        pass
    @property
    def storage_paths(self) -> global___BenchmarkStoragePaths:
        """Where to store state. This would typically be set dynamically, as it
        depends on the application folder.
        """
        pass
    def __init__(self,
        *,
        settings_to_test: typing.Optional[typing.Iterable[global___TFLiteSettings]] = ...,
        model_file: typing.Optional[global___ModelFile] = ...,
        storage_paths: typing.Optional[global___BenchmarkStoragePaths] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["model_file",b"model_file","storage_paths",b"storage_paths"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_file",b"model_file","settings_to_test",b"settings_to_test","storage_paths",b"storage_paths"]) -> None: ...
global___MinibenchmarkSettings = MinibenchmarkSettings
