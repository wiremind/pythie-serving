# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nEtensorflow/core/grappler/optimizers/inference/batch_op_rewriter.proto\x12\x12tensorflow.serving\x1a\x1egoogle/protobuf/wrappers.proto\"\xe1\x04\n\x14\x42\x61tchOpRewriteConfig\x12\x33\n+enable_adaptive_shared_batching_thread_pool\x18\x04 \x01(\x08\x12\x64\n\x17model_scheduler_options\x18\x01 \x03(\x0b\x32\x43.tensorflow.serving.BatchOpRewriteConfig.ModelSchedulerOptionsEntry\x1a\xa7\x02\n\x1c\x41\x64\x61ptiveBatchSchedulerOption\x12@\n\x1amin_inflight_batches_limit\x18\x01 \x01(\x0b\x32\x1c.google.protobuf.UInt32Value\x12\x44\n\x1einitial_inflight_batches_limit\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.UInt32Value\x12@\n\x1amax_inflight_batches_limit\x18\x03 \x01(\x0b\x32\x1c.google.protobuf.UInt32Value\x12=\n\x17\x62\x61tches_to_average_over\x18\x04 \x01(\x0b\x32\x1c.google.protobuf.UInt32Value\x1a\x83\x01\n\x1aModelSchedulerOptionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12T\n\x05value\x18\x02 \x01(\x0b\x32\x45.tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption:\x02\x38\x01\x62\x06proto3')



_BATCHOPREWRITECONFIG = DESCRIPTOR.message_types_by_name['BatchOpRewriteConfig']
_BATCHOPREWRITECONFIG_ADAPTIVEBATCHSCHEDULEROPTION = _BATCHOPREWRITECONFIG.nested_types_by_name['AdaptiveBatchSchedulerOption']
_BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY = _BATCHOPREWRITECONFIG.nested_types_by_name['ModelSchedulerOptionsEntry']
BatchOpRewriteConfig = _reflection.GeneratedProtocolMessageType('BatchOpRewriteConfig', (_message.Message,), {

  'AdaptiveBatchSchedulerOption' : _reflection.GeneratedProtocolMessageType('AdaptiveBatchSchedulerOption', (_message.Message,), {
    'DESCRIPTOR' : _BATCHOPREWRITECONFIG_ADAPTIVEBATCHSCHEDULEROPTION,
    '__module__' : 'tensorflow.core.grappler.optimizers.inference.batch_op_rewriter_pb2'
    # @@protoc_insertion_point(class_scope:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
    })
  ,

  'ModelSchedulerOptionsEntry' : _reflection.GeneratedProtocolMessageType('ModelSchedulerOptionsEntry', (_message.Message,), {
    'DESCRIPTOR' : _BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY,
    '__module__' : 'tensorflow.core.grappler.optimizers.inference.batch_op_rewriter_pb2'
    # @@protoc_insertion_point(class_scope:tensorflow.serving.BatchOpRewriteConfig.ModelSchedulerOptionsEntry)
    })
  ,
  'DESCRIPTOR' : _BATCHOPREWRITECONFIG,
  '__module__' : 'tensorflow.core.grappler.optimizers.inference.batch_op_rewriter_pb2'
  # @@protoc_insertion_point(class_scope:tensorflow.serving.BatchOpRewriteConfig)
  })
_sym_db.RegisterMessage(BatchOpRewriteConfig)
_sym_db.RegisterMessage(BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
_sym_db.RegisterMessage(BatchOpRewriteConfig.ModelSchedulerOptionsEntry)

if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY._options = None
  _BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY._serialized_options = b'8\001'
  _BATCHOPREWRITECONFIG._serialized_start=126
  _BATCHOPREWRITECONFIG._serialized_end=735
  _BATCHOPREWRITECONFIG_ADAPTIVEBATCHSCHEDULEROPTION._serialized_start=306
  _BATCHOPREWRITECONFIG_ADAPTIVEBATCHSCHEDULEROPTION._serialized_end=601
  _BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY._serialized_start=604
  _BATCHOPREWRITECONFIG_MODELSCHEDULEROPTIONSENTRY._serialized_end=735
# @@protoc_insertion_point(module_scope)
