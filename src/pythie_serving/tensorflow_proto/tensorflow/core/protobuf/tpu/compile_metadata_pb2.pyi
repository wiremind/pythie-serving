"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import tensorflow.compiler.xla.xla_data_pb2
import tensorflow.compiler.xla.xla_pb2
import tensorflow.core.framework.tensor_shape_pb2
import tensorflow.core.framework.types_pb2
import tensorflow.core.protobuf.tpu.dynamic_padding_pb2
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

@typing_extensions.final
class TPUCompileMetadataProto(google.protobuf.message.Message):
    """This is an experimental proto used in the TF/XLA bridge to store metadata to
    a compile op (e.g. _TPUCompileMlir).
    TODO(lyandy): Deprecate proto once generic metadata proto is created.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing_extensions.final
    class Arg(google.protobuf.message.Message):
        """Description of the types and shapes of the arguments to a computation."""

        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        class _Kind:
            ValueType = typing.NewType("ValueType", builtins.int)
            V: typing_extensions.TypeAlias = ValueType

        class _KindEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[TPUCompileMetadataProto.Arg._Kind.ValueType], builtins.type):  # noqa: F821
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            INVALID: TPUCompileMetadataProto.Arg._Kind.ValueType  # 0
            PARAMETER: TPUCompileMetadataProto.Arg._Kind.ValueType  # 1
            VARIABLE: TPUCompileMetadataProto.Arg._Kind.ValueType  # 2
            GUARANTEED_CONSTANT: TPUCompileMetadataProto.Arg._Kind.ValueType  # 3
            """These are args which have been guaranteed to be constants during the
            session lifetime by the use of the GuaranteeConstOp (or ConstantOp).
            """

        class Kind(_Kind, metaclass=_KindEnumTypeWrapper): ...
        INVALID: TPUCompileMetadataProto.Arg.Kind.ValueType  # 0
        PARAMETER: TPUCompileMetadataProto.Arg.Kind.ValueType  # 1
        VARIABLE: TPUCompileMetadataProto.Arg.Kind.ValueType  # 2
        GUARANTEED_CONSTANT: TPUCompileMetadataProto.Arg.Kind.ValueType  # 3
        """These are args which have been guaranteed to be constants during the
        session lifetime by the use of the GuaranteeConstOp (or ConstantOp).
        """

        class _EnableXlaSharding:
            ValueType = typing.NewType("ValueType", builtins.int)
            V: typing_extensions.TypeAlias = ValueType

        class _EnableXlaShardingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType], builtins.type):  # noqa: F821
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            DISALLOWED: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 0
            TENTATIVE: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 1
            """Sharding is allowed if host training loop exists."""
            ALLOWED: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 2

        class EnableXlaSharding(_EnableXlaSharding, metaclass=_EnableXlaShardingEnumTypeWrapper): ...
        DISALLOWED: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 0
        TENTATIVE: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 1
        """Sharding is allowed if host training loop exists."""
        ALLOWED: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 2

        DTYPE_FIELD_NUMBER: builtins.int
        SHAPE_FIELD_NUMBER: builtins.int
        KIND_FIELD_NUMBER: builtins.int
        SHARDING_FIELD_NUMBER: builtins.int
        IS_SAME_DATA_ACROSS_REPLICAS_FIELD_NUMBER: builtins.int
        ENABLE_XLA_SHARDING_FIELD_NUMBER: builtins.int
        RETVAL_INDEX_FOR_SHARDING_FIELD_NUMBER: builtins.int
        FAST_MEM_FIELD_NUMBER: builtins.int
        UNRESTRICTED_LAYOUT_FIELD_NUMBER: builtins.int
        NAME_FIELD_NUMBER: builtins.int
        REQUIRES_XLA_BROADCAST_FIELD_NUMBER: builtins.int
        dtype: tensorflow.core.framework.types_pb2.DataType.ValueType
        @property
        def shape(self) -> tensorflow.core.framework.tensor_shape_pb2.TensorShapeProto: ...
        kind: global___TPUCompileMetadataProto.Arg.Kind.ValueType
        @property
        def sharding(self) -> tensorflow.compiler.xla.xla_data_pb2.OpSharding:
            """The cross-core sharding of this input within each replica, e.g.,
            assigning to one core, or replicate across all cores.
            """
        is_same_data_across_replicas: builtins.bool
        """Whether this argument will receive the same data across all replicas."""
        enable_xla_sharding: global___TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType
        """Whether to allow XLA to produce separate programs to shard/unshard this
        argument. Requires this arg to be an on-device Kind::VARIABLE, or a
        Kind::PARAMETER. For Kind::PARAMETER, it represents the initial value of
        a variable, and retval_index_for_sharding must be specified for the
        corresponding updated value.
        """
        retval_index_for_sharding: builtins.int
        """If XLA sharding is allowed on a Kind::PARAMETER, this field is used to
        specify the corresponding updated value in the return values. Use -1 for
        variables that are not updated.
        """
        fast_mem: builtins.bool
        """Whether this argument is placed on fast memory or not."""
        unrestricted_layout: builtins.bool
        """Whether to let XLA to decide the layout during compilation, as opposed to
        using a fixed layout determined by the shape.
        """
        name: builtins.str
        """Name of the node that the arg comes from."""
        requires_xla_broadcast: builtins.bool
        """Whether to use XLA collectives to broadcast this parameter to all
        replicas, instead of using TensorFlow Send/Recv among the tasks.
        """
        def __init__(
            self,
            *,
            dtype: tensorflow.core.framework.types_pb2.DataType.ValueType = ...,
            shape: tensorflow.core.framework.tensor_shape_pb2.TensorShapeProto | None = ...,
            kind: global___TPUCompileMetadataProto.Arg.Kind.ValueType = ...,
            sharding: tensorflow.compiler.xla.xla_data_pb2.OpSharding | None = ...,
            is_same_data_across_replicas: builtins.bool = ...,
            enable_xla_sharding: global___TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType = ...,
            retval_index_for_sharding: builtins.int = ...,
            fast_mem: builtins.bool = ...,
            unrestricted_layout: builtins.bool = ...,
            name: builtins.str = ...,
            requires_xla_broadcast: builtins.bool = ...,
        ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["shape", b"shape", "sharding", b"sharding"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["dtype", b"dtype", "enable_xla_sharding", b"enable_xla_sharding", "fast_mem", b"fast_mem", "is_same_data_across_replicas", b"is_same_data_across_replicas", "kind", b"kind", "name", b"name", "requires_xla_broadcast", b"requires_xla_broadcast", "retval_index_for_sharding", b"retval_index_for_sharding", "shape", b"shape", "sharding", b"sharding", "unrestricted_layout", b"unrestricted_layout"]) -> None: ...

    @typing_extensions.final
    class Retval(google.protobuf.message.Message):
        """Description of the return values from a computation."""

        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        SHARDING_FIELD_NUMBER: builtins.int
        @property
        def sharding(self) -> tensorflow.compiler.xla.xla_data_pb2.OpSharding:
            """The cross-core sharding of this return value within each replica, e.g.,
            assigning to one core, or replicate across all cores.
            """
        def __init__(
            self,
            *,
            sharding: tensorflow.compiler.xla.xla_data_pb2.OpSharding | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["sharding", b"sharding"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["sharding", b"sharding"]) -> None: ...

    ARGS_FIELD_NUMBER: builtins.int
    RETVALS_FIELD_NUMBER: builtins.int
    NUM_REPLICAS_FIELD_NUMBER: builtins.int
    NUM_CORES_PER_REPLICA_FIELD_NUMBER: builtins.int
    DEVICE_ASSIGNMENT_FIELD_NUMBER: builtins.int
    FUNCTION_LIBRARY_FINGERPRINT_FIELD_NUMBER: builtins.int
    SESSION_HANDLE_FIELD_NUMBER: builtins.int
    GUARANTEED_CONST_FINGERPRINT_FIELD_NUMBER: builtins.int
    PADDING_MAPS_FIELD_NUMBER: builtins.int
    STEP_MARKER_LOCATION_FIELD_NUMBER: builtins.int
    XLA_FUSION_AUTOTUNER_THRESH_FIELD_NUMBER: builtins.int
    ENABLE_AUTOMATIC_MODEL_PARALLELISM_FIELD_NUMBER: builtins.int
    USE_SPMD_FOR_XLA_PARTITIONING_FIELD_NUMBER: builtins.int
    USE_AUTO_SPMD_FOR_XLA_PARTITIONING_FIELD_NUMBER: builtins.int
    AUTO_SPMD_MESH_SHAPE_FIELD_NUMBER: builtins.int
    AUTO_SPMD_MESH_IDS_FIELD_NUMBER: builtins.int
    MLIR_FINGERPRINT_FIELD_NUMBER: builtins.int
    COMPILE_OPTIONS_FIELD_NUMBER: builtins.int
    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TPUCompileMetadataProto.Arg]: ...
    @property
    def retvals(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TPUCompileMetadataProto.Retval]: ...
    num_replicas: builtins.int
    """Number of replicas of the computation and number of cores in each replica.
    TODO(b/140721404): it may not be necessary to state the number of cores per
    replica here. Reconsider when replicated model-parallelism is implemented
    in XLA.
    """
    num_cores_per_replica: builtins.int
    @property
    def device_assignment(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto: ...
    function_library_fingerprint: builtins.int
    """A fingerprint of the function library. Ensures that any functions called
    by the computation have matching definitions.
    """
    session_handle: builtins.str
    """Unique session identifier. Can be empty."""
    guaranteed_const_fingerprint: builtins.str
    """Fingerprint of guaranteed_const value. The fingerprint computation inside
    tpu_compile_op may be slow. The computation can be avoided by setting the
    fingerprint value here.
    """
    @property
    def padding_maps(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.core.protobuf.tpu.dynamic_padding_pb2.PaddingMap]: ...
    step_marker_location: tensorflow.compiler.xla.xla_pb2.DebugOptions.StepMarkerLocation.ValueType
    """The location of step markers that XLA compile will instrument."""
    xla_fusion_autotuner_thresh: builtins.int
    """Minimum number of batches run through the XLA graph before XLA fusion
    autotuner is enabled. Default value of zero disables the autotuner.
    The XLA fusion autotuner can improve performance by executing a heuristic
    search on the compiler parameters.
    """
    enable_automatic_model_parallelism: builtins.bool
    """Enables TPU compiler to add partitioning policies for inputs/outputs to
    the XLA computation for model parallelism.
    """
    use_spmd_for_xla_partitioning: builtins.bool
    """Whether to use XLA's SPMD or MPMD partitioner when compiler partitioning is
    requested.
    """
    use_auto_spmd_for_xla_partitioning: builtins.bool
    """Whether to automatically generate XLA shardings for SPMD partitioner."""
    @property
    def auto_spmd_mesh_shape(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Device mesh shape used to create the sharding search space when
        use_auto_spmd_partitioning=true.
        """
    @property
    def auto_spmd_mesh_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Device mesh ids compatible with the above mesh_shape used when
        use_auto_spmd_partitioning=true.
        """
    mlir_fingerprint: builtins.int
    """A fingerprint generated by hashing the MLIR module content."""
    @property
    def compile_options(self) -> global___TPUCompileOptions: ...
    def __init__(
        self,
        *,
        args: collections.abc.Iterable[global___TPUCompileMetadataProto.Arg] | None = ...,
        retvals: collections.abc.Iterable[global___TPUCompileMetadataProto.Retval] | None = ...,
        num_replicas: builtins.int = ...,
        num_cores_per_replica: builtins.int = ...,
        device_assignment: tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto | None = ...,
        function_library_fingerprint: builtins.int = ...,
        session_handle: builtins.str = ...,
        guaranteed_const_fingerprint: builtins.str = ...,
        padding_maps: collections.abc.Iterable[tensorflow.core.protobuf.tpu.dynamic_padding_pb2.PaddingMap] | None = ...,
        step_marker_location: tensorflow.compiler.xla.xla_pb2.DebugOptions.StepMarkerLocation.ValueType = ...,
        xla_fusion_autotuner_thresh: builtins.int = ...,
        enable_automatic_model_parallelism: builtins.bool = ...,
        use_spmd_for_xla_partitioning: builtins.bool = ...,
        use_auto_spmd_for_xla_partitioning: builtins.bool = ...,
        auto_spmd_mesh_shape: collections.abc.Iterable[builtins.int] | None = ...,
        auto_spmd_mesh_ids: collections.abc.Iterable[builtins.int] | None = ...,
        mlir_fingerprint: builtins.int = ...,
        compile_options: global___TPUCompileOptions | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["compile_options", b"compile_options", "device_assignment", b"device_assignment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["args", b"args", "auto_spmd_mesh_ids", b"auto_spmd_mesh_ids", "auto_spmd_mesh_shape", b"auto_spmd_mesh_shape", "compile_options", b"compile_options", "device_assignment", b"device_assignment", "enable_automatic_model_parallelism", b"enable_automatic_model_parallelism", "function_library_fingerprint", b"function_library_fingerprint", "guaranteed_const_fingerprint", b"guaranteed_const_fingerprint", "mlir_fingerprint", b"mlir_fingerprint", "num_cores_per_replica", b"num_cores_per_replica", "num_replicas", b"num_replicas", "padding_maps", b"padding_maps", "retvals", b"retvals", "session_handle", b"session_handle", "step_marker_location", b"step_marker_location", "use_auto_spmd_for_xla_partitioning", b"use_auto_spmd_for_xla_partitioning", "use_spmd_for_xla_partitioning", b"use_spmd_for_xla_partitioning", "xla_fusion_autotuner_thresh", b"xla_fusion_autotuner_thresh"]) -> None: ...

global___TPUCompileMetadataProto = TPUCompileMetadataProto

@typing_extensions.final
class TPUCompileOptions(google.protobuf.message.Message):
    """Stable protobuf for TPU compilation options, suitable for persistent storage.
    This proto needs to be backward compatible under maintenance.
    TODO(timshen): investigate and migrate other options from
    TPUCompileMetadataProto.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _Precision:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _PrecisionEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[TPUCompileOptions._Precision.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DEFAULT: TPUCompileOptions._Precision.ValueType  # 0
        BFLOAT16: TPUCompileOptions._Precision.ValueType  # 1
        FLOAT32: TPUCompileOptions._Precision.ValueType  # 2
        TENSOR_FLOAT32: TPUCompileOptions._Precision.ValueType  # 3

    class Precision(_Precision, metaclass=_PrecisionEnumTypeWrapper): ...
    DEFAULT: TPUCompileOptions.Precision.ValueType  # 0
    BFLOAT16: TPUCompileOptions.Precision.ValueType  # 1
    FLOAT32: TPUCompileOptions.Precision.ValueType  # 2
    TENSOR_FLOAT32: TPUCompileOptions.Precision.ValueType  # 3

    MATRIX_UNIT_OPERAND_PRECISION_FIELD_NUMBER: builtins.int
    matrix_unit_operand_precision: global___TPUCompileOptions.Precision.ValueType
    def __init__(
        self,
        *,
        matrix_unit_operand_precision: global___TPUCompileOptions.Precision.ValueType = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["matrix_unit_operand_precision", b"matrix_unit_operand_precision"]) -> None: ...

global___TPUCompileOptions = TPUCompileOptions
