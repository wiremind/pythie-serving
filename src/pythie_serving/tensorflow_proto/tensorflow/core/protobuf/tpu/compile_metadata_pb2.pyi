"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import tensorflow.compiler.xla.xla_data_pb2
import tensorflow.compiler.xla.xla_pb2
import tensorflow.core.framework.tensor_shape_pb2
import tensorflow.core.framework.types_pb2
import tensorflow.core.protobuf.tpu.dynamic_padding_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class TPUCompileMetadataProto(google.protobuf.message.Message):
    """This is an experimental proto used in the TF/XLA bridge to store metadata to
    a compile op (e.g. _TPUCompileMlir).
    TODO(lyandy): Deprecate proto once generic metadata proto is created.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class Arg(google.protobuf.message.Message):
        """Description of the types and shapes of the arguments to a computation."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor
        class _Kind:
            ValueType = typing.NewType('ValueType', builtins.int)
            V: typing_extensions.TypeAlias = ValueType
        class _KindEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[TPUCompileMetadataProto.Arg._Kind.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            INVALID: TPUCompileMetadataProto.Arg._Kind.ValueType  # 0
            PARAMETER: TPUCompileMetadataProto.Arg._Kind.ValueType  # 1
            VARIABLE: TPUCompileMetadataProto.Arg._Kind.ValueType  # 2
            GUARANTEED_CONSTANT: TPUCompileMetadataProto.Arg._Kind.ValueType  # 3
            """These are args which have been guaranteed to be constants during the
            session lifetime by the use of the GuaranteeConstOp (or ConstantOp).
            """

        class Kind(_Kind, metaclass=_KindEnumTypeWrapper):
            pass

        INVALID: TPUCompileMetadataProto.Arg.Kind.ValueType  # 0
        PARAMETER: TPUCompileMetadataProto.Arg.Kind.ValueType  # 1
        VARIABLE: TPUCompileMetadataProto.Arg.Kind.ValueType  # 2
        GUARANTEED_CONSTANT: TPUCompileMetadataProto.Arg.Kind.ValueType  # 3
        """These are args which have been guaranteed to be constants during the
        session lifetime by the use of the GuaranteeConstOp (or ConstantOp).
        """


        class _EnableXlaSharding:
            ValueType = typing.NewType('ValueType', builtins.int)
            V: typing_extensions.TypeAlias = ValueType
        class _EnableXlaShardingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            DISALLOWED: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 0
            TENTATIVE: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 1
            """Sharding is allowed if host training loop exists."""

            ALLOWED: TPUCompileMetadataProto.Arg._EnableXlaSharding.ValueType  # 2
        class EnableXlaSharding(_EnableXlaSharding, metaclass=_EnableXlaShardingEnumTypeWrapper):
            pass

        DISALLOWED: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 0
        TENTATIVE: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 1
        """Sharding is allowed if host training loop exists."""

        ALLOWED: TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType  # 2

        DTYPE_FIELD_NUMBER: builtins.int
        SHAPE_FIELD_NUMBER: builtins.int
        KIND_FIELD_NUMBER: builtins.int
        SHARDING_FIELD_NUMBER: builtins.int
        IS_SAME_DATA_ACROSS_REPLICAS_FIELD_NUMBER: builtins.int
        ENABLE_XLA_SHARDING_FIELD_NUMBER: builtins.int
        RETVAL_INDEX_FOR_SHARDING_FIELD_NUMBER: builtins.int
        FAST_MEM_FIELD_NUMBER: builtins.int
        UNRESTRICTED_LAYOUT_FIELD_NUMBER: builtins.int
        NAME_FIELD_NUMBER: builtins.int
        REQUIRES_XLA_BROADCAST_FIELD_NUMBER: builtins.int
        dtype: tensorflow.core.framework.types_pb2.DataType.ValueType
        @property
        def shape(self) -> tensorflow.core.framework.tensor_shape_pb2.TensorShapeProto: ...
        kind: global___TPUCompileMetadataProto.Arg.Kind.ValueType
        @property
        def sharding(self) -> tensorflow.compiler.xla.xla_data_pb2.OpSharding:
            """The cross-core sharding of this input within each replica, e.g.,
            assigning to one core, or replicate across all cores.
            """
            pass
        is_same_data_across_replicas: builtins.bool
        """Whether this argument will receive the same data across all replicas."""

        enable_xla_sharding: global___TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType
        """Whether to allow XLA to produce separate programs to shard/unshard this
        argument. Requires this arg to be an on-device Kind::VARIABLE, or a
        Kind::PARAMETER. For Kind::PARAMETER, it represents the initial value of
        a variable, and retval_index_for_sharding must be specified for the
        corresponding updated value.
        """

        retval_index_for_sharding: builtins.int
        """If XLA sharding is allowed on a Kind::PARAMETER, this field is used to
        specify the corresponding updated value in the return values. Use -1 for
        variables that are not updated.
        """

        fast_mem: builtins.bool
        """Whether this argument is placed on fast memory or not."""

        unrestricted_layout: builtins.bool
        """Whether to let XLA to decide the layout during compilation, as opposed to
        using a fixed layout determined by the shape.
        """

        name: typing.Text
        """Name of the node that the arg comes from."""

        requires_xla_broadcast: builtins.bool
        """Whether to use XLA collectives to broadcast this parameter to all
        replicas, instead of using TensorFlow Send/Recv among the tasks.
        """

        def __init__(self,
            *,
            dtype: tensorflow.core.framework.types_pb2.DataType.ValueType = ...,
            shape: typing.Optional[tensorflow.core.framework.tensor_shape_pb2.TensorShapeProto] = ...,
            kind: global___TPUCompileMetadataProto.Arg.Kind.ValueType = ...,
            sharding: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.OpSharding] = ...,
            is_same_data_across_replicas: builtins.bool = ...,
            enable_xla_sharding: global___TPUCompileMetadataProto.Arg.EnableXlaSharding.ValueType = ...,
            retval_index_for_sharding: builtins.int = ...,
            fast_mem: builtins.bool = ...,
            unrestricted_layout: builtins.bool = ...,
            name: typing.Text = ...,
            requires_xla_broadcast: builtins.bool = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["shape",b"shape","sharding",b"sharding"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["dtype",b"dtype","enable_xla_sharding",b"enable_xla_sharding","fast_mem",b"fast_mem","is_same_data_across_replicas",b"is_same_data_across_replicas","kind",b"kind","name",b"name","requires_xla_broadcast",b"requires_xla_broadcast","retval_index_for_sharding",b"retval_index_for_sharding","shape",b"shape","sharding",b"sharding","unrestricted_layout",b"unrestricted_layout"]) -> None: ...

    class Retval(google.protobuf.message.Message):
        """Description of the return values from a computation."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor
        SHARDING_FIELD_NUMBER: builtins.int
        @property
        def sharding(self) -> tensorflow.compiler.xla.xla_data_pb2.OpSharding:
            """The cross-core sharding of this return value within each replica, e.g.,
            assigning to one core, or replicate across all cores.
            """
            pass
        def __init__(self,
            *,
            sharding: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.OpSharding] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["sharding",b"sharding"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["sharding",b"sharding"]) -> None: ...

    ARGS_FIELD_NUMBER: builtins.int
    RETVALS_FIELD_NUMBER: builtins.int
    NUM_REPLICAS_FIELD_NUMBER: builtins.int
    NUM_CORES_PER_REPLICA_FIELD_NUMBER: builtins.int
    DEVICE_ASSIGNMENT_FIELD_NUMBER: builtins.int
    FUNCTION_LIBRARY_FINGERPRINT_FIELD_NUMBER: builtins.int
    SESSION_HANDLE_FIELD_NUMBER: builtins.int
    GUARANTEED_CONST_FINGERPRINT_FIELD_NUMBER: builtins.int
    PADDING_MAPS_FIELD_NUMBER: builtins.int
    STEP_MARKER_LOCATION_FIELD_NUMBER: builtins.int
    XLA_FUSION_AUTOTUNER_THRESH_FIELD_NUMBER: builtins.int
    ENABLE_AUTOMATIC_MODEL_PARALLELISM_FIELD_NUMBER: builtins.int
    USE_SPMD_FOR_XLA_PARTITIONING_FIELD_NUMBER: builtins.int
    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TPUCompileMetadataProto.Arg]: ...
    @property
    def retvals(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TPUCompileMetadataProto.Retval]: ...
    num_replicas: builtins.int
    """Number of replicas of the computation and number of cores in each replica.
    TODO(b/140721404): it may not be necessary to state the number of cores per
    replica here. Reconsider when replicated model-parallelism is implemented
    in XLA.
    """

    num_cores_per_replica: builtins.int
    @property
    def device_assignment(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto: ...
    function_library_fingerprint: builtins.int
    """A fingerprint of the function library. Ensures that any functions called
    by the computation have matching definitions.
    """

    session_handle: typing.Text
    """Unique session identifier. Can be empty."""

    guaranteed_const_fingerprint: typing.Text
    """Fingerprint of guaranteed_const value. The fingerprint computation inside
    tpu_compile_op may be slow. The computation can be avoided by setting the
    fingerprint value here.
    """

    @property
    def padding_maps(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.core.protobuf.tpu.dynamic_padding_pb2.PaddingMap]: ...
    step_marker_location: tensorflow.compiler.xla.xla_pb2.DebugOptions.StepMarkerLocation.ValueType
    """The location of step markers that XLA compile will instrument."""

    xla_fusion_autotuner_thresh: builtins.int
    """Minimum number of batches run through the XLA graph before XLA fusion
    autotuner is enabled. Default value of zero disables the autotuner.
    The XLA fusion autotuner can improve performance by executing a heuristic
    search on the compiler parameters.
    """

    enable_automatic_model_parallelism: builtins.bool
    """Enables TPU compiler to add partitioning policies for inputs/outputs to
    the XLA computation for model parallelism.
    """

    use_spmd_for_xla_partitioning: builtins.bool
    """Whether to use XLA's SPMD or MPMD partitioner when compiler partitioning is
    requested.
    """

    def __init__(self,
        *,
        args: typing.Optional[typing.Iterable[global___TPUCompileMetadataProto.Arg]] = ...,
        retvals: typing.Optional[typing.Iterable[global___TPUCompileMetadataProto.Retval]] = ...,
        num_replicas: builtins.int = ...,
        num_cores_per_replica: builtins.int = ...,
        device_assignment: typing.Optional[tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto] = ...,
        function_library_fingerprint: builtins.int = ...,
        session_handle: typing.Text = ...,
        guaranteed_const_fingerprint: typing.Text = ...,
        padding_maps: typing.Optional[typing.Iterable[tensorflow.core.protobuf.tpu.dynamic_padding_pb2.PaddingMap]] = ...,
        step_marker_location: tensorflow.compiler.xla.xla_pb2.DebugOptions.StepMarkerLocation.ValueType = ...,
        xla_fusion_autotuner_thresh: builtins.int = ...,
        enable_automatic_model_parallelism: builtins.bool = ...,
        use_spmd_for_xla_partitioning: builtins.bool = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device_assignment",b"device_assignment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["args",b"args","device_assignment",b"device_assignment","enable_automatic_model_parallelism",b"enable_automatic_model_parallelism","function_library_fingerprint",b"function_library_fingerprint","guaranteed_const_fingerprint",b"guaranteed_const_fingerprint","num_cores_per_replica",b"num_cores_per_replica","num_replicas",b"num_replicas","padding_maps",b"padding_maps","retvals",b"retvals","session_handle",b"session_handle","step_marker_location",b"step_marker_location","use_spmd_for_xla_partitioning",b"use_spmd_for_xla_partitioning","xla_fusion_autotuner_thresh",b"xla_fusion_autotuner_thresh"]) -> None: ...
global___TPUCompileMetadataProto = TPUCompileMetadataProto
