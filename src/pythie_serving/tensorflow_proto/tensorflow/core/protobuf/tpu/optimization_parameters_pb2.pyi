"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.wrappers_pb2
import tensorflow.compiler.xla.service.hlo_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class ClippingLimits(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    LOWER_FIELD_NUMBER: builtins.int
    UPPER_FIELD_NUMBER: builtins.int
    @property
    def lower(self) -> google.protobuf.wrappers_pb2.FloatValue:
        """-inf if not set"""
        pass
    @property
    def upper(self) -> google.protobuf.wrappers_pb2.FloatValue:
        """+inf if not set"""
        pass
    def __init__(self,
        *,
        lower: typing.Optional[google.protobuf.wrappers_pb2.FloatValue] = ...,
        upper: typing.Optional[google.protobuf.wrappers_pb2.FloatValue] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["lower",b"lower","upper",b"upper"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["lower",b"lower","upper",b"upper"]) -> None: ...
global___ClippingLimits = ClippingLimits

class DynamicLearningRate(google.protobuf.message.Message):
    """Dynamic learning rate specification in the TPUEmbeddingConfiguration. The
    actual learning rates are provided as a scalar input list to the
    SendTPUEmbeddingGradients Op indexed by their tag specified through the
    following proto.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    TAG_FIELD_NUMBER: builtins.int
    tag: builtins.int
    """For tables where learning rates are dynamically computed and communicated
    to the TPU embedding program, a tag must be specified for the learning
    rate.

    The tag must be a non-negative  integer. The total number of unique tags
    must be less than or equal to the number of tables in the TPU embedding
    configuration (a table does not specify any tag if it uses a constant
    learning rate, and specifies exactly one tag if it uses dynamic learning
    rates).

    All tags in the range [0, number_of_unique_tags) must be present in the TPU
    embedding configuration, i.e. a tag cannot be skipped if a different tag
    numerically greater than it is used in the configuration.

    If multiple tables specify the same tag, they *MUST* have
    the same dynamic learning rate, for example, their dynamic learning rate
    could be computed by the same TensorFlow sub-graph. The partitioning of the
    embedding layer would be more optimal if the number_of_unique_tags is as
    *LOW* as possible, i.e., if many tables share the same tag.

    The learning_rate input of the SendTPUEmbeddingGradients op is used to
    communicate dynamic learning rates to the TPU embedding program.
    The learning_rate input is a list of scalars where the size of the list is
    equal to the number of unique tags. The learning rate associated with a
    particular tag is specified by populating its corresponding index in the
    list of learning_rate scalars.
    """

    def __init__(self,
        *,
        tag: builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["tag",b"tag"]) -> None: ...
global___DynamicLearningRate = DynamicLearningRate

class LearningRate(google.protobuf.message.Message):
    """Source of learning rate to use."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    CONSTANT_FIELD_NUMBER: builtins.int
    DYNAMIC_FIELD_NUMBER: builtins.int
    constant: builtins.float
    @property
    def dynamic(self) -> global___DynamicLearningRate: ...
    def __init__(self,
        *,
        constant: builtins.float = ...,
        dynamic: typing.Optional[global___DynamicLearningRate] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["constant",b"constant","dynamic",b"dynamic","learning_rate",b"learning_rate"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["constant",b"constant","dynamic",b"dynamic","learning_rate",b"learning_rate"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["learning_rate",b"learning_rate"]) -> typing.Optional[typing_extensions.Literal["constant","dynamic"]]: ...
global___LearningRate = LearningRate

class AdagradParameters(google.protobuf.message.Message):
    """Each optimizer's parameter proto has a link to its documentation and CPU
    implementation (if available) for user reference.

    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L1634
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    def __init__(self,
        ) -> None: ...
global___AdagradParameters = AdagradParameters

class BoundedAdagradParameters(google.protobuf.message.Message):
    """Algorithm in http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    UPDATE_ACCUMULATOR_FIRST_FIELD_NUMBER: builtins.int
    MAX_VAR_UPDATE_FIELD_NUMBER: builtins.int
    MAX_ACCUMULATOR_FIELD_NUMBER: builtins.int
    update_accumulator_first: builtins.bool
    """Whether to use the updated or the old value of the accumulator when
    computing the effective learning rate. When update_accumulator_first is set
    to True, the updated value of the accumulator is used.
    """

    max_var_update: builtins.float
    """The max_var_update value to use. Set value to 0 (default) to disable using
    max_var_update to clip the gradient.
    """

    max_accumulator: builtins.float
    """The maximum value of the accumulator. Set max_accumulator to 0 (default)
    to disable using max_accumulator to clip the accumulator.
    """

    def __init__(self,
        *,
        update_accumulator_first: builtins.bool = ...,
        max_var_update: builtins.float = ...,
        max_accumulator: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_accumulator",b"max_accumulator","max_var_update",b"max_var_update","update_accumulator_first",b"update_accumulator_first"]) -> None: ...
global___BoundedAdagradParameters = BoundedAdagradParameters

class StochasticGradientDescentParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L629
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    def __init__(self,
        ) -> None: ...
global___StochasticGradientDescentParameters = StochasticGradientDescentParameters

class FtrlParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl
    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L2646

    The hyperparameters for FTRL are the same as for the Keras implementation,
    with some additions. The "beta" parameter matches the behavior described in
    the second link above; "beta" / (2 * learning rate) should be added to "l2"
    to get equivalent behavior in the other TensorFlow implementations of this
    optimizer. When the multiply_linear_by_lr field is set to true, a modified
    formula is used for FTRL that treats the "linear" accumulator as being
    pre-multiplied by the learning rate (i.e., the accumulator named "linear"
    actually stores "linear * learning_rate"). Other than checkpoint
    compatibility, this is mathematically equivalent for a static learning rate;
    for a dynamic learning rate, it is nearly the same as long as the learning
    rate does not change quickly. The benefit of setting multiply_linear_by_lr to
    true is that the modified formula handles zero and near-zero learning rates
    without producing NaNs, improving flexibility for learning rate ramp-up. The
    allow_zero_accumulator parameter changes some internal formulas to allow zero
    and near-zero accumulator values at the cost of some performance; this only
    needs to be set if you are using an initial accumulator value of zero, which
    is uncommon.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    L1_FIELD_NUMBER: builtins.int
    L2_FIELD_NUMBER: builtins.int
    LR_POWER_FIELD_NUMBER: builtins.int
    BETA_FIELD_NUMBER: builtins.int
    MULTIPLY_LINEAR_BY_LR_FIELD_NUMBER: builtins.int
    ALLOW_ZERO_ACCUMULATOR_FIELD_NUMBER: builtins.int
    l1: builtins.float
    l2: builtins.float
    lr_power: builtins.float
    beta: builtins.float
    multiply_linear_by_lr: builtins.bool
    allow_zero_accumulator: builtins.bool
    def __init__(self,
        *,
        l1: builtins.float = ...,
        l2: builtins.float = ...,
        lr_power: builtins.float = ...,
        beta: builtins.float = ...,
        multiply_linear_by_lr: builtins.bool = ...,
        allow_zero_accumulator: builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["allow_zero_accumulator",b"allow_zero_accumulator","beta",b"beta","l1",b"l1","l2",b"l2","lr_power",b"lr_power","multiply_linear_by_lr",b"multiply_linear_by_lr"]) -> None: ...
global___FtrlParameters = FtrlParameters

class AdamParameters(google.protobuf.message.Message):
    """The Adam optimizer does not implement hyper-parameter update due to hardware
    limitations; use the dynamic learning rate feature instead, setting the
    learning rate to: user learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
    Here, t is the current timestep.

    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam
    https://github.com/tensorflow/tensorflow/blob/ab51450c817674c8ff08a7ae4f8ac50cdc4bed8b/tensorflow/python/training/adam.py#L32

    Note that the code by default implements the lazy version of Adam
    (https://www.tensorflow.org/api_docs/python/tf/contrib/opt/LazyAdamOptimizer)
    unless the use_non_lazy_adam parameter is set, in which case it implements
    the normal version of Adam that updates all parameters in the embedding
    table, even for entries that are not used in the current minibatch
    (https://www.tensorflow.org/api_docs/python/tf/contrib/opt/AdamOptimizer). If
    use_non_lazy_adam is enabled, gradient accumulation is also required to be
    enabled in order to get correct results; a warning will be printed otherwise
    (which may change to an error in the future). If use_sum_inside_sqrt is set,
    the Adam variable update formula will be changed from m / (sqrt(v) + epsilon)
    to m / sqrt(v + epsilon**2); this option improves the performance of TPU
    training and is not expected to harm model quality.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    BETA1_FIELD_NUMBER: builtins.int
    BETA2_FIELD_NUMBER: builtins.int
    EPSILON_FIELD_NUMBER: builtins.int
    USE_NON_LAZY_ADAM_FIELD_NUMBER: builtins.int
    USE_SUM_INSIDE_SQRT_FIELD_NUMBER: builtins.int
    beta1: builtins.float
    beta2: builtins.float
    epsilon: builtins.float
    use_non_lazy_adam: builtins.bool
    use_sum_inside_sqrt: builtins.bool
    def __init__(self,
        *,
        beta1: builtins.float = ...,
        beta2: builtins.float = ...,
        epsilon: builtins.float = ...,
        use_non_lazy_adam: builtins.bool = ...,
        use_sum_inside_sqrt: builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["beta1",b"beta1","beta2",b"beta2","epsilon",b"epsilon","use_non_lazy_adam",b"use_non_lazy_adam","use_sum_inside_sqrt",b"use_sum_inside_sqrt"]) -> None: ...
global___AdamParameters = AdamParameters

class MomentumParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L3068
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    MOMENTUM_FIELD_NUMBER: builtins.int
    USE_NESTEROV_FIELD_NUMBER: builtins.int
    momentum: builtins.float
    use_nesterov: builtins.bool
    def __init__(self,
        *,
        momentum: builtins.float = ...,
        use_nesterov: builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["momentum",b"momentum","use_nesterov",b"use_nesterov"]) -> None: ...
global___MomentumParameters = MomentumParameters

class RmsPropParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L4229
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    RHO_FIELD_NUMBER: builtins.int
    MOMENTUM_FIELD_NUMBER: builtins.int
    EPSILON_FIELD_NUMBER: builtins.int
    rho: builtins.float
    momentum: builtins.float
    epsilon: builtins.float
    def __init__(self,
        *,
        rho: builtins.float = ...,
        momentum: builtins.float = ...,
        epsilon: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["epsilon",b"epsilon","momentum",b"momentum","rho",b"rho"]) -> None: ...
global___RmsPropParameters = RmsPropParameters

class CenteredRmsPropParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L4358
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    RHO_FIELD_NUMBER: builtins.int
    MOMENTUM_FIELD_NUMBER: builtins.int
    EPSILON_FIELD_NUMBER: builtins.int
    rho: builtins.float
    momentum: builtins.float
    epsilon: builtins.float
    def __init__(self,
        *,
        rho: builtins.float = ...,
        momentum: builtins.float = ...,
        epsilon: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["epsilon",b"epsilon","momentum",b"momentum","rho",b"rho"]) -> None: ...
global___CenteredRmsPropParameters = CenteredRmsPropParameters

class MdlAdagradLightParameters(google.protobuf.message.Message):
    """Variant of algorithm in http://proceedings.mlr.press/v44/shamir15.pdf"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    L2_FIELD_NUMBER: builtins.int
    LR_POWER_FIELD_NUMBER: builtins.int
    MIN_SERVABLE_MDL_BENEFIT_FIELD_NUMBER: builtins.int
    MDL_MIX_IN_MARGIN_FIELD_NUMBER: builtins.int
    MDL_BENEFIT_RAMPUP_COEFF_FIELD_NUMBER: builtins.int
    MDL_MIN_WEIGHT_FIELD_NUMBER: builtins.int
    BENEFIT_REVISIT_SCALE_FIELD_NUMBER: builtins.int
    MAX_EVENT_BENEFIT_FIELD_NUMBER: builtins.int
    MAX_TOTAL_BENEFIT_FIELD_NUMBER: builtins.int
    MDL_HARD_LIMIT_FIELD_NUMBER: builtins.int
    HARD_LIMIT_MIN_BENEFIT_FIELD_NUMBER: builtins.int
    MDL_REGULARIZE_FIELD_NUMBER: builtins.int
    l2: builtins.float
    lr_power: builtins.float
    min_servable_mdl_benefit: builtins.float
    mdl_mix_in_margin: builtins.float
    mdl_benefit_rampup_coeff: builtins.float
    mdl_min_weight: builtins.float
    benefit_revisit_scale: builtins.float
    max_event_benefit: builtins.float
    max_total_benefit: builtins.float
    mdl_hard_limit: builtins.float
    hard_limit_min_benefit: builtins.bool
    mdl_regularize: builtins.bool
    def __init__(self,
        *,
        l2: builtins.float = ...,
        lr_power: builtins.float = ...,
        min_servable_mdl_benefit: builtins.float = ...,
        mdl_mix_in_margin: builtins.float = ...,
        mdl_benefit_rampup_coeff: builtins.float = ...,
        mdl_min_weight: builtins.float = ...,
        benefit_revisit_scale: builtins.float = ...,
        max_event_benefit: builtins.float = ...,
        max_total_benefit: builtins.float = ...,
        mdl_hard_limit: builtins.float = ...,
        hard_limit_min_benefit: builtins.bool = ...,
        mdl_regularize: builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["benefit_revisit_scale",b"benefit_revisit_scale","hard_limit_min_benefit",b"hard_limit_min_benefit","l2",b"l2","lr_power",b"lr_power","max_event_benefit",b"max_event_benefit","max_total_benefit",b"max_total_benefit","mdl_benefit_rampup_coeff",b"mdl_benefit_rampup_coeff","mdl_hard_limit",b"mdl_hard_limit","mdl_min_weight",b"mdl_min_weight","mdl_mix_in_margin",b"mdl_mix_in_margin","mdl_regularize",b"mdl_regularize","min_servable_mdl_benefit",b"min_servable_mdl_benefit"]) -> None: ...
global___MdlAdagradLightParameters = MdlAdagradLightParameters

class AdadeltaParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L933
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    RHO_FIELD_NUMBER: builtins.int
    EPSILON_FIELD_NUMBER: builtins.int
    rho: builtins.float
    epsilon: builtins.float
    def __init__(self,
        *,
        rho: builtins.float = ...,
        epsilon: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["epsilon",b"epsilon","rho",b"rho"]) -> None: ...
global___AdadeltaParameters = AdadeltaParameters

class ProximalAdagradParameters(google.protobuf.message.Message):
    """https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer
    https://github.com/tensorflow/tensorflow/blob/6b6471f3ffb7f1fefe42d814aa5fb9ab7a535b58/tensorflow/core/kernels/training_ops.cc#L1961
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    L1_FIELD_NUMBER: builtins.int
    L2_FIELD_NUMBER: builtins.int
    l1: builtins.float
    l2: builtins.float
    def __init__(self,
        *,
        l1: builtins.float = ...,
        l2: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["l1",b"l1","l2",b"l2"]) -> None: ...
global___ProximalAdagradParameters = ProximalAdagradParameters

class OnlineYogiParameters(google.protobuf.message.Message):
    """The online Yogi optimizer does not implement hyper-parameter update; use the
    dynamic learning rate feature instead, setting the learning rate to:
    user learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
    Here, t is the current timestep.

    https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf
    plus some extensions based on FTRL.

    Note that the code by default implements the lazy version of online Yogi.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    L1_FIELD_NUMBER: builtins.int
    L2_FIELD_NUMBER: builtins.int
    BETA2_FIELD_NUMBER: builtins.int
    l1: builtins.float
    """The L1 regularization parameter (used analogously to the one in FTRL)."""

    l2: builtins.float
    """The L2 regularization parameter (used analogously to the one in FTRL)."""

    beta2: builtins.float
    """\\beta_2 from Algorithm 2 in the paper."""

    def __init__(self,
        *,
        l1: builtins.float = ...,
        l2: builtins.float = ...,
        beta2: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["beta2",b"beta2","l1",b"l1","l2",b"l2"]) -> None: ...
global___OnlineYogiParameters = OnlineYogiParameters

class ProximalYogiParameters(google.protobuf.message.Message):
    """The online Yogi optimizer does not implement hyper-parameter update; use the
    dynamic learning rate feature instead, setting the learning rate to:
    user learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
    Here, t is the current timestep.

    https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf
    plus some extensions based on FTRL.

    Note that the code by default implements the lazy version of proximal Yogi.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    L1_FIELD_NUMBER: builtins.int
    L2_FIELD_NUMBER: builtins.int
    BETA1_FIELD_NUMBER: builtins.int
    BETA2_FIELD_NUMBER: builtins.int
    EPSILON_FIELD_NUMBER: builtins.int
    l1: builtins.float
    """The L1 regularization parameter."""

    l2: builtins.float
    """The L2 regularization parameter."""

    beta1: builtins.float
    """The exponential decay rate for the 1st moment estimates."""

    beta2: builtins.float
    """The exponential decay rate for the 2nd moment estimates."""

    epsilon: builtins.float
    """A constant trading off adaptivity and noise."""

    def __init__(self,
        *,
        l1: builtins.float = ...,
        l2: builtins.float = ...,
        beta1: builtins.float = ...,
        beta2: builtins.float = ...,
        epsilon: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["beta1",b"beta1","beta2",b"beta2","epsilon",b"epsilon","l1",b"l1","l2",b"l2"]) -> None: ...
global___ProximalYogiParameters = ProximalYogiParameters

class FrequencyEstimatorParameters(google.protobuf.message.Message):
    """Estimator for the frequency of updates to a lookup table. It maintains an
    array (tf.Variable) D, where each element records the average number of
    global steps between two consecutive batches that hit the corresponding
    bucket. Once an item with bucket id i is sampled, D[i] is updated by:
      D[i] <- D[i] * (1 - tau) + delta[i] * tau,

    where tau is a learning rate between 0 and 1 (exclusive), and
      delta[i] = current global step - last step i is sampled.

    The estimated frequency (sampling rate in a batch) is thus 1 / D[i].

    Elements in D are initialized with a large value max_delta. delta[i] will
    also be capped by this value.

    The exact sequence of operations used in the optimizer is shown below.
    last_hit_step[i] is a tf.Variable that holds the last global step at which i
    was sampled.

      delta = global_step - last_hit_step[i]
      clipped_delta = min(delta, params.max_delta)
      is_outlier = (delta >= params.outlier_threshold * D[i])
      D[i] <- is_outlier ? clipped_delta
                         : D[i] * (1 - params.tau) + clipped_delta * params.tau
      last_hit_step[i] <- global_step
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    TAU_FIELD_NUMBER: builtins.int
    MAX_DELTA_FIELD_NUMBER: builtins.int
    OUTLIER_THRESHOLD_FIELD_NUMBER: builtins.int
    WEIGHT_EXPONENT_FIELD_NUMBER: builtins.int
    tau: builtins.float
    """Learning rate between (0, 1) that is used to update the array D."""

    max_delta: builtins.float
    """Maximum value of delta: difference between the current global step and the
    last global step at which the row was sampled.
    """

    outlier_threshold: builtins.float
    """Threshold used to determine whether the current update is an outlier."""

    weight_exponent: builtins.float
    """The weight exponent used to transform the estimated delta into weights.
    The transformation function is: (delta / max_delta) ^ (weight_exponent)
    """

    def __init__(self,
        *,
        tau: builtins.float = ...,
        max_delta: builtins.float = ...,
        outlier_threshold: builtins.float = ...,
        weight_exponent: builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_delta",b"max_delta","outlier_threshold",b"outlier_threshold","tau",b"tau","weight_exponent",b"weight_exponent"]) -> None: ...
global___FrequencyEstimatorParameters = FrequencyEstimatorParameters

class UserDefinedProgramParameters(google.protobuf.message.Message):
    """A user-defined optimizer.
    The contained HLO program must take the following arguments in the following
    order:
    1.  gradients
    2.  table weights
    3.  slot variables
    4.  an optional scalar input that is passed in via the dynamic learning
        rate mechanism.

    It must return/end in a tuple op that contains the following values in the
    following order:
    1.  new table values
    2.  new slot variable value

    The program must have shape (1,1) with dtype float32 throughout and only use
    HLO that operate elementwise (e.g., no reduce, no variables, no control flow
    and no broadcasting outside of the single scalar input).
    The HLO program should be written as if it were a dense update. It will be
    called on each row that needs an update and will applied elementwise.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PROGRAM_FIELD_NUMBER: builtins.int
    PADDING_VALUES_FIELD_NUMBER: builtins.int
    @property
    def program(self) -> tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto: ...
    @property
    def padding_values(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]:
        """Padding values for the parameter and the slots, see
        StateVariableSpecification.padding_initial_value below for more details on
        how this should be set. One value is needed for the weights and one for
        each slot.
        """
        pass
    def __init__(self,
        *,
        program: typing.Optional[tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto] = ...,
        padding_values: typing.Optional[typing.Iterable[builtins.float]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["program",b"program"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["padding_values",b"padding_values","program",b"program"]) -> None: ...
global___UserDefinedProgramParameters = UserDefinedProgramParameters

class AssignParameters(google.protobuf.message.Message):
    """Optimizer that just sets the variable to the value of the gradient. To be
    correct, this requires either gradient accumulation (to sum the values of a
    computed expression across the samples) or to deduplicate IDs within a single
    host (to assign the value from an arbitrary sample).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    def __init__(self,
        ) -> None: ...
global___AssignParameters = AssignParameters

class GradientAccumulationStatus(google.protobuf.message.Message):
    """Status of using gradient accumulation (doing two passes over the input
    gradients: one to accumulate them into a temporary array and another to apply
    them using the actual optimization algorithm). The extra message is to wrap
    the enum for scoping.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _Status:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StatusEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[GradientAccumulationStatus._Status.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNSPECIFIED: GradientAccumulationStatus._Status.ValueType  # 0
        ENABLED: GradientAccumulationStatus._Status.ValueType  # 1
        DISABLED: GradientAccumulationStatus._Status.ValueType  # 2
    class Status(_Status, metaclass=_StatusEnumTypeWrapper):
        """if UNSPECIFIED (default), gradient accumulation is ENABLED."""
        pass

    UNSPECIFIED: GradientAccumulationStatus.Status.ValueType  # 0
    ENABLED: GradientAccumulationStatus.Status.ValueType  # 1
    DISABLED: GradientAccumulationStatus.Status.ValueType  # 2

    def __init__(self,
        ) -> None: ...
global___GradientAccumulationStatus = GradientAccumulationStatus

class HotIdReplicationConfiguration(google.protobuf.message.Message):
    """Configuration proto for hot ID optimization. This is an experimental feature
    that is currently disabled (by default).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _Status:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StatusEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[HotIdReplicationConfiguration._Status.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        UNSPECIFIED: HotIdReplicationConfiguration._Status.ValueType  # 0
        ENABLED: HotIdReplicationConfiguration._Status.ValueType  # 1
        DISABLED: HotIdReplicationConfiguration._Status.ValueType  # 2
    class Status(_Status, metaclass=_StatusEnumTypeWrapper):
        """Whether to enable or disable hot ID optimization.
        If UNSPECIFIED (default), hot ID optimization is DISABLED.
        """
        pass

    UNSPECIFIED: HotIdReplicationConfiguration.Status.ValueType  # 0
    ENABLED: HotIdReplicationConfiguration.Status.ValueType  # 1
    DISABLED: HotIdReplicationConfiguration.Status.ValueType  # 2

    STATUS_FIELD_NUMBER: builtins.int
    status: global___HotIdReplicationConfiguration.Status.ValueType
    def __init__(self,
        *,
        status: global___HotIdReplicationConfiguration.Status.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["status",b"status"]) -> None: ...
global___HotIdReplicationConfiguration = HotIdReplicationConfiguration

class OptimizationParameters(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    LEARNING_RATE_FIELD_NUMBER: builtins.int
    CLIPPING_LIMITS_FIELD_NUMBER: builtins.int
    GRADIENT_CLIPPING_LIMITS_FIELD_NUMBER: builtins.int
    WEIGHT_DECAY_FACTOR_FIELD_NUMBER: builtins.int
    MULTIPLY_WEIGHT_DECAY_FACTOR_BY_LEARNING_RATE_FIELD_NUMBER: builtins.int
    GRADIENT_ACCUMULATION_STATUS_FIELD_NUMBER: builtins.int
    HOT_ID_REPLICATION_CONFIGURATION_FIELD_NUMBER: builtins.int
    ADAGRAD_FIELD_NUMBER: builtins.int
    BOUNDED_ADAGRAD_FIELD_NUMBER: builtins.int
    STOCHASTIC_GRADIENT_DESCENT_FIELD_NUMBER: builtins.int
    FTRL_FIELD_NUMBER: builtins.int
    ADAM_FIELD_NUMBER: builtins.int
    MOMENTUM_FIELD_NUMBER: builtins.int
    RMS_PROP_FIELD_NUMBER: builtins.int
    CENTERED_RMS_PROP_FIELD_NUMBER: builtins.int
    MDL_ADAGRAD_LIGHT_FIELD_NUMBER: builtins.int
    ADADELTA_FIELD_NUMBER: builtins.int
    PROXIMAL_ADAGRAD_FIELD_NUMBER: builtins.int
    ONLINE_YOGI_FIELD_NUMBER: builtins.int
    PROXIMAL_YOGI_FIELD_NUMBER: builtins.int
    FREQUENCY_ESTIMATOR_FIELD_NUMBER: builtins.int
    USER_DEFINED_PROGRAM_FIELD_NUMBER: builtins.int
    ASSIGN_FIELD_NUMBER: builtins.int
    @property
    def learning_rate(self) -> global___LearningRate:
        """Learning rate used for updating the embedding layer parameters."""
        pass
    @property
    def clipping_limits(self) -> global___ClippingLimits:
        """Limits to which to clip the weight values after the backward pass; not
        present means no limits are applied.
        """
        pass
    @property
    def gradient_clipping_limits(self) -> global___ClippingLimits:
        """Limits to which to clip the backward pass gradient before using it for
        updates; not present means no limits are applied.
        """
        pass
    weight_decay_factor: builtins.float
    """Amount of weight decay to apply; see weight_decay_optimizers.py for
    details. Almost all optimizers are supported with this option (MDL Adagrad
    Light does not work, and SGD does not behave as expected if it is enabled).
    Although there is no check, users who want weight decay will probably also
    want to enable gradient accumulation as well so that the decay will happen
    once per minibatch.
    """

    multiply_weight_decay_factor_by_learning_rate: builtins.bool
    """If true, the weight decay factor is multiplied by the current learning rate
    before use; this is to match the note in DecoupledWeightDecayExtension in
    weight_decay_optimizers.py.
    """

    gradient_accumulation_status: global___GradientAccumulationStatus.Status.ValueType
    """Status of using gradient accumulation (doing two passes over the input
    gradients: one to accumulate them into a temporary array and another to
    apply them using the actual optimization algorithm).
    """

    @property
    def hot_id_replication_configuration(self) -> global___HotIdReplicationConfiguration:
        """Configuration proto for hot ID replication. This is an experimental
        feature that is currently disabled (by default).
        """
        pass
    @property
    def adagrad(self) -> global___AdagradParameters: ...
    @property
    def bounded_adagrad(self) -> global___BoundedAdagradParameters: ...
    @property
    def stochastic_gradient_descent(self) -> global___StochasticGradientDescentParameters: ...
    @property
    def ftrl(self) -> global___FtrlParameters: ...
    @property
    def adam(self) -> global___AdamParameters: ...
    @property
    def momentum(self) -> global___MomentumParameters: ...
    @property
    def rms_prop(self) -> global___RmsPropParameters: ...
    @property
    def centered_rms_prop(self) -> global___CenteredRmsPropParameters: ...
    @property
    def mdl_adagrad_light(self) -> global___MdlAdagradLightParameters: ...
    @property
    def adadelta(self) -> global___AdadeltaParameters: ...
    @property
    def proximal_adagrad(self) -> global___ProximalAdagradParameters: ...
    @property
    def online_yogi(self) -> global___OnlineYogiParameters: ...
    @property
    def proximal_yogi(self) -> global___ProximalYogiParameters: ...
    @property
    def frequency_estimator(self) -> global___FrequencyEstimatorParameters: ...
    @property
    def user_defined_program(self) -> global___UserDefinedProgramParameters: ...
    @property
    def assign(self) -> global___AssignParameters: ...
    def __init__(self,
        *,
        learning_rate: typing.Optional[global___LearningRate] = ...,
        clipping_limits: typing.Optional[global___ClippingLimits] = ...,
        gradient_clipping_limits: typing.Optional[global___ClippingLimits] = ...,
        weight_decay_factor: builtins.float = ...,
        multiply_weight_decay_factor_by_learning_rate: builtins.bool = ...,
        gradient_accumulation_status: global___GradientAccumulationStatus.Status.ValueType = ...,
        hot_id_replication_configuration: typing.Optional[global___HotIdReplicationConfiguration] = ...,
        adagrad: typing.Optional[global___AdagradParameters] = ...,
        bounded_adagrad: typing.Optional[global___BoundedAdagradParameters] = ...,
        stochastic_gradient_descent: typing.Optional[global___StochasticGradientDescentParameters] = ...,
        ftrl: typing.Optional[global___FtrlParameters] = ...,
        adam: typing.Optional[global___AdamParameters] = ...,
        momentum: typing.Optional[global___MomentumParameters] = ...,
        rms_prop: typing.Optional[global___RmsPropParameters] = ...,
        centered_rms_prop: typing.Optional[global___CenteredRmsPropParameters] = ...,
        mdl_adagrad_light: typing.Optional[global___MdlAdagradLightParameters] = ...,
        adadelta: typing.Optional[global___AdadeltaParameters] = ...,
        proximal_adagrad: typing.Optional[global___ProximalAdagradParameters] = ...,
        online_yogi: typing.Optional[global___OnlineYogiParameters] = ...,
        proximal_yogi: typing.Optional[global___ProximalYogiParameters] = ...,
        frequency_estimator: typing.Optional[global___FrequencyEstimatorParameters] = ...,
        user_defined_program: typing.Optional[global___UserDefinedProgramParameters] = ...,
        assign: typing.Optional[global___AssignParameters] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["adadelta",b"adadelta","adagrad",b"adagrad","adam",b"adam","assign",b"assign","bounded_adagrad",b"bounded_adagrad","centered_rms_prop",b"centered_rms_prop","clipping_limits",b"clipping_limits","frequency_estimator",b"frequency_estimator","ftrl",b"ftrl","gradient_clipping_limits",b"gradient_clipping_limits","hot_id_replication_configuration",b"hot_id_replication_configuration","learning_rate",b"learning_rate","mdl_adagrad_light",b"mdl_adagrad_light","momentum",b"momentum","online_yogi",b"online_yogi","parameters",b"parameters","proximal_adagrad",b"proximal_adagrad","proximal_yogi",b"proximal_yogi","rms_prop",b"rms_prop","stochastic_gradient_descent",b"stochastic_gradient_descent","user_defined_program",b"user_defined_program"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["adadelta",b"adadelta","adagrad",b"adagrad","adam",b"adam","assign",b"assign","bounded_adagrad",b"bounded_adagrad","centered_rms_prop",b"centered_rms_prop","clipping_limits",b"clipping_limits","frequency_estimator",b"frequency_estimator","ftrl",b"ftrl","gradient_accumulation_status",b"gradient_accumulation_status","gradient_clipping_limits",b"gradient_clipping_limits","hot_id_replication_configuration",b"hot_id_replication_configuration","learning_rate",b"learning_rate","mdl_adagrad_light",b"mdl_adagrad_light","momentum",b"momentum","multiply_weight_decay_factor_by_learning_rate",b"multiply_weight_decay_factor_by_learning_rate","online_yogi",b"online_yogi","parameters",b"parameters","proximal_adagrad",b"proximal_adagrad","proximal_yogi",b"proximal_yogi","rms_prop",b"rms_prop","stochastic_gradient_descent",b"stochastic_gradient_descent","user_defined_program",b"user_defined_program","weight_decay_factor",b"weight_decay_factor"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["parameters",b"parameters"]) -> typing.Optional[typing_extensions.Literal["adagrad","bounded_adagrad","stochastic_gradient_descent","ftrl","adam","momentum","rms_prop","centered_rms_prop","mdl_adagrad_light","adadelta","proximal_adagrad","online_yogi","proximal_yogi","frequency_estimator","user_defined_program","assign"]]: ...
global___OptimizationParameters = OptimizationParameters

class StateVariableSpecification(google.protobuf.message.Message):
    """Specification of an optimization algorithm's state variables (both the main
    value vector and any extra accumulators, etc.). This proto is only used
    internally by the TPU software and is not exposed directly to the TF model.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class UserDefined(google.protobuf.message.Message):
        """A normal state variable that should be saved and restored in checkpoints
        and used as an input or output to non-debug TensorFlow ops.
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor
        PADDING_INITIAL_VALUE_FIELD_NUMBER: builtins.int
        padding_initial_value: builtins.float
        """For padding embedding rows, this field specifies the initial value to be
        used. Separate initial values need to be specified for the embeddings and
        any extra accumulators. The initial values should be specified so as to
        maintain two invariants during model training:
        (1) The embedding vector multiplied by zero returns a vector containing
            all zeros. To maintain this invariant, the embedding values should
            never be NaNs or +-infinity.
        (2) Repeatedly applying the optimizer using a gradient vector of all
            zeros does not cause the embeddings or slot variables to become NaNs
            or +-infinity.
        The padding row is looked up when no embedding IDs are present for a
        feature. The semantics of embedding lookup dictate that the output must
        be zero under this scenario.
        """

        def __init__(self,
            *,
            padding_initial_value: builtins.float = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["padding_initial_value",b"padding_initial_value"]) -> None: ...

    class FillWithConstant(google.protobuf.message.Message):
        """A state variable that should be filled with a constant and normally hidden
        from users (used for intermediate gradients being accumulated, for
        example).
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor
        INITIAL_VALUE_FIELD_NUMBER: builtins.int
        initial_value: builtins.float
        def __init__(self,
            *,
            initial_value: builtins.float = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["initial_value",b"initial_value"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    USER_DEFINED_FIELD_NUMBER: builtins.int
    FILL_WITH_CONSTANT_FIELD_NUMBER: builtins.int
    name: typing.Text
    """Parameter name for the state variable."""

    @property
    def user_defined(self) -> global___StateVariableSpecification.UserDefined: ...
    @property
    def fill_with_constant(self) -> global___StateVariableSpecification.FillWithConstant: ...
    def __init__(self,
        *,
        name: typing.Text = ...,
        user_defined: typing.Optional[global___StateVariableSpecification.UserDefined] = ...,
        fill_with_constant: typing.Optional[global___StateVariableSpecification.FillWithConstant] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["fill_with_constant",b"fill_with_constant","usage",b"usage","user_defined",b"user_defined"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["fill_with_constant",b"fill_with_constant","name",b"name","usage",b"usage","user_defined",b"user_defined"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["usage",b"usage"]) -> typing.Optional[typing_extensions.Literal["user_defined","fill_with_constant"]]: ...
global___StateVariableSpecification = StateVariableSpecification
