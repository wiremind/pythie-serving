# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: tensorflow_serving/apis/model_service.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pythie_serving.tensorflow_proto.tensorflow_serving.apis import get_model_status_pb2 as tensorflow__serving_dot_apis_dot_get__model__status__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import model_management_pb2 as tensorflow__serving_dot_apis_dot_model__management__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n+tensorflow_serving/apis/model_service.proto\x12\x12tensorflow.serving\x1a.tensorflow_serving/apis/get_model_status.proto\x1a.tensorflow_serving/apis/model_management.proto2\xe7\x01\n\x0cModelService\x12g\n\x0eGetModelStatus\x12).tensorflow.serving.GetModelStatusRequest\x1a*.tensorflow.serving.GetModelStatusResponse\x12n\n\x19HandleReloadConfigRequest\x12\'.tensorflow.serving.ReloadConfigRequest\x1a(.tensorflow.serving.ReloadConfigResponseB\x03\xf8\x01\x01\x62\x06proto3')



_MODELSERVICE = DESCRIPTOR.services_by_name['ModelService']
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'\370\001\001'
  _MODELSERVICE._serialized_start=164
  _MODELSERVICE._serialized_end=395
# @@protoc_insertion_point(module_scope)
