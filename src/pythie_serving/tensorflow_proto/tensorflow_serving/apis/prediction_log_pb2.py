# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: tensorflow_serving/apis/prediction_log.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pythie_serving.tensorflow_proto.tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import logging_pb2 as tensorflow__serving_dot_apis_dot_logging__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2
from pythie_serving.tensorflow_proto.tensorflow_serving.apis import session_service_pb2 as tensorflow__serving_dot_apis_dot_session__service__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n,tensorflow_serving/apis/prediction_log.proto\x12\x12tensorflow.serving\x1a,tensorflow_serving/apis/classification.proto\x1a\'tensorflow_serving/apis/inference.proto\x1a%tensorflow_serving/apis/logging.proto\x1a%tensorflow_serving/apis/predict.proto\x1a(tensorflow_serving/apis/regression.proto\x1a-tensorflow_serving/apis/session_service.proto\"\x87\x01\n\x0b\x43lassifyLog\x12:\n\x07request\x18\x01 \x01(\x0b\x32).tensorflow.serving.ClassificationRequest\x12<\n\x08response\x18\x02 \x01(\x0b\x32*.tensorflow.serving.ClassificationResponse\"~\n\nRegressLog\x12\x36\n\x07request\x18\x01 \x01(\x0b\x32%.tensorflow.serving.RegressionRequest\x12\x38\n\x08response\x18\x02 \x01(\x0b\x32&.tensorflow.serving.RegressionResponse\"x\n\nPredictLog\x12\x33\n\x07request\x18\x01 \x01(\x0b\x32\".tensorflow.serving.PredictRequest\x12\x35\n\x08response\x18\x02 \x01(\x0b\x32#.tensorflow.serving.PredictResponse\"\x8d\x01\n\x11MultiInferenceLog\x12:\n\x07request\x18\x01 \x01(\x0b\x32).tensorflow.serving.MultiInferenceRequest\x12<\n\x08response\x18\x02 \x01(\x0b\x32*.tensorflow.serving.MultiInferenceResponse\"\x81\x01\n\rSessionRunLog\x12\x36\n\x07request\x18\x01 \x01(\x0b\x32%.tensorflow.serving.SessionRunRequest\x12\x38\n\x08response\x18\x02 \x01(\x0b\x32&.tensorflow.serving.SessionRunResponse\"\xfd\x02\n\rPredictionLog\x12\x35\n\x0clog_metadata\x18\x01 \x01(\x0b\x32\x1f.tensorflow.serving.LogMetadata\x12\x37\n\x0c\x63lassify_log\x18\x02 \x01(\x0b\x32\x1f.tensorflow.serving.ClassifyLogH\x00\x12\x35\n\x0bregress_log\x18\x03 \x01(\x0b\x32\x1e.tensorflow.serving.RegressLogH\x00\x12\x35\n\x0bpredict_log\x18\x06 \x01(\x0b\x32\x1e.tensorflow.serving.PredictLogH\x00\x12\x44\n\x13multi_inference_log\x18\x04 \x01(\x0b\x32%.tensorflow.serving.MultiInferenceLogH\x00\x12<\n\x0fsession_run_log\x18\x05 \x01(\x0b\x32!.tensorflow.serving.SessionRunLogH\x00\x42\n\n\x08log_typeB\x03\xf8\x01\x01\x62\x06proto3')

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tensorflow_serving.apis.prediction_log_pb2', globals())
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'\370\001\001'
  _CLASSIFYLOG._serialized_start=323
  _CLASSIFYLOG._serialized_end=458
  _REGRESSLOG._serialized_start=460
  _REGRESSLOG._serialized_end=586
  _PREDICTLOG._serialized_start=588
  _PREDICTLOG._serialized_end=708
  _MULTIINFERENCELOG._serialized_start=711
  _MULTIINFERENCELOG._serialized_end=852
  _SESSIONRUNLOG._serialized_start=855
  _SESSIONRUNLOG._serialized_end=984
  _PREDICTIONLOG._serialized_start=987
  _PREDICTIONLOG._serialized_end=1368
# @@protoc_insertion_point(module_scope)
